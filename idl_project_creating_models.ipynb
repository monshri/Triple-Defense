{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17975,
     "status": "ok",
     "timestamp": 1604177829763,
     "user": {
      "displayName": "Shivani Shekhar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgoJRLkMl-_Mww1DJ9mjcT55y0dR6mIJE-30KVL=s64",
      "userId": "17493010925335591166"
     },
     "user_tz": 240
    },
    "id": "AcnIH59Pvv4n",
    "outputId": "306e9d2d-ca94-4689-a5fa-93ca453b8ccf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/project\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3972,
     "status": "ok",
     "timestamp": 1604177836120,
     "user": {
      "displayName": "Shivani Shekhar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgoJRLkMl-_Mww1DJ9mjcT55y0dR6mIJE-30KVL=s64",
      "userId": "17493010925335591166"
     },
     "user_tz": 240
    },
    "id": "6SqUPdAKkDIn",
    "outputId": "78fa7e18-e0d1-4fcc-ce5a-0fe92cce0d07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import PIL\n",
    "\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov  4 20:52:27 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   47C    P0    28W /  70W |      3MiB / 15109MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PfyaOczXoRqQ"
   },
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "      res = tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "      return torch.clamp(input=res, min=-0.5, max=0.5)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NkIqO0-ntgcy"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total_predictions = 0.0\n",
    "    correct_predictions = 0.0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):   \n",
    "        optimizer.zero_grad()   # .backward() accumulates gradients\n",
    "        data = data.to(device)\n",
    "        target = target.to(device) # all data & model on same device\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "          outputs = model(data)\n",
    "          loss = criterion(outputs, target)\n",
    "          running_loss += loss.item()\n",
    "\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total_predictions += target.size(0)\n",
    "          correct_predictions += (predicted == target).sum().item()\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "\n",
    "        del data\n",
    "        del target\n",
    "    \n",
    "    \n",
    "    running_loss /= len(train_loader)\n",
    "    acc = (correct_predictions/total_predictions)*100.0\n",
    "    logging.info('Training Loss: {}'.format(running_loss))\n",
    "    logging.info('Training Accuracy: {}%'.format(acc))\n",
    "    return running_loss\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(test_loader): \n",
    "            #print(data.shape)  \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "              outputs = model(data)\n",
    "\n",
    "              _, predicted = torch.max(outputs.data, 1)\n",
    "              total_predictions += target.size(0)\n",
    "              correct_predictions += (predicted == target).sum().item()\n",
    "\n",
    "              loss = criterion(outputs, target).detach()\n",
    "              running_loss += loss.item()\n",
    "\n",
    "            del data\n",
    "            del target\n",
    "\n",
    "        running_loss /= len(test_loader)\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        logging.info('Testing Loss: {}'.format(running_loss))\n",
    "        logging.info('Testing Accuracy: {}%'.format(acc))\n",
    "        return running_loss, acc\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "executionInfo": {
     "elapsed": 350,
     "status": "error",
     "timestamp": 1604177838595,
     "user": {
      "displayName": "Shivani Shekhar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgoJRLkMl-_Mww1DJ9mjcT55y0dR6mIJE-30KVL=s64",
      "userId": "17493010925335591166"
     },
     "user_tz": 240
    },
    "id": "dXX4443Y16Ul",
    "outputId": "57316766-ca6c-4dc5-e24b-81ebc0783d8f"
   },
   "outputs": [],
   "source": [
    "# configure logging\n",
    "logger = logging.getLogger(\"\")\n",
    "\n",
    "# reset handler\n",
    "for handler in logging.root.handlers[:]:\n",
    "  logging.root.removeHandler(handler)\n",
    "\n",
    "# set handler\n",
    "stream_hdlr = logging.StreamHandler()\n",
    "file_hdlr = logging.FileHandler('/home/ubuntu/project/log_{}.log'.format(datetime.datetime.now()))\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "stream_hdlr.setFormatter(formatter)\n",
    "file_hdlr.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(stream_hdlr)\n",
    "logger.addHandler(file_hdlr)\n",
    "\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zq_mk_bEBtlt"
   },
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, nb_classes=10):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.vgg16 = models.vgg16_bn()\n",
    "        self.linear = nn.Linear(1000, nb_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.vgg16(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fFZ8ZtrqxZ7j"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-04 20:52:27,327 INFO {'lr': 0.0001, 'min_lr': 1e-08, 'weight_decay': 0.0005, 'num_models': 30, 'num_epochs': 30, 'noise_std': 0.06}\n"
     ]
    }
   ],
   "source": [
    "train_batchsize = 128\n",
    "test_batchsize = 100\n",
    "num_workers = 8\n",
    "num_classes = 10\n",
    "\n",
    "n_epochs = 30\n",
    "img_size = 224\n",
    "lr = 1e-4\n",
    "min_lr = 1e-8\n",
    "weight_decay = 5e-4\n",
    "num_models = 30\n",
    "noise_std = 0.06\n",
    "\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "hyper_params = {'lr': lr, 'min_lr': min_lr, 'weight_decay': weight_decay, 'num_models': num_models, 'num_epochs': n_epochs, 'noise_std': noise_std}\n",
    "logging.info(hyper_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUfvg4LvwMA0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-04 20:52:27,415 INFO Seed: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-04 20:52:35,710 INFO Epoch: 0\n",
      "2020-11-04 21:00:34,021 INFO Training Loss: 2.340395213088111\n",
      "2020-11-04 21:00:34,023 INFO Training Accuracy: 28.362%\n",
      "2020-11-04 21:01:05,328 INFO Testing Loss: 1.579775584936142\n",
      "2020-11-04 21:01:05,329 INFO Testing Accuracy: 42.3%\n",
      "2020-11-04 21:01:05,330 INFO Time: 509.6202492713928s\n",
      "2020-11-04 21:01:05,331 INFO ====================\n",
      "2020-11-04 21:01:05,331 INFO Epoch: 1\n",
      "2020-11-04 21:09:28,506 INFO Training Loss: 1.54556321670942\n",
      "2020-11-04 21:09:28,507 INFO Training Accuracy: 43.586000000000006%\n",
      "2020-11-04 21:10:00,129 INFO Testing Loss: 1.490691261291504\n",
      "2020-11-04 21:10:00,131 INFO Testing Accuracy: 46.79%\n",
      "2020-11-04 21:10:00,133 INFO Time: 534.8012945652008s\n",
      "2020-11-04 21:10:00,133 INFO ====================\n",
      "2020-11-04 21:10:00,134 INFO Epoch: 2\n",
      "2020-11-04 21:18:27,273 INFO Training Loss: 1.3295782542289676\n",
      "2020-11-04 21:18:27,275 INFO Training Accuracy: 52.124%\n",
      "2020-11-04 21:18:58,968 INFO Testing Loss: 1.0692749750614166\n",
      "2020-11-04 21:18:58,969 INFO Testing Accuracy: 61.480000000000004%\n",
      "2020-11-04 21:18:58,970 INFO Time: 538.8360788822174s\n",
      "2020-11-04 21:18:58,970 INFO ====================\n",
      "2020-11-04 21:18:58,972 INFO Epoch: 3\n",
      "2020-11-04 21:27:25,835 INFO Training Loss: 1.1335049661833916\n",
      "2020-11-04 21:27:25,836 INFO Training Accuracy: 60.087999999999994%\n",
      "2020-11-04 21:27:57,608 INFO Testing Loss: 0.934257630109787\n",
      "2020-11-04 21:27:57,609 INFO Testing Accuracy: 67.65%\n",
      "2020-11-04 21:27:57,610 INFO Time: 538.6380879878998s\n",
      "2020-11-04 21:27:57,610 INFO ====================\n",
      "2020-11-04 21:27:57,611 INFO Epoch: 4\n",
      "2020-11-04 21:36:20,147 INFO Training Loss: 1.0006079280467899\n",
      "2020-11-04 21:36:20,149 INFO Training Accuracy: 65.41%\n",
      "2020-11-04 21:36:51,713 INFO Testing Loss: 0.8984512710571289\n",
      "2020-11-04 21:36:51,714 INFO Testing Accuracy: 69.03%\n",
      "2020-11-04 21:36:51,715 INFO Time: 534.1038188934326s\n",
      "2020-11-04 21:36:51,716 INFO ====================\n",
      "2020-11-04 21:36:51,716 INFO Epoch: 5\n",
      "2020-11-04 21:45:14,604 INFO Training Loss: 0.8906905750179535\n",
      "2020-11-04 21:45:14,605 INFO Training Accuracy: 69.256%\n",
      "2020-11-04 21:45:45,896 INFO Testing Loss: 0.8853736788034439\n",
      "2020-11-04 21:45:45,897 INFO Testing Accuracy: 68.61%\n",
      "2020-11-04 21:45:45,897 INFO Time: 534.1814346313477s\n",
      "2020-11-04 21:45:45,898 INFO ====================\n",
      "2020-11-04 21:45:45,898 INFO Epoch: 6\n",
      "2020-11-04 21:54:07,634 INFO Training Loss: 0.8221176711799544\n",
      "2020-11-04 21:54:07,636 INFO Training Accuracy: 71.946%\n",
      "2020-11-04 21:54:39,023 INFO Testing Loss: 0.7195642060041427\n",
      "2020-11-04 21:54:39,024 INFO Testing Accuracy: 75.38%\n",
      "2020-11-04 21:54:39,026 INFO Time: 533.1277403831482s\n",
      "2020-11-04 21:54:39,027 INFO ====================\n",
      "2020-11-04 21:54:39,028 INFO Epoch: 7\n",
      "2020-11-04 22:02:59,883 INFO Training Loss: 0.7637963450473287\n",
      "2020-11-04 22:02:59,884 INFO Training Accuracy: 74.072%\n",
      "2020-11-04 22:03:31,420 INFO Testing Loss: 0.8024331694841385\n",
      "2020-11-04 22:03:31,422 INFO Testing Accuracy: 72.08%\n",
      "2020-11-04 22:03:31,422 INFO Time: 532.3944799900055s\n",
      "2020-11-04 22:03:31,423 INFO ====================\n",
      "2020-11-04 22:03:31,423 INFO Epoch: 8\n",
      "2020-11-04 22:11:51,653 INFO Training Loss: 0.7047091881027612\n",
      "2020-11-04 22:11:51,654 INFO Training Accuracy: 75.916%\n",
      "2020-11-04 22:12:23,078 INFO Testing Loss: 0.697902561724186\n",
      "2020-11-04 22:12:23,080 INFO Testing Accuracy: 76.99000000000001%\n",
      "2020-11-04 22:12:23,080 INFO Time: 531.6569397449493s\n",
      "2020-11-04 22:12:23,081 INFO ====================\n",
      "2020-11-04 22:12:23,082 INFO Epoch: 9\n",
      "2020-11-04 22:20:44,243 INFO Training Loss: 0.671970489384878\n",
      "2020-11-04 22:20:44,245 INFO Training Accuracy: 77.25%\n",
      "2020-11-04 22:21:15,545 INFO Testing Loss: 0.6820010930299759\n",
      "2020-11-04 22:21:15,546 INFO Testing Accuracy: 76.42%\n",
      "2020-11-04 22:21:15,547 INFO Time: 532.4653062820435s\n",
      "2020-11-04 22:21:15,547 INFO ====================\n",
      "2020-11-04 22:21:15,548 INFO Epoch: 10\n",
      "2020-11-04 22:29:45,344 INFO Training Loss: 0.6367526828023173\n",
      "2020-11-04 22:29:45,346 INFO Training Accuracy: 78.402%\n",
      "2020-11-04 22:30:17,338 INFO Testing Loss: 0.6904381513595581\n",
      "2020-11-04 22:30:17,339 INFO Testing Accuracy: 77.96%\n",
      "2020-11-04 22:30:17,340 INFO Time: 541.792542219162s\n",
      "2020-11-04 22:30:17,341 INFO ====================\n",
      "2020-11-04 22:30:17,342 INFO Epoch: 11\n",
      "2020-11-04 22:38:47,818 INFO Training Loss: 0.601530285671239\n",
      "2020-11-04 22:38:47,819 INFO Training Accuracy: 79.584%\n",
      "2020-11-04 22:39:19,793 INFO Testing Loss: 0.6042165231704711\n",
      "2020-11-04 22:39:19,794 INFO Testing Accuracy: 80.03%\n",
      "2020-11-04 22:39:19,795 INFO Time: 542.4532871246338s\n",
      "2020-11-04 22:39:19,796 INFO ====================\n",
      "2020-11-04 22:39:19,796 INFO Epoch: 12\n",
      "2020-11-04 22:47:49,735 INFO Training Loss: 0.5722875630154329\n",
      "2020-11-04 22:47:49,736 INFO Training Accuracy: 80.498%\n",
      "2020-11-04 22:48:21,857 INFO Testing Loss: 0.5378452387452125\n",
      "2020-11-04 22:48:21,858 INFO Testing Accuracy: 82.28%\n",
      "2020-11-04 22:48:21,858 INFO Time: 542.0621738433838s\n",
      "2020-11-04 22:48:21,859 INFO ====================\n",
      "2020-11-04 22:48:21,860 INFO Epoch: 13\n",
      "2020-11-04 22:56:50,476 INFO Training Loss: 0.5472186905953585\n",
      "2020-11-04 22:56:50,477 INFO Training Accuracy: 81.648%\n",
      "2020-11-04 22:57:22,352 INFO Testing Loss: 0.5605075725913048\n",
      "2020-11-04 22:57:22,353 INFO Testing Accuracy: 81.24%\n",
      "2020-11-04 22:57:22,354 INFO Time: 540.4941699504852s\n",
      "2020-11-04 22:57:22,355 INFO ====================\n",
      "2020-11-04 22:57:22,355 INFO Epoch: 14\n",
      "2020-11-04 23:05:52,083 INFO Training Loss: 0.5231015575511376\n",
      "2020-11-04 23:05:52,085 INFO Training Accuracy: 82.254%\n",
      "2020-11-04 23:06:23,965 INFO Testing Loss: 0.5266066581010819\n",
      "2020-11-04 23:06:23,966 INFO Testing Accuracy: 82.47%\n",
      "2020-11-04 23:06:23,967 INFO Time: 541.6113703250885s\n",
      "2020-11-04 23:06:23,968 INFO ====================\n",
      "2020-11-04 23:06:23,968 INFO Epoch: 15\n",
      "2020-11-04 23:14:54,041 INFO Training Loss: 0.50410125520833\n",
      "2020-11-04 23:14:54,042 INFO Training Accuracy: 83.11%\n",
      "2020-11-04 23:15:25,897 INFO Testing Loss: 0.5470469510555267\n",
      "2020-11-04 23:15:25,898 INFO Testing Accuracy: 81.75%\n",
      "2020-11-04 23:15:25,899 INFO Time: 541.9308350086212s\n",
      "2020-11-04 23:15:25,899 INFO ====================\n",
      "2020-11-04 23:15:25,900 INFO Epoch: 16\n",
      "2020-11-04 23:23:55,625 INFO Training Loss: 0.4839318342068616\n",
      "2020-11-04 23:23:55,626 INFO Training Accuracy: 83.426%\n",
      "2020-11-04 23:24:27,360 INFO Testing Loss: 0.5125814056396485\n",
      "2020-11-04 23:24:27,362 INFO Testing Accuracy: 82.56%\n",
      "2020-11-04 23:24:27,362 INFO Time: 541.4622116088867s\n",
      "2020-11-04 23:24:27,363 INFO ====================\n",
      "2020-11-04 23:24:27,364 INFO Epoch: 17\n",
      "2020-11-04 23:32:57,610 INFO Training Loss: 0.4625297228393652\n",
      "2020-11-04 23:32:57,611 INFO Training Accuracy: 84.318%\n",
      "2020-11-04 23:33:29,419 INFO Testing Loss: 0.5096832558512687\n",
      "2020-11-04 23:33:29,420 INFO Testing Accuracy: 82.95%\n",
      "2020-11-04 23:33:29,420 INFO Time: 542.0568041801453s\n",
      "2020-11-04 23:33:29,421 INFO ====================\n",
      "2020-11-04 23:33:29,422 INFO Epoch: 18\n",
      "2020-11-04 23:41:59,475 INFO Training Loss: 0.44756938002603436\n",
      "2020-11-04 23:41:59,476 INFO Training Accuracy: 85.028%\n",
      "2020-11-04 23:42:31,447 INFO Testing Loss: 0.46564062386751176\n",
      "2020-11-04 23:42:31,448 INFO Testing Accuracy: 84.28%\n",
      "2020-11-04 23:42:31,448 INFO Time: 542.026529788971s\n",
      "2020-11-04 23:42:31,449 INFO ====================\n",
      "2020-11-04 23:42:31,449 INFO Epoch: 19\n",
      "2020-11-04 23:51:01,060 INFO Training Loss: 0.43064141650791365\n",
      "2020-11-04 23:51:01,061 INFO Training Accuracy: 85.49600000000001%\n",
      "2020-11-04 23:51:32,944 INFO Testing Loss: 0.4918174868822098\n",
      "2020-11-04 23:51:32,945 INFO Testing Accuracy: 83.23%\n",
      "2020-11-04 23:51:32,946 INFO Time: 541.4963314533234s\n",
      "2020-11-04 23:51:32,946 INFO ====================\n",
      "2020-11-04 23:51:32,947 INFO Epoch: 20\n"
     ]
    }
   ],
   "source": [
    "seeds = np.arange(0, num_models)\n",
    "\n",
    "for seed in seeds:\n",
    "  logging.info('Seed: {}'.format(seed))\n",
    "  torch.manual_seed(seed)\n",
    "  \n",
    "\n",
    "  train_transform = transforms.Compose([transforms.Resize(size=img_size),\n",
    "                      transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "                      transforms.RandomHorizontalFlip(p=0.5),\n",
    "                      transforms.RandomRotation(20, resample=PIL.Image.BILINEAR),\n",
    "                      transforms.ToTensor(),\n",
    "                      transforms.Normalize((0, 0, 0), (1, 1, 1)),\n",
    "                      AddGaussianNoise(0., noise_std)])\n",
    "\n",
    "  test_transform = transforms.Compose([transforms.Resize(size=img_size),\n",
    "                     transforms.ToTensor(),\n",
    "                     transforms.Normalize((0, 0, 0), (1, 1, 1)),\n",
    "                     AddGaussianNoise(0., noise_std)])\n",
    "\n",
    "  \n",
    "  trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "  testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "  trainloader = torch.utils.data.DataLoader(trainset, batch_size=train_batchsize, shuffle=True, num_workers=num_workers)\n",
    "  testloader = torch.utils.data.DataLoader(testset, batch_size=test_batchsize, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "  # model = models.vgg16()\n",
    "  model = VGG16(num_classes)\n",
    "  model.apply(init_weights)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  # optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, nesterov=False)\n",
    "  optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "  scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, cooldown=5, min_lr=min_lr, verbose=True)\n",
    "  scaler = torch.cuda.amp.GradScaler()\n",
    "  model.to(device)\n",
    "\n",
    "  # Train_loss = []\n",
    "  # Test_loss = []\n",
    "  # Test_acc = []\n",
    "\n",
    "  for i in range(n_epochs):\n",
    "    tic = time.time()\n",
    "    logging.info('Epoch: {}'.format(i))\n",
    "    train_loss = train_epoch(model, trainloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test_model(model, testloader, criterion)\n",
    "    scheduler.step(test_loss)\n",
    "    # Train_loss.append(train_loss)\n",
    "    # Test_loss.append(test_loss)\n",
    "    # Test_acc.append(test_acc)\n",
    "    toc = time.time()\n",
    "    logging.info('Time: {}s'.format(toc - tic))\n",
    "    logging.info('='*20)\n",
    "    \n",
    "  torch.save({'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict()}, \n",
    "        \"/home/ubuntu/project/Model_{}\".format(datetime.datetime.now()))\n",
    "  \n",
    "  torch.cuda.empty_cache()\n",
    "  del model\n",
    "  del criterion\n",
    "  del optimizer\n",
    "  del scheduler\n",
    "  del scaler\n",
    "  del trainloader\n",
    "  del testloader\n",
    "  del train_transform\n",
    "  del test_transform\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "executionInfo": {
     "elapsed": 1632,
     "status": "ok",
     "timestamp": 1602890774297,
     "user": {
      "displayName": "Kaicheng Ding",
      "photoUrl": "",
      "userId": "10577885083990106787"
     },
     "user_tz": 240
    },
    "id": "W6wXNJ1JkJD8",
    "outputId": "3abcb5c9-39ed-49be-d189-969bbc337b4c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img + 1 / 2     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "idl_project_creating_models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
