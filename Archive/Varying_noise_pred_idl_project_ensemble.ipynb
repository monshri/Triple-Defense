{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Varying_noise_pred_idl_project_ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaichengDING/Triple-Defense/blob/main/Varying_noise_pred_idl_project_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8idp0UC0Oj6"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ESeViSYxQfa",
        "outputId": "5c1a644a-757a-43e1-d841-e356775b6a41"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgPpHa2d0Rt4"
      },
      "source": [
        "# Install ART"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzKNCLpGws5x",
        "outputId": "816e38da-e4ea-48a6-94dd-a11b809f6cd0"
      },
      "source": [
        "!pip install adversarial-robustness-toolbox==1.4.3"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: adversarial-robustness-toolbox==1.4.3 in /usr/local/lib/python3.6/dist-packages (1.4.3)\n",
            "Requirement already satisfied: mypy in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (0.790)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (0.24.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (4.41.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn==0.22.2 in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (0.22.2)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (50.3.2)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (0.10.2)\n",
            "Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (1.18.5)\n",
            "Requirement already satisfied: resampy in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (0.2.2)\n",
            "Requirement already satisfied: cma in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (3.0.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (7.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from mypy->adversarial-robustness-toolbox==1.4.3) (3.7.4.3)\n",
            "Requirement already satisfied: mypy-extensions<0.5.0,>=0.4.3 in /usr/local/lib/python3.6/dist-packages (from mypy->adversarial-robustness-toolbox==1.4.3) (0.4.3)\n",
            "Requirement already satisfied: typed-ast<1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from mypy->adversarial-robustness-toolbox==1.4.3) (1.4.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox==1.4.3) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox==1.4.3) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox==1.4.3) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox==1.4.3) (2.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.22.2->adversarial-robustness-toolbox==1.4.3) (0.17.0)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from statsmodels->adversarial-robustness-toolbox==1.4.3) (1.1.5)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels->adversarial-robustness-toolbox==1.4.3) (0.5.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from ffmpeg-python->adversarial-robustness-toolbox==1.4.3) (0.16.0)\n",
            "Requirement already satisfied: numba>=0.32 in /usr/local/lib/python3.6/dist-packages (from resampy->adversarial-robustness-toolbox==1.4.3) (0.48.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->statsmodels->adversarial-robustness-toolbox==1.4.3) (2018.9)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.32->resampy->adversarial-robustness-toolbox==1.4.3) (0.31.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL8u7HvJ0Xka"
      },
      "source": [
        "# Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SqUPdAKkDIn"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import PIL\n",
        "\n",
        "import sys\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "from torch.utils import data\n",
        "\n",
        "from art.attacks.evasion import FastGradientMethod\n",
        "from art.attacks.evasion import ProjectedGradientDescentPyTorch\n",
        "from art.estimators.classification import PyTorchClassifier\n",
        "\n",
        "from art.estimators.classification import EnsembleClassifier\n",
        "from typing import List, Optional, Union, TYPE_CHECKING\n",
        "from art.estimators.classification.classifier import ClassifierNeuralNetwork\n",
        "from scipy.special import softmax\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import logging\n",
        "import datetime\n",
        "import random\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcOnXEFW1m4K"
      },
      "source": [
        "# Model Definition\n",
        "`ShuffleNet` and `ShuffleNetV2` are actually identical, but models with seed number smaller than 200 are created using ShuffleNet and models with seed number greater than 200 are created using ShuffleNetV2. In order to load all models into ensemble, the two definitions are provided here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfyaOczXoRqQ"
      },
      "source": [
        "class ShuffleNet(nn.Module):\n",
        "    def __init__(self, nb_classes =10):\n",
        "        super(ShuffleNet, self).__init__()\n",
        "        self.shuffle = models.shufflenet_v2_x2_0()\n",
        "        self.linear = nn.Linear(1000, nb_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.shuffle(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ShuffleNetV2(nn.Module):\n",
        "    def __init__(self, nb_classes=10):\n",
        "        super(ShuffleNetV2, self).__init__()\n",
        "        self.shufflenet = models.shufflenet_v2_x2_0()\n",
        "        self.linear = nn.Linear(1000, nb_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.shufflenet(x)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlygMiHF0iSB"
      },
      "source": [
        "# Logging Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXX4443Y16Ul"
      },
      "source": [
        "# configure logging\n",
        "logger = logging.getLogger(\"\")\n",
        "\n",
        "# reset handler\n",
        "for handler in logging.root.handlers[:]:\n",
        "  logging.root.removeHandler(handler)\n",
        "\n",
        "# set handler\n",
        "stream_hdlr = logging.StreamHandler()\n",
        "# logging on colab\n",
        "file_hdlr = logging.FileHandler('/content/gdrive/My Drive/IDL_Project/logs/Rlog_{}.log'.format(datetime.datetime.now()))\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
        "stream_hdlr.setFormatter(formatter)\n",
        "file_hdlr.setFormatter(formatter)\n",
        "\n",
        "logger.addHandler(stream_hdlr)\n",
        "logger.addHandler(file_hdlr)\n",
        "\n",
        "logger.setLevel(logging.INFO)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFiB-FbgT9sk"
      },
      "source": [
        "# Get Test Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk68nikXxa4G"
      },
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, X, Y, transform=None):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    self.transform = transform\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.Y)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    if self.transform is None:\n",
        "      return torch.from_numpy(self.X[idx]), torch.tensor(self.Y[idx]).long()\n",
        "    else:\n",
        "      return self.transform(self.X[idx]), torch.tensor(self.Y[idx]).long()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV2pPKKzeESx",
        "outputId": "b899a7fd-5f0d-4e0f-d892-816e47b5c282"
      },
      "source": [
        "test_batchsize = 200\n",
        "num_workers = 4\n",
        "nb_classes = 10\n",
        "img_size = 224\n",
        "subset_size = 1000 #### CHANGED SUBSET SIZE\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToPILImage(),\n",
        "                                     transforms.Resize(size=img_size),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0, 0, 0), (1, 1, 1))])\n",
        "\n",
        "testset= torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "testset_data = testset.data[0:subset_size]\n",
        "testset_labels = testset.targets[0:subset_size]\n",
        "\n",
        "testset_sub = MyDataset(testset_data, testset_labels, transform=test_transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset_sub, batch_size=test_batchsize, shuffle=False, num_workers=num_workers, drop_last=True)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inhJeBV-9ic0"
      },
      "source": [
        "# MyEnsembleClassifier\n",
        "Inherited from `EnsembleClassifier` with `predict`, `loss_gradient` and `loss_gradient_framework` overwritten\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ratuA8nTIKyu"
      },
      "source": [
        "class MyEnsembleClassifier(EnsembleClassifier):\n",
        "    def __init__(\n",
        "        self,\n",
        "        classifiers: List[ClassifierNeuralNetwork],\n",
        "        device,\n",
        "        infer_noise,\n",
        "        num_selected_models,\n",
        "        n_prediction,\n",
        "        n_loss,\n",
        "        classifier_weights: Union[list, np.ndarray, None] = None,\n",
        "        channels_first: bool = False,\n",
        "        clip_values: Optional[\"CLIP_VALUES_TYPE\"] = None,\n",
        "        preprocessing_defences: Union[\"Preprocessor\", List[\"Preprocessor\"], None] = None,\n",
        "        postprocessing_defences: Union[\"Postprocessor\", List[\"Postprocessor\"], None] = None,\n",
        "        preprocessing: \"PREPROCESSING_TYPE\" = (0, 1),\n",
        "    ) -> None:\n",
        "      super().__init__(\n",
        "          classifiers=classifiers,\n",
        "          classifier_weights=classifier_weights,\n",
        "          channels_first=channels_first,\n",
        "          clip_values=clip_values,\n",
        "          preprocessing_defences=preprocessing_defences,\n",
        "          postprocessing_defences=postprocessing_defences,\n",
        "          preprocessing=preprocessing\n",
        "      )\n",
        "      self.device = device\n",
        "      self.infer_noise = infer_noise\n",
        "      self.num_models = len(classifiers)\n",
        "      self.num_selected_models = num_selected_models\n",
        "      self.n_prediction = n_prediction\n",
        "      self.n_loss = n_loss\n",
        "\n",
        "    def predict(self, x: np.ndarray, batch_size: int = 128, raw: bool = False, **kwargs) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Perform prediction for a batch of inputs. Predictions from classifiers should only be aggregated if they all\n",
        "        have the same type of output (e.g., probabilities). Otherwise, use `raw=True` to get predictions from all\n",
        "        models without aggregation. The same option should be used for logits output, as logits are not comparable\n",
        "        between models and should not be aggregated.\n",
        "        :param x: Test set.\n",
        "        :param batch_size: Size of batches.\n",
        "        :param raw: Return the individual classifier raw outputs (not aggregated).\n",
        "        :return: Array of predictions of shape `(nb_inputs, nb_classes)`, or of shape\n",
        "                 `(nb_classifiers, nb_inputs, nb_classes)` if `raw=True`.\n",
        "        \"\"\"\n",
        "        indices = np.random.choice(self.num_models, self.n_prediction, replace=True)\n",
        "\n",
        "        noise_list = [np.random.randn(*x.shape) * self.infer_noise for i in indices]\n",
        "\n",
        "        preds = []\n",
        "        for iidx, i in enumerate(indices):\n",
        "          preds.append(softmax(self._classifiers[i].predict(np.float32(x + noise_list[iidx])), axis=1))\n",
        "        preds = np.array(preds)\n",
        "\n",
        "\n",
        "        del indices\n",
        "        del noise_list\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        if raw:\n",
        "            return preds\n",
        "\n",
        "        # 6 x 100\n",
        "        preds_classes = np.argmax(preds, axis=2)\n",
        "        row, col = preds_classes.shape\n",
        "\n",
        "        # 100,\n",
        "        majority_vote = np.array(\n",
        "            [\n",
        "             np.bincount(preds_classes[:,c]).argmax()\n",
        "             for c in range(col)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        mask = preds_classes == majority_vote\n",
        "        del majority_vote\n",
        "\n",
        "        mask = np.repeat(np.expand_dims(mask, axis=2), repeats=10, axis=2)\n",
        "\n",
        "        # Aggregate predictions only at probabilities level, as logits are not comparable between models\n",
        "        var_z = np.sum(mask * preds, axis=0) / np.sum(mask, axis=0) # take mean (check sum to 1)\n",
        "        del mask\n",
        "        del preds\n",
        "\n",
        "        # Apply postprocessing\n",
        "        predictions = self._apply_postprocessing(preds=var_z, fit=False)\n",
        "        del var_z\n",
        "\n",
        "        return predictions\n",
        "\n",
        "\n",
        "    def loss_gradient(self, x: np.ndarray, y: np.ndarray, raw: bool = False, **kwargs) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute the gradient of the loss function w.r.t. `x`.\n",
        "        :param x: Sample input with shape as expected by the model.\n",
        "        :param y: Target values (class labels) one-hot-encoded of shape (nb_samples, nb_classes) or indices of shape\n",
        "                  (nb_samples,).\n",
        "        :param raw: Return the individual classifier raw outputs (not aggregated).\n",
        "        :return: Array of gradients of the same shape as `x`. If `raw=True`, shape becomes `[nb_classifiers, x.shape]`.\n",
        "        \"\"\"\n",
        "\n",
        "        indices = np.random.choice(self.num_models, self.num_selected_models, replace=True)\n",
        "\n",
        "        noise_list = [np.random.randn(*x.shape) * self.infer_noise for i in indices]\n",
        "        \n",
        "        grads = np.array(\n",
        "            [\n",
        "                self._classifiers[i].loss_gradient(np.float32(x + noise_list[iidx]), y)\n",
        "                for iidx, i in enumerate(indices)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        del indices\n",
        "        del noise_list\n",
        "\n",
        "        if raw:\n",
        "            return grads\n",
        "\n",
        "        return np.sum(grads, axis=0)\n",
        "\n",
        "    def loss_gradient_framework(self, x: \"torch.Tensor\", y: \"torch.Tensor\", **kwargs) -> \"torch.Tensor\":\n",
        "        \"\"\"\n",
        "        Compute the gradient of the loss function w.r.t. `x`.\n",
        "        :param x: Sample input with shape as expected by the model.\n",
        "        :param y: Target values (class labels) one-hot-encoded of shape (nb_samples, nb_classes) or indices of shape\n",
        "                  (nb_samples,).\n",
        "        :param raw: Return the individual classifier raw outputs (not aggregated).\n",
        "        :return: Array of gradients of the same shape as `x`. If `raw=True`, shape becomes `[nb_classifiers, x.shape]`.\n",
        "        \"\"\"\n",
        "\n",
        "        indices = np.random.choice(self.num_models, self.n_loss, replace=True)\n",
        "\n",
        "        noise_list = [(torch.randn(x.size()) * self.infer_noise).to(device) for i in indices]\n",
        "\n",
        "\n",
        "        # gradient shape: batch_size, 3, 224, 224\n",
        "        accumulator = torch.zeros(test_batchsize, 3, img_size, img_size)\n",
        "        for iidx, i in enumerate(indices):\n",
        "          accumulator += self._classifiers[i].loss_gradient_framework(x + noise_list[iidx], y).cpu()\n",
        "        \n",
        "        torch.cuda.empty_cache()\n",
        "        del indices\n",
        "        del noise_list\n",
        "\n",
        "        return accumulator.to(device)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80wKHjAJV9EF"
      },
      "source": [
        "# Experiment Settings\n",
        "### common settings:\n",
        "*   inference noise: 0.1\n",
        "*   eps: 0.5\n",
        "*   eps step: 0.4\n",
        "*   max iter: 20\n",
        "*   L2 norm\n",
        "*   number of samplings: 50\n",
        "\n",
        "### to experiment:\n",
        "1. Total number of candidate models in the ensemble: 1 (randomized smoothing)\n",
        "2. Total number of candidate models in the ensemble: 5\n",
        "3. Total number of candidate models in the ensemble: 9\n",
        "4. Total number of candidate models in the ensemble: 13"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qgPgn2hXwzB"
      },
      "source": [
        "# Run this code block to load common variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge6pmhN3X51_"
      },
      "source": [
        "infer_noise = 0.1\n",
        "eps = 0.5\n",
        "eps_step = 0.4\n",
        "max_iter = 20\n",
        "norm = 2"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ2kzTjNYcg4"
      },
      "source": [
        "# Load 5 models all"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzefdlNUYbx5"
      },
      "source": [
        "model_names = ['ShuffleNet_1',\n",
        "               'ShuffleNet_2',\n",
        "               'ShuffleNet_3',\n",
        "               'ShuffleNet_4',\n",
        "               'ShuffleNet_5']"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUxyuajA8ugH"
      },
      "source": [
        "Kaichen\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKdVEVNp7U3-"
      },
      "source": [
        "n_prediction=5\n",
        "n_loss=10,"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3hWCLU283Dq"
      },
      "source": [
        "Shivani"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yixba3i85Ba"
      },
      "source": [
        "n_prediction=10\n",
        "n_loss=5"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D7HuN4sY5cz"
      },
      "source": [
        "# Kick off Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdUjTkdWBdgh"
      },
      "source": [
        "# **The rest parts of the notebook was regarding previous work. Discard them for now.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWETm6HGE1b4",
        "outputId": "191466ce-acd2-44b9-f266-9a9e3f2f09f2"
      },
      "source": [
        "# load models\n",
        "model_dict = {}\n",
        "for model_name in model_names:\n",
        "  model = ShuffleNet()\n",
        "  # MAKE CHANGE\n",
        "  model_data = torch.load('/content/gdrive/MyDrive/ShuffleNetModels/{}'.format(model_name), map_location=torch.device('cpu'))\n",
        "  model.load_state_dict(model_data['model_state_dict'])\n",
        "  model = model.to(device)\n",
        "  model_dict[model_name] = model\n",
        "\n",
        "logging.info(\"Models loaded successfully!\")\n",
        "logging.info(\"Total number of models: {}\".format(len(model_names)))\n",
        "\n",
        "# create classifier list\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "classifier_list = []\n",
        "for model_name in model_dict:\n",
        "  classifier_list.append(PyTorchClassifier(model=model_dict[model_name],\n",
        "                                           clip_values=(0, 1),\n",
        "                                           loss=criterion,\n",
        "                                           input_shape=(3, img_size, img_size),\n",
        "                                           nb_classes=nb_classes))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-13 21:18:26,460 INFO Models loaded successfully!\n",
            "2020-12-13 21:18:26,462 INFO Total number of models: 5\n",
            "2020-12-13 21:18:26,464 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-13 21:18:26,471 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-13 21:18:26,476 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-13 21:18:26,481 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-13 21:18:26,487 INFO Inferred 1 hidden layers on PyTorch classifier.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkSBhvN8FEvK"
      },
      "source": [
        "# create ensemble classifier\n",
        "my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
        "                                              device, \n",
        "                                              infer_noise=infer_noise,\n",
        "                                              num_selected_models=1,\n",
        "                                              n_prediction=n_prediction,\n",
        "                                              n_loss=n_loss,\n",
        "                                              clip_values=[0., 1.], \n",
        "                                              channels_first=True)\n",
        "\n",
        "# create attack object\n",
        "attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, \n",
        "                                         eps=eps, \n",
        "                                         eps_step=eps_step, \n",
        "                                         norm=norm, \n",
        "                                         max_iter=max_iter, \n",
        "                                         batch_size=test_batchsize)\n",
        "\n",
        "# kick off experiment\n",
        "Acc_adv = []\n",
        "Acc_nat = []\n",
        "for batch_idx, (X, Y) in enumerate(testloader):\n",
        "  x_test = X.numpy()\n",
        "  y_test = Y.numpy()\n",
        "  predictions_nat = my_ensemble_classifier.predict(x_test)\n",
        "  accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
        "  Acc_nat.append(accuracy_nat)\n",
        "\n",
        "  x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "  predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
        "  accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
        "  Acc_adv.append(accuracy_adv)\n",
        "\n",
        "  logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
        "  logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
        "    \n",
        "\n",
        "# saving the dataframe \n",
        "Acc_adv = np.asanyarray(Acc_adv)\n",
        "Acc_nat = np.asanyarray(Acc_nat)\n",
        "res = {'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
        "df = pd.DataFrame(res)\n",
        "\n",
        "#### MAKE CHANGE\n",
        "df.to_csv('./gdrive/My Drive/IDL_Project/results/result_raphaelC_1avg_20{}models_{}samples.csv'.format(len(model_names), num_selected_models),index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zcdVgQe9Sjq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}