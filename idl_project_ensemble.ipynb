{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of Copy of Copy of idl_project_ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "J0oc512o9c4d",
        "IffxeALQ_ajk",
        "inhJeBV-9ic0"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5ef081c59ed04995add8fccc2bd812a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9a2abaeb36774ef2be2975e239d49d1f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c93721be1cd744abb180cc7e0ff12cb8",
              "IPY_MODEL_b40a68816f8645a2a4d1b5dde4807602"
            ]
          }
        },
        "9a2abaeb36774ef2be2975e239d49d1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c93721be1cd744abb180cc7e0ff12cb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_33dc6599f418483fae46fa73edcd4d04",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9c3aeff16912417385c5ea8a1a26f676"
          }
        },
        "b40a68816f8645a2a4d1b5dde4807602": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dab12bbcd3f54895b134c7a835909ddc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 30213166.48it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a938158194e44b5587a0ff297c59acc4"
          }
        },
        "33dc6599f418483fae46fa73edcd4d04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9c3aeff16912417385c5ea8a1a26f676": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dab12bbcd3f54895b134c7a835909ddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a938158194e44b5587a0ff297c59acc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaichengDING/Triple-Defense/blob/main/idl_project_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8idp0UC0Oj6"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ESeViSYxQfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89cff3e2-4587-46db-fb07-68f6475f9c05"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgPpHa2d0Rt4"
      },
      "source": [
        "# Install ART"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzKNCLpGws5x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2070b116-9844-4996-dad8-8764fefb91c1"
      },
      "source": [
        "!pip install adversarial-robustness-toolbox==1.4.3"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting adversarial-robustness-toolbox==1.4.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/aa/1136628240b23de5c311aa1595d327f3908296a71f72093370a0459b8f0b/adversarial_robustness_toolbox-1.4.3-py3-none-any.whl (765kB)\n",
            "\u001b[K     |████████████████████████████████| 768kB 12.7MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.22.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/7f/366dcba1ba076a88a50bea732dbc033c0c5bbf7876010e6edc67948579d5/scikit_learn-0.22.2-cp36-cp36m-manylinux1_x86_64.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 31.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (0.10.2)\n",
            "Collecting ffmpeg-python\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/0c/56be52741f75bad4dc6555991fabd2e07b432d333da82c11ad701123888a/ffmpeg_python-0.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (1.4.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (7.0.0)\n",
            "Requirement already satisfied: resampy in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (0.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (3.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (50.3.2)\n",
            "Collecting pydub\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/d1/fbfa79371a8cd9bb15c2e3c480d7e6e340ed5cc55005174e16f48418333a/pydub-0.24.1-py2.py3-none-any.whl\n",
            "Collecting mypy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/cb/cf5530d063e7e703e2fbec677bfba633de6e70fe44bc323deeaa27f273b8/mypy-0.790-cp36-cp36m-manylinux1_x86_64.whl (21.0MB)\n",
            "\u001b[K     |████████████████████████████████| 21.0MB 1.4MB/s \n",
            "\u001b[?25hCollecting cma\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/c0/0a1c41f7cad0a51e07991cf86423d0e6651d035f1fe7dcff48e8858848f2/cma-3.0.3-py2.py3-none-any.whl (230kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 75.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (4.41.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.22.2->adversarial-robustness-toolbox==1.4.3) (0.17.0)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels->adversarial-robustness-toolbox==1.4.3) (0.5.1)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from statsmodels->adversarial-robustness-toolbox==1.4.3) (1.1.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from ffmpeg-python->adversarial-robustness-toolbox==1.4.3) (0.16.0)\n",
            "Requirement already satisfied: numba>=0.32 in /usr/local/lib/python3.6/dist-packages (from resampy->adversarial-robustness-toolbox==1.4.3) (0.48.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox==1.4.3) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox==1.4.3) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox==1.4.3) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox==1.4.3) (2.4.7)\n",
            "Collecting typed-ast<1.5.0,>=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/ed/5459080d95eb87a02fe860d447197be63b6e2b5e9ff73c2b0a85622994f4/typed_ast-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (737kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 61.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from mypy->adversarial-robustness-toolbox==1.4.3) (3.7.4.3)\n",
            "Collecting mypy-extensions<0.5.0,>=0.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->statsmodels->adversarial-robustness-toolbox==1.4.3) (2018.9)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.32->resampy->adversarial-robustness-toolbox==1.4.3) (0.31.0)\n",
            "Installing collected packages: scikit-learn, ffmpeg-python, pydub, typed-ast, mypy-extensions, mypy, cma, adversarial-robustness-toolbox\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed adversarial-robustness-toolbox-1.4.3 cma-3.0.3 ffmpeg-python-0.2.0 mypy-0.790 mypy-extensions-0.4.3 pydub-0.24.1 scikit-learn-0.22.2 typed-ast-1.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL8u7HvJ0Xka"
      },
      "source": [
        "# Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SqUPdAKkDIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a416938-d426-4f34-b61d-c3c2ed1eea18"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import PIL\n",
        "\n",
        "import sys\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "from torch.utils import data\n",
        "\n",
        "from art.attacks.evasion import FastGradientMethod\n",
        "from art.attacks.evasion import ProjectedGradientDescentPyTorch\n",
        "from art.estimators.classification import PyTorchClassifier\n",
        "\n",
        "from art.estimators.classification import EnsembleClassifier\n",
        "from typing import List, Optional, Union, TYPE_CHECKING\n",
        "from art.estimators.classification.classifier import ClassifierNeuralNetwork\n",
        "from scipy.special import softmax\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import logging\n",
        "import datetime\n",
        "import random\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "cuda"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcOnXEFW1m4K"
      },
      "source": [
        "# Model Definition\n",
        "`ShuffleNet` and `ShuffleNetV2` are actually identical, but models with seed number smaller than 200 are created using ShuffleNet and models with seed number greater than 200 are created using ShuffleNetV2. In order to load all models into ensemble, the two definitions are provided here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfyaOczXoRqQ"
      },
      "source": [
        "class ShuffleNet(nn.Module):\n",
        "    def __init__(self, nb_classes =10):\n",
        "        super(ShuffleNet, self).__init__()\n",
        "        self.shuffle = models.shufflenet_v2_x2_0()\n",
        "        self.linear = nn.Linear(1000, nb_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.shuffle(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ShuffleNetV2(nn.Module):\n",
        "    def __init__(self, nb_classes=10):\n",
        "        super(ShuffleNetV2, self).__init__()\n",
        "        self.shufflenet = models.shufflenet_v2_x2_0()\n",
        "        self.linear = nn.Linear(1000, nb_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.shufflenet(x)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlygMiHF0iSB"
      },
      "source": [
        "# Logging Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXX4443Y16Ul"
      },
      "source": [
        "# configure logging\n",
        "logger = logging.getLogger(\"\")\n",
        "\n",
        "# reset handler\n",
        "for handler in logging.root.handlers[:]:\n",
        "  logging.root.removeHandler(handler)\n",
        "\n",
        "# set handler\n",
        "stream_hdlr = logging.StreamHandler()\n",
        "# logging on colab\n",
        "file_hdlr = logging.FileHandler('/content/gdrive/My Drive/IDL_Project/logs/log_{}.log'.format(datetime.datetime.now()))\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
        "stream_hdlr.setFormatter(formatter)\n",
        "file_hdlr.setFormatter(formatter)\n",
        "\n",
        "logger.addHandler(stream_hdlr)\n",
        "logger.addHandler(file_hdlr)\n",
        "\n",
        "logger.setLevel(logging.INFO)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFiB-FbgT9sk"
      },
      "source": [
        "# Get Test Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk68nikXxa4G"
      },
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, X, Y, transform=None):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    self.transform = transform\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.Y)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    if self.transform is None:\n",
        "      return torch.from_numpy(self.X[idx]), torch.tensor(self.Y[idx]).long()\n",
        "    else:\n",
        "      return self.transform(self.X[idx]), torch.tensor(self.Y[idx]).long()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV2pPKKzeESx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "5ef081c59ed04995add8fccc2bd812a5",
            "9a2abaeb36774ef2be2975e239d49d1f",
            "c93721be1cd744abb180cc7e0ff12cb8",
            "b40a68816f8645a2a4d1b5dde4807602",
            "33dc6599f418483fae46fa73edcd4d04",
            "9c3aeff16912417385c5ea8a1a26f676",
            "dab12bbcd3f54895b134c7a835909ddc",
            "a938158194e44b5587a0ff297c59acc4"
          ]
        },
        "outputId": "6b8ee0eb-8e57-48cc-8f8a-cd3a2ffd4545"
      },
      "source": [
        "test_batchsize = 200\n",
        "num_workers = 4\n",
        "img_size = 224\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToPILImage(),\n",
        "                                     transforms.Resize(size=img_size),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0, 0, 0), (1, 1, 1))])\n",
        "\n",
        "testset= torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "testset_data = testset.data[0:1000]\n",
        "testset_labels = testset.targets[0:1000]\n",
        "\n",
        "testset_sub = MyDataset(testset_data, testset_labels, transform=test_transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset_sub, batch_size=test_batchsize, shuffle=False, num_workers=num_workers, drop_last=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ef081c59ed04995add8fccc2bd812a5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOPbwP7FTWEY"
      },
      "source": [
        "# Load Saved Models\n",
        "All saved models will be loaded into `model_dict`. Models with seed number smaller than 200 belong to `ShuffleNet` class, and models with seed number greater than 200 belong to `ShuffleNetV2` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUiLYb1EeNtS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65db38bc-28f5-4ac8-a8f0-3b685fe90b32"
      },
      "source": [
        "# models with 0.1 training noise\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "# model_names = ['ShuffleNet_1',\n",
        "#                'ShuffleNet_2',\n",
        "#                'ShuffleNet_3',\n",
        "#                'ShuffleNet_4',\n",
        "#                'ShuffleNet_5',\n",
        "#                'ShuffleNet_6',\n",
        "#                'ShuffleNet_7',\n",
        "#                'ShuffleNet_8',\n",
        "#                'ShuffleNet_31',\n",
        "#                'ShuffleNet_32',\n",
        "#                'ShuffleNet_33',\n",
        "#                'ShuffleNet_34',\n",
        "#                'ShuffleNet_35',\n",
        "#                'ShuffleNet_36',\n",
        "#                'ShuffleNet_37',\n",
        "#                'ShuffleNet_38',\n",
        "#                'ShuffleNet_39',\n",
        "#                'ShuffleNet_61',\n",
        "#                'ShuffleNet_62',\n",
        "#                'ShuffleNet_63',\n",
        "#                'ShuffleNet_64',\n",
        "#                'ShuffleNet_65',\n",
        "#                'ShuffleNet_66',\n",
        "#                'ShuffleNet_67',\n",
        "#                'ShuffleNet_68',\n",
        "#                'ShuffleNet_69',\n",
        "#                'ShuffleNet_70',\n",
        "#                'ShuffleNet_71',\n",
        "#                'ShuffleNet_72',\n",
        "#                'ShuffleNet_73']\n",
        "\n",
        "# model_names_v2 = ['ShuffleNet_200', \n",
        "#                   'ShuffleNet_201',\n",
        "#                   'ShuffleNet_202',\n",
        "#                   'ShuffleNet_203',\n",
        "#                   'ShuffleNet_204',\n",
        "#                   'ShuffleNet_205',\n",
        "#                   'ShuffleNet_206',\n",
        "#                   'ShuffleNet_207',\n",
        "#                   'ShuffleNet_208',\n",
        "#                   'ShuffleNet_209']\n",
        "\n",
        "\n",
        "model_names = ['ShuffleNet_1',\n",
        "               'ShuffleNet_2']\n",
        "model_names_v2 = []   \n",
        "\n",
        "model_dict = {}\n",
        "\n",
        "for model_name in model_names:\n",
        "  model = ShuffleNet()\n",
        "  model_data = torch.load('/content/gdrive/My Drive/IDL_Project/modelS/{}'.format(model_name), map_location=torch.device('cpu'))\n",
        "  model.load_state_dict(model_data['model_state_dict'])\n",
        "  model = model.to(device)\n",
        "  model_dict[model_name] = model\n",
        "\n",
        "for model_name in model_names_v2:\n",
        "  model = ShuffleNetV2()\n",
        "  model_data = torch.load('/content/gdrive/My Drive/IDL_Project/modelS/{}'.format(model_name), map_location=torch.device('cpu'))\n",
        "  model.load_state_dict(model_data['model_state_dict'])\n",
        "  model = model.to(device)\n",
        "  model_dict[model_name] = model\n",
        "\n",
        "logging.info(\"Total number of models: {}\".format(len(model_names) + len(model_names_v2)))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-02 20:28:34,430 INFO Total number of models: 2\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inhJeBV-9ic0"
      },
      "source": [
        "# MyEnsembleClassifier\n",
        "Inherited from `EnsembleClassifier` with `predict`, `loss_gradient` and `loss_gradient_framework` overwritten\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ratuA8nTIKyu"
      },
      "source": [
        "class MyEnsembleClassifier(EnsembleClassifier):\n",
        "    def __init__(\n",
        "        self,\n",
        "        classifiers: List[ClassifierNeuralNetwork],\n",
        "        device,\n",
        "        infer_noise,\n",
        "        num_selected_models,\n",
        "        classifier_weights: Union[list, np.ndarray, None] = None,\n",
        "        channels_first: bool = False,\n",
        "        clip_values: Optional[\"CLIP_VALUES_TYPE\"] = None,\n",
        "        preprocessing_defences: Union[\"Preprocessor\", List[\"Preprocessor\"], None] = None,\n",
        "        postprocessing_defences: Union[\"Postprocessor\", List[\"Postprocessor\"], None] = None,\n",
        "        preprocessing: \"PREPROCESSING_TYPE\" = (0, 1),\n",
        "    ) -> None:\n",
        "      super().__init__(\n",
        "          classifiers=classifiers,\n",
        "          classifier_weights=classifier_weights,\n",
        "          channels_first=channels_first,\n",
        "          clip_values=clip_values,\n",
        "          preprocessing_defences=preprocessing_defences,\n",
        "          postprocessing_defences=postprocessing_defences,\n",
        "          preprocessing=preprocessing\n",
        "      )\n",
        "      self.device = device\n",
        "      self.infer_noise = infer_noise\n",
        "      self.num_models = len(classifiers)\n",
        "      self.num_selected_models = num_selected_models\n",
        "\n",
        "    def predict(self, x: np.ndarray, batch_size: int = 128, raw: bool = False, **kwargs) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Perform prediction for a batch of inputs. Predictions from classifiers should only be aggregated if they all\n",
        "        have the same type of output (e.g., probabilities). Otherwise, use `raw=True` to get predictions from all\n",
        "        models without aggregation. The same option should be used for logits output, as logits are not comparable\n",
        "        between models and should not be aggregated.\n",
        "        :param x: Test set.\n",
        "        :param batch_size: Size of batches.\n",
        "        :param raw: Return the individual classifier raw outputs (not aggregated).\n",
        "        :return: Array of predictions of shape `(nb_inputs, nb_classes)`, or of shape\n",
        "                 `(nb_classifiers, nb_inputs, nb_classes)` if `raw=True`.\n",
        "        \"\"\"\n",
        "        # indices = [i for i in range(self.num_models)]\n",
        "        # random.shuffle(indices)\n",
        "        # indices = indices[:self.num_selected_models]\n",
        "\n",
        "        # selected_classifiers = random.choice(self._classifiers, k=self.num_selected_models)\n",
        "\n",
        "        indices = np.random.choice(self.num_models, self.num_selected_models, replace=True)\n",
        "\n",
        "        noise_list = [np.random.randn(*x.shape) * self.infer_noise for i in indices]\n",
        "\n",
        "        # replace with for loop\n",
        "        preds = np.array(\n",
        "            [softmax(self._classifiers[i].predict(np.float32(x + noise_list[iidx])), axis=1) for iidx, i in enumerate(indices)]\n",
        "        )\n",
        "\n",
        "        if raw:\n",
        "            return preds\n",
        "\n",
        "        # 6 x 100\n",
        "        preds_classes = np.argmax(preds, axis=2)\n",
        "        row, col = preds_classes.shape\n",
        "\n",
        "        # 100,\n",
        "        majority_vote = np.array(\n",
        "            [\n",
        "             np.bincount(preds_classes[:,c]).argmax()\n",
        "             for c in range(col)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        mask = preds_classes == majority_vote\n",
        "        mask = np.repeat(np.expand_dims(mask, axis=2), repeats=10, axis=2)\n",
        "\n",
        "        # Aggregate predictions only at probabilities level, as logits are not comparable between models\n",
        "        var_z = np.sum(mask * preds, axis=0) / np.sum(mask, axis=0) # take mean (check sum to 1)\n",
        "\n",
        "        # Apply postprocessing\n",
        "        predictions = self._apply_postprocessing(preds=var_z, fit=False)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "\n",
        "    def loss_gradient(self, x: np.ndarray, y: np.ndarray, raw: bool = False, **kwargs) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute the gradient of the loss function w.r.t. `x`.\n",
        "        :param x: Sample input with shape as expected by the model.\n",
        "        :param y: Target values (class labels) one-hot-encoded of shape (nb_samples, nb_classes) or indices of shape\n",
        "                  (nb_samples,).\n",
        "        :param raw: Return the individual classifier raw outputs (not aggregated).\n",
        "        :return: Array of gradients of the same shape as `x`. If `raw=True`, shape becomes `[nb_classifiers, x.shape]`.\n",
        "        \"\"\"\n",
        "\n",
        "        # indices = [i for i in range(self.num_models)]\n",
        "        # random.shuffle(indices)\n",
        "        # indices = indices[:self.num_selected_models]\n",
        "\n",
        "        indices = np.random.choice(self.num_models, self.num_selected_models, replace=True)\n",
        "\n",
        "        noise_list = [np.random.randn(*x.shape) * self.infer_noise for i in indices]\n",
        "        \n",
        "        grads = np.array(\n",
        "            [\n",
        "                self._classifiers[i].loss_gradient(np.float32(x + noise_list[iidx]), y)\n",
        "                for iidx, i in enumerate(indices)\n",
        "            ]\n",
        "        )\n",
        "        if raw:\n",
        "            return grads\n",
        "\n",
        "        return np.sum(grads, axis=0)\n",
        "\n",
        "    def loss_gradient_framework(self, x: \"torch.Tensor\", y: \"torch.Tensor\", **kwargs) -> \"torch.Tensor\":\n",
        "        \"\"\"\n",
        "        Compute the gradient of the loss function w.r.t. `x`.\n",
        "        :param x: Sample input with shape as expected by the model.\n",
        "        :param y: Target values (class labels) one-hot-encoded of shape (nb_samples, nb_classes) or indices of shape\n",
        "                  (nb_samples,).\n",
        "        :param raw: Return the individual classifier raw outputs (not aggregated).\n",
        "        :return: Array of gradients of the same shape as `x`. If `raw=True`, shape becomes `[nb_classifiers, x.shape]`.\n",
        "        \"\"\"\n",
        "\n",
        "        # indices = [i for i in range(self.num_models)]\n",
        "        # random.shuffle(indices)\n",
        "        # indices = indices[:self.num_selected_models]\n",
        "        \n",
        "        indices = np.random.choice(self.num_models, self.num_selected_models, replace=True)\n",
        "\n",
        "        noise_list = [(torch.randn(x.size()) * self.infer_noise).to(device) for i in indices]\n",
        "\n",
        "        # try detach here\n",
        "        # grads = [\n",
        "        #           self._classifiers[i].loss_gradient_framework(x + noise_list[iidx], y).unsqueeze(0)\n",
        "        #           for iidx, i in enumerate(indices)\n",
        "        #         ]\n",
        "\n",
        "        # grads = []\n",
        "        # gradient shape: batch_size, 3, 224, 224\n",
        "        accumulator = torch.zeros(test_batchsize, 3, img_size, img_size)\n",
        "        for iidx, i in enumerate(indices):\n",
        "          accumulator += self._classifiers[i].loss_gradient_framework(x + noise_list[iidx], y).cpu()\n",
        "          # grads.append((self._classifiers[i].loss_gradient_framework(x + noise_list[iidx], y).unsqueeze(0)).cpu())\n",
        "        \n",
        "        # return torch.sum(torch.cat(grads, axis=0), axis=0).to(device)\n",
        "        return accumulator.to(device)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKzm9D8h5bFR"
      },
      "source": [
        "# Creating classifier list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z0e7i6HJi7i",
        "outputId": "c2e48b0b-7349-4f7c-bf45-57034893b652",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nb_classes = 10\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "classifier_list = []\n",
        "for model_name in model_dict:\n",
        "  classifier_list.append(PyTorchClassifier(\n",
        "    model=model_dict[model_name],\n",
        "    clip_values=(0, 1),\n",
        "    loss=criterion,\n",
        "    input_shape=(3, img_size, img_size),\n",
        "    nb_classes=nb_classes,\n",
        "))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-02 20:41:45,509 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 20:41:45,518 INFO Inferred 1 hidden layers on PyTorch classifier.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emCbgIPQ5yA5"
      },
      "source": [
        "# Experiment Settings\n",
        "Refer to [this google doc](https://docs.google.com/document/d/1VaS5THALdudd_63Zv7ZR2u8rj8r8iDF6xgXzzLHXR8w/edit)\n",
        "\n",
        "1.   `num_selected_models`=15, `infer_noise`=[0.0025, 0.005, 0.01, 0.05, 0.08, 0.1], `eps`=[0.0025, 0.01, 0.1, 0.5, 0.8], `norm`=Linf\n",
        "2.   `num_selected_models`=[5, 10, 15, 20, 25, 30, 35, 40], `infer_noise`=0.1, `eps`=[0.0025, 0.01, 0.1, 0.5, 0.8], `norm`=Linf\n",
        "3.   `num_selected_models`=15, `infer_noise`=[0.0025, 0.005, 0.01, 0.05, 0.08, 0.1], `eps`=[0.1, 0.5, 0.8, 0.9, 1], `norm`=L2\n",
        "4.   `num_selected_models`=[5, 10, 15, 20, 25, 30, 35, 40], `infer_noise`=0.1, `eps`=[0.1, 0.5, 0.8, 0.9, 1], `norm`=L2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfpLU-YJlx92"
      },
      "source": [
        "### Arbitrary Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "64xN40H5nqHR",
        "outputId": "eeed3282-8187-4043-da50-b1559876ae47"
      },
      "source": [
        "num_selected_models = 200\n",
        "norm = np.inf\n",
        "infer_noise = 0.1\n",
        "eps = 8/255\n",
        "logging.info(\"Current experiment setting: eps={}, inference noise={}\".format(eps, infer_noise))\n",
        "my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
        "                                              device, \n",
        "                                              infer_noise=infer_noise,\n",
        "                                              num_selected_models=num_selected_models,\n",
        "                                              clip_values=[0., 1.], \n",
        "                                              channels_first=True)\n",
        "attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=eps, eps_step=0.025, norm=norm, max_iter=20, batch_size=test_batchsize)\n",
        "Acc_adv = []\n",
        "Acc_nat = []\n",
        "for batch_idx, (X, Y) in enumerate(testloader):\n",
        "  x_test = X.numpy()\n",
        "  y_test = Y.numpy()\n",
        "  predictions_nat = my_ensemble_classifier.predict(x_test)\n",
        "  accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
        "  Acc_nat.append(accuracy_nat)\n",
        "\n",
        "  x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "  predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
        "  accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
        "  Acc_adv.append(accuracy_adv)\n",
        "      \n",
        "  logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
        "  logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
        "    \n",
        "Acc_adv = np.asanyarray(Acc_adv)\n",
        "Acc_nat = np.asanyarray(Acc_nat)\n",
        "idx = np.arange(0, len(testset)/test_batchsize).tolist()\n",
        "# res = {'Id': idx ,'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
        "# df = pd.DataFrame(res) \n",
        "# # saving the dataframe \n",
        "# df.to_csv('./gdrive/My Drive/IDL_Project/results/PGD_exp1_infer_noise{}_eps{}.csv'.format(infer_noise, eps),index=False)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-02 20:41:50,285 INFO Current experiment setting: eps=0.03137254901960784, inference noise=0.1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-8774e12c5ea6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mpredictions_nat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_ensemble_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0maccuracy_nat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_nat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mAcc_nat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_nat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/art/estimators/classification/classifier.py\u001b[0m in \u001b[0;36mreplacement_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfunc_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mreplacement_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfunc_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-9be796de73fc>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, raw, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_selected_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mnoise_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_noise\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# replace with for loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-9be796de73fc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_selected_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mnoise_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_noise\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# replace with for loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/signal_handling.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 1291) is killed by signal: Killed. "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEzaTmhp7qFg"
      },
      "source": [
        "# Experiment 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIZS0qT17u3_"
      },
      "source": [
        "num_selected_models = 15\n",
        "norm = np.inf\n",
        "infer_noise_list = [0.0025, 0.005, 0.01, 0.05, 0.08, 0.1]\n",
        "eps_list = [0.0025, 0.01, 0.1, 0.5, 0.8]\n",
        "\n",
        "for infer_noise in infer_noise_list:\n",
        "  for eps in eps_list:\n",
        "    my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
        "                                                  device, \n",
        "                                                  infer_noise=infer_noise,\n",
        "                                                  num_selected_models=num_selected_models,\n",
        "                                                  clip_values=[0., 1.], \n",
        "                                                  channels_first=True)\n",
        "    attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=eps, eps_step=eps/3, norm=norm)\n",
        "    Acc_adv = []\n",
        "    Acc_nat = []\n",
        "    for batch_idx, (X, Y) in enumerate(testloader):\n",
        "      x_test = X.numpy()\n",
        "      y_test = Y.numpy()\n",
        "      predictions_nat = my_ensemble_classifier.predict(x_test)\n",
        "      accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
        "      Acc_nat.append(accuracy_nat)\n",
        "\n",
        "      x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "      predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
        "      accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
        "      Acc_adv.append(accuracy_adv)\n",
        "      \n",
        "      logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
        "      logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
        "    \n",
        "    Acc_adv = np.asanyarray(Acc_adv)\n",
        "    Acc_nat = np.asanyarray(Acc_nat)\n",
        "    idx = np.arange(0, len(testset)/test_batchsize).tolist()\n",
        "    res = {'Id': idx ,'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
        "    df = pd.DataFrame(res) \n",
        "    # saving the dataframe \n",
        "    df.to_csv('./gdrive/My Drive/IDL_Project/results/PGD_exp1_infer_noise{}_eps{}.csv'.format(infer_noise, eps),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH-xmYJL8n14"
      },
      "source": [
        "# Experiment 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e44AHWvw8qxI"
      },
      "source": [
        "num_selected_models_list = [5, 10, 15, 20, 25, 30, 35, 40]\n",
        "norm = np.inf\n",
        "infer_noise = 0.1\n",
        "eps_list = [0.0025, 0.01, 0.1, 0.5, 0.8]\n",
        "\n",
        "for num_selected_models in num_selected_models_list:\n",
        "  for eps in eps_list:\n",
        "    my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
        "                                                  device, \n",
        "                                                  infer_noise=infer_noise,\n",
        "                                                  num_selected_models=num_selected_models,\n",
        "                                                  clip_values=[0., 1.], \n",
        "                                                  channels_first=True)\n",
        "    attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=eps, eps_step=eps/3, norm=norm)\n",
        "    Acc_adv = []\n",
        "    Acc_nat = []\n",
        "    for batch_idx, (X, Y) in enumerate(testloader):\n",
        "      x_test = X.numpy()\n",
        "      y_test = Y.numpy()\n",
        "      x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "      predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
        "      predictions_nat = my_ensemble_classifier.predict(x_test)\n",
        "      accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
        "      accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
        "      Acc_adv.append(accuracy_adv)\n",
        "      Acc_nat.append(accuracy_nat)\n",
        "      logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
        "      logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
        "    \n",
        "    Acc_adv = np.asanyarray(Acc_adv)\n",
        "    Acc_nat = np.asanyarray(Acc_nat)\n",
        "    idx = np.arange(0, len(testset)/test_batchsize).tolist()\n",
        "    res = {'Id': idx ,'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
        "    df = pd.DataFrame(res) \n",
        "    # saving the dataframe \n",
        "    df.to_csv('./gdrive/My Drive/IDL_Project/results/PGD_exp2_N{}_eps{}.csv'.format(num_selected_models, eps),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng7Xe9Zq8rai"
      },
      "source": [
        "# Experiment 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3rzYvCu8uoa"
      },
      "source": [
        "num_selected_models = 15\n",
        "norm = 2\n",
        "infer_noise_list = [0.0025, 0.005, 0.01, 0.05, 0.08, 0.1]\n",
        "eps_list = [0.1, 0.5, 0.8, 0.9, 1]\n",
        "\n",
        "for infer_noise in infer_noise_list:\n",
        "  for eps in eps_list:\n",
        "    my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
        "                                                  device, \n",
        "                                                  infer_noise=infer_noise,\n",
        "                                                  num_selected_models=num_selected_models,\n",
        "                                                  clip_values=[0., 1.], \n",
        "                                                  channels_first=True)\n",
        "    attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=eps, eps_step=eps/3, norm=norm)\n",
        "    Acc_adv = []\n",
        "    Acc_nat = []\n",
        "    for batch_idx, (X, Y) in enumerate(testloader):\n",
        "      x_test = X.numpy()\n",
        "      y_test = Y.numpy()\n",
        "      x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "      predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
        "      predictions_nat = my_ensemble_classifier.predict(x_test)\n",
        "      accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
        "      accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
        "      Acc_adv.append(accuracy_adv)\n",
        "      Acc_nat.append(accuracy_nat)\n",
        "      logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
        "      logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
        "    \n",
        "    Acc_adv = np.asanyarray(Acc_adv)\n",
        "    Acc_nat = np.asanyarray(Acc_nat)\n",
        "    idx = np.arange(0, len(testset)/test_batchsize).tolist()\n",
        "    res = {'Id': idx ,'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
        "    df = pd.DataFrame(res) \n",
        "    # saving the dataframe \n",
        "    df.to_csv('./gdrive/My Drive/IDL_Project/results/PGD_exp3_infer_noise{}_eps{}.csv'.format(infer_noise, eps),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-w5HNiC8vDY"
      },
      "source": [
        "# Experiment 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTCAdeqj8w17"
      },
      "source": [
        "num_selected_models_list = [5, 10, 15, 20, 25, 30, 35, 40]\n",
        "norm = 2\n",
        "infer_noise = 0.1\n",
        "eps_list = [0.1, 0.5, 0.8, 0.9, 1]\n",
        "\n",
        "for num_selected_models in num_selected_models_list:\n",
        "  for eps in eps_list:\n",
        "    my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
        "                                                  device, \n",
        "                                                  infer_noise=infer_noise,\n",
        "                                                  num_selected_models=num_selected_models,\n",
        "                                                  clip_values=[0., 1.], \n",
        "                                                  channels_first=True)\n",
        "    attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=eps, eps_step=eps/3, norm=norm)\n",
        "    Acc_adv = []\n",
        "    Acc_nat = []\n",
        "    for batch_idx, (X, Y) in enumerate(testloader):\n",
        "      x_test = X.numpy()\n",
        "      y_test = Y.numpy()\n",
        "      x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "      predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
        "      predictions_nat = my_ensemble_classifier.predict(x_test)\n",
        "      accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
        "      accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
        "      Acc_adv.append(accuracy_adv)\n",
        "      Acc_nat.append(accuracy_nat)\n",
        "      logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
        "      logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
        "    \n",
        "    Acc_adv = np.asanyarray(Acc_adv)\n",
        "    Acc_nat = np.asanyarray(Acc_nat)\n",
        "    idx = np.arange(0, len(testset)/test_batchsize).tolist()\n",
        "    res = {'Id': idx ,'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
        "    df = pd.DataFrame(res) \n",
        "    # saving the dataframe \n",
        "    df.to_csv('./gdrive/My Drive/IDL_Project/results/PGD_exp4_N{}_eps{}.csv'.format(num_selected_models, eps),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdUjTkdWBdgh"
      },
      "source": [
        "# **The rest parts of the notebook was regarding previous work. Discard them for now.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP_Gxq-2jc-9"
      },
      "source": [
        "# FGM Attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmXkglUUKXO3"
      },
      "source": [
        "attack = FastGradientMethod(estimator=my_ensemble_classifier, eps=0.5, targeted=False, norm=2)\n",
        "Acc = []\n",
        "for batch_idx, (X, Y) in enumerate(testloader):\n",
        "    x_test = X.numpy()\n",
        "    y_test = Y.numpy()\n",
        "    # target = (y_test + 1) % 10\n",
        "    x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "    predictions = my_ensemble_classifier.predict(x_test_adv)\n",
        "    predictions_test = my_ensemble_classifier.predict(x_test)\n",
        "    accuracy = np.sum(np.argmax(predictions, axis=1) == y_test) / len(y_test)\n",
        "    acc_test = np.sum(np.argmax(predictions_test, axis=1) == y_test) / len(y_test)\n",
        "    # print(y_test.shape)\n",
        "    # acc_rate, coverage_rate = compute_accuracy(preds=predictions, labels=np.reshape(y_test, (test_batchsize, 1)))\n",
        "    # logging.info('acc rate: {}, coverage rate: {}'.format(acc_rate, coverage_rate))\n",
        "    Acc.append(accuracy)\n",
        "    logging.info('accuracy: {}'.format(accuracy))\n",
        "    logging.info('accuracy non adv: {}'.format(acc_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoYiKeGCjfDR"
      },
      "source": [
        "# PGD Attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZRM5XEZjchO"
      },
      "source": [
        "attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=0.5, eps_step=0.03)\n",
        "Acc = []\n",
        "for batch_idx, (X, Y) in enumerate(testloader):\n",
        "    x_test = X.numpy()\n",
        "    y_test = Y.numpy()\n",
        "    x_test_adv = attack.generate(torch.from_numpy(x_test))\n",
        "    predictions = my_ensemble_classifier.predict(x_test_adv)\n",
        "    accuracy = np.sum(np.argmax(predictions, axis=1) == np.array(y_test)) / len(y_test)\n",
        "    Acc.append(accuracy)\n",
        "    logging.info('accuracy: {}'.format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyMeGON8BFNL"
      },
      "source": [
        "# Experiment settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjUPKB3jvijF"
      },
      "source": [
        "fgm_epsilon = np.array([0.005, 0.01, 0.02, 0.04, 0.08])\n",
        "pgd_epsilon = 8.0 / 255.0 / np.array([2.0, 4.0, 8.0, 16.0])\n",
        "diff = np.array([1e-3, 1e-4, 1e-5, 1e-6])\n",
        "pgd_epsilon_step = pgd_epsilon - diff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XG44M27BNIT"
      },
      "source": [
        "Experiment on inference noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U79zJmCsDEKc"
      },
      "source": [
        "noise_stds = [0.02, 0.04, 0.06, 0.08, 0.1]\n",
        "for i in range(len(noise_stds)):\n",
        "  my_ensemble = MyEnsemble(model_dict, num_models_selected, noise_std=noise_stds[i], num_classes=10)\n",
        "  classifier = PyTorchClassifier(\n",
        "      model=my_ensemble,\n",
        "      clip_values=(0, 1),\n",
        "      loss=nn.CrossEntropyLoss(),\n",
        "      input_shape=(3, 224, 224),\n",
        "      nb_classes=10)\n",
        "  attack = FastGradientMethod(estimator=classifier, eps=0.01)\n",
        "  Acc = []\n",
        "  for batch_idx, (X, Y) in enumerate(testloader):\n",
        "    x_test = X.numpy()\n",
        "    y_test = Y.numpy()\n",
        "    x_test_adv = attack.generate(torch.from_numpy(x_test))\n",
        "    predictions = classifier.predict(x_test_adv)\n",
        "    accuracy = np.sum(np.argmax(predictions, axis=1) == np.array(y_test)) / len(y_test)\n",
        "    Acc.append(accuracy)\n",
        "    logging.info('accuracy: {}'.format(accuracy))\n",
        "  \n",
        "  logging.info(\"Accuracy for eps={}: {}\".format(fgm_epsilon[i], np.mean(np.array(Acc))))\n",
        "\n",
        "  pred_list = np.asanyarray(Acc)\n",
        "  idx = np.arange(0,len(testset)/test_batchsize).tolist()\n",
        "  dict = {'Id': idx ,'acc': pred_list} \n",
        "  df = pd.DataFrame(dict) \n",
        "  # saving the dataframe \n",
        "  df.to_csv('./gdrive/My Drive/IDL_Project/results/FGM_result_noise{}_{}.csv'.format(noise_stds[i], datetime.datetime.now()),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LotwWsVpMYKN"
      },
      "source": [
        "pred_list = np.asanyarray(Acc)\n",
        "idx = np.arange(0,len(testset)/test_batchsize).tolist()\n",
        "dict = {'Id': idx ,'acc': pred_list} \n",
        "df = pd.DataFrame(dict) \n",
        "# saving the dataframe \n",
        "df.to_csv('./gdrive/My Drive/IDL_Project/results/FGM_result_eps{}_{}.csv'.format(fgm_epsilon[-1], datetime.datetime.now()),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqIgjfhwBXVL"
      },
      "source": [
        "Plot PGD adversarial examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Eu7qdDQMnGq"
      },
      "source": [
        "attack = ProjectedGradientDescentPyTorch(estimator=classifier, eps=pgd_epsilon[0], eps_step=pgd_epsilon_step[0])\n",
        "Acc = []\n",
        "adv = []\n",
        "orig = []\n",
        "labels = []\n",
        "pred = []\n",
        "for batch_idx, (X, Y) in enumerate(testloader):\n",
        "  if batch_idx > 0:\n",
        "    break\n",
        "  x_test = X.numpy()\n",
        "  y_test = Y.numpy()\n",
        "  x_test_adv = attack.generate(torch.from_numpy(x_test))\n",
        "  predictions = classifier.predict(x_test_adv)\n",
        "  accuracy = np.sum(np.argmax(predictions, axis=1) == np.array(y_test)) / len(y_test)\n",
        "  Acc.append(accuracy)\n",
        "  adv.append(x_test_adv)\n",
        "  orig.append(x_test)\n",
        "  labels.append(y_test)\n",
        "  pred.append(predictions)\n",
        "  logging.info('accuracy: {}'.format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8sj2LdnNV5n"
      },
      "source": [
        "example_adv = adv[-1]\n",
        "example_orig = orig[-1]\n",
        "label = labels[-1]\n",
        "predi = np.argmax(predictions, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoD6QHEwNt4T"
      },
      "source": [
        "print(label)\n",
        "print(predi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnkXnHRePFeW"
      },
      "source": [
        "idx = 2\n",
        "selected_adv = example_adv[idx]\n",
        "selected_orig = example_orig[idx]\n",
        "selected_lab = label[idx]\n",
        "selected_pred = predi[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZuD1OfoWroX"
      },
      "source": [
        "print(selected_lab)\n",
        "print(selected_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkxSl4PTPab0"
      },
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(np.transpose(selected_orig, (1, 2, 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tpehq_0hVobG"
      },
      "source": [
        "plt.imshow(np.transpose(selected_adv, (1, 2, 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuLSJh8QVsHy"
      },
      "source": [
        "plt.imshow(10*(np.transpose(selected_orig, (1, 2, 0)) - np.transpose(selected_adv, (1, 2, 0))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lOaioJIBh1G"
      },
      "source": [
        "Experiment PGD attacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuxeEuf-8UVE"
      },
      "source": [
        "for i in range(len(pgd_epsilon)):\n",
        "  attack = FastGradientMethod(estimator=classifier, eps=fgm_epsilon[i])\n",
        "  Acc = []\n",
        "  for batch_idx, (X, Y) in enumerate(testloader):\n",
        "    x_test = X.numpy()\n",
        "    y_test = Y.numpy()\n",
        "    x_test_adv = attack.generate(torch.from_numpy(x_test))\n",
        "    predictions = classifier.predict(x_test_adv)\n",
        "    accuracy = np.sum(np.argmax(predictions, axis=1) == np.array(y_test)) / len(y_test)\n",
        "    Acc.append(accuracy)\n",
        "    logging.info('accuracy: {}'.format(accuracy))\n",
        "  \n",
        "  logging.info(\"Accuracy for eps={}: {}\".format(fgm_epsilon[i], np.mean(np.array(Acc))))\n",
        "\n",
        "  pred_list = np.asanyarray(Acc)\n",
        "  idx = np.arange(0,len(testset)/test_batchsize).tolist()\n",
        "  dict = {'Id': idx ,'acc': pred_list} \n",
        "  df = pd.DataFrame(dict) \n",
        "  # saving the dataframe \n",
        "  df.to_csv('./gdrive/My Drive/IDL_Project/results/FGM_result_eps{}_{}.csv'.format(fgm_epsilon[i], datetime.datetime.now()),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}