{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of Copy of Copy of idl_project_ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "J0oc512o9c4d",
        "IffxeALQ_ajk",
        "inhJeBV-9ic0"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "74fb884d13e940a99b6fd32f31195e97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7c55734aad0b417784bd2c4d45b769a0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_381513376c8740e88481ccda1e80a51d",
              "IPY_MODEL_87635946b9934355adeeb99d6eda6639"
            ]
          }
        },
        "7c55734aad0b417784bd2c4d45b769a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "381513376c8740e88481ccda1e80a51d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d73dd716933942378f0a76d2343b77f1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_199e147f8a444afe87f19aefd76e54f0"
          }
        },
        "87635946b9934355adeeb99d6eda6639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_72c5cbe3731b4d1c9493b483a4840d3f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/? [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c185786ae7404fb2b49c7b7f0c9b8940"
          }
        },
        "d73dd716933942378f0a76d2343b77f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "199e147f8a444afe87f19aefd76e54f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "72c5cbe3731b4d1c9493b483a4840d3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c185786ae7404fb2b49c7b7f0c9b8940": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "df9f08dd7f414827884ccf1f2401987c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9a7b17c1f7fd4163a6b49384ca64dc17",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f5641ae2e2e34f11ac6d9e2f7ac0abfa",
              "IPY_MODEL_d483977698404551a4bef464cc0c1b2a"
            ]
          }
        },
        "9a7b17c1f7fd4163a6b49384ca64dc17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f5641ae2e2e34f11ac6d9e2f7ac0abfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_43f5277c530241a3a6ae22e2c1f86296",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f62e1d72bf144dbd9b8125d59471572b"
          }
        },
        "d483977698404551a4bef464cc0c1b2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3f8e569f06e44b74afa0f4c661445e81",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:19&lt;00:00, 32740952.89it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_84c4423316e646d7bac59e76fc94735b"
          }
        },
        "43f5277c530241a3a6ae22e2c1f86296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f62e1d72bf144dbd9b8125d59471572b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3f8e569f06e44b74afa0f4c661445e81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "84c4423316e646d7bac59e76fc94735b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaichengDING/Triple-Defense/blob/kaicheng-branch/idl_project_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8idp0UC0Oj6"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ESeViSYxQfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "696e9633-8cb2-416f-a401-fd906039990c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgPpHa2d0Rt4"
      },
      "source": [
        "# Install ART"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzKNCLpGws5x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7ad7a40-ff78-472e-97d7-a35df5ab8b18"
      },
      "source": [
        "!pip install adversarial-robustness-toolbox==1.4.3"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting adversarial-robustness-toolbox==1.4.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/aa/1136628240b23de5c311aa1595d327f3908296a71f72093370a0459b8f0b/adversarial_robustness_toolbox-1.4.3-py3-none-any.whl (765kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 768kB 10.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (7.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (50.3.2)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (0.10.2)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (1.18.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (3.2.2)\n",
            "Collecting pydub\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/d1/fbfa79371a8cd9bb15c2e3c480d7e6e340ed5cc55005174e16f48418333a/pydub-0.24.1-py2.py3-none-any.whl\n",
            "Collecting cma\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/c0/0a1c41f7cad0a51e07991cf86423d0e6651d035f1fe7dcff48e8858848f2/cma-3.0.3-py2.py3-none-any.whl (230kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235kB 14.3MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.22.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/7f/366dcba1ba076a88a50bea732dbc033c0c5bbf7876010e6edc67948579d5/scikit_learn-0.22.2-cp36-cp36m-manylinux1_x86_64.whl (7.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.1MB 19.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (1.15.0)\n",
            "Collecting mypy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/cb/cf5530d063e7e703e2fbec677bfba633de6e70fe44bc323deeaa27f273b8/mypy-0.790-cp36-cp36m-manylinux1_x86_64.whl (21.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21.0MB 1.4MB/s \n",
            "\u001b[?25hCollecting ffmpeg-python\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/0c/56be52741f75bad4dc6555991fabd2e07b432d333da82c11ad701123888a/ffmpeg_python-0.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: resampy in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (0.2.2)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels->adversarial-robustness-toolbox==1.4.3) (0.5.1)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from statsmodels->adversarial-robustness-toolbox==1.4.3) (1.1.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox==1.4.3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox==1.4.3) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox==1.4.3) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox==1.4.3) (2.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.22.2->adversarial-robustness-toolbox==1.4.3) (0.17.0)\n",
            "Collecting typed-ast<1.5.0,>=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/ed/5459080d95eb87a02fe860d447197be63b6e2b5e9ff73c2b0a85622994f4/typed_ast-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (737kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 747kB 53.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from mypy->adversarial-robustness-toolbox==1.4.3) (3.7.4.3)\n",
            "Collecting mypy-extensions<0.5.0,>=0.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from ffmpeg-python->adversarial-robustness-toolbox==1.4.3) (0.16.0)\n",
            "Requirement already satisfied: numba>=0.32 in /usr/local/lib/python3.6/dist-packages (from resampy->adversarial-robustness-toolbox==1.4.3) (0.48.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->statsmodels->adversarial-robustness-toolbox==1.4.3) (2018.9)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.32->resampy->adversarial-robustness-toolbox==1.4.3) (0.31.0)\n",
            "Installing collected packages: pydub, cma, scikit-learn, typed-ast, mypy-extensions, mypy, ffmpeg-python, adversarial-robustness-toolbox\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed adversarial-robustness-toolbox-1.4.3 cma-3.0.3 ffmpeg-python-0.2.0 mypy-0.790 mypy-extensions-0.4.3 pydub-0.24.1 scikit-learn-0.22.2 typed-ast-1.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL8u7HvJ0Xka"
      },
      "source": [
        "# Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SqUPdAKkDIn"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import PIL\n",
        "\n",
        "import sys\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "from torch.utils import data\n",
        "\n",
        "from art.attacks.evasion import FastGradientMethod\n",
        "from art.attacks.evasion import ProjectedGradientDescentPyTorch\n",
        "from art.estimators.classification import PyTorchClassifier\n",
        "\n",
        "from art.estimators.classification import EnsembleClassifier\n",
        "from typing import List, Optional, Union, TYPE_CHECKING\n",
        "from art.estimators.classification.classifier import ClassifierNeuralNetwork\n",
        "from scipy.special import softmax\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import logging\n",
        "import datetime\n",
        "import random\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcOnXEFW1m4K"
      },
      "source": [
        "# Model Definition\n",
        "`ShuffleNet` and `ShuffleNetV2` are actually identical, but models with seed number smaller than 200 are created using ShuffleNet and models with seed number greater than 200 are created using ShuffleNetV2. In order to load all models into ensemble, the two definitions are provided here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfyaOczXoRqQ"
      },
      "source": [
        "class ShuffleNet(nn.Module):\n",
        "    def __init__(self, nb_classes =10):\n",
        "        super(ShuffleNet, self).__init__()\n",
        "        self.shuffle = models.shufflenet_v2_x2_0()\n",
        "        self.linear = nn.Linear(1000, nb_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.shuffle(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ShuffleNetV2(nn.Module):\n",
        "    def __init__(self, nb_classes=10):\n",
        "        super(ShuffleNetV2, self).__init__()\n",
        "        self.shufflenet = models.shufflenet_v2_x2_0()\n",
        "        self.linear = nn.Linear(1000, nb_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.shufflenet(x)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlygMiHF0iSB"
      },
      "source": [
        "# Logging Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXX4443Y16Ul"
      },
      "source": [
        "# configure logging\n",
        "logger = logging.getLogger(\"\")\n",
        "\n",
        "# reset handler\n",
        "for handler in logging.root.handlers[:]:\n",
        "  logging.root.removeHandler(handler)\n",
        "\n",
        "# set handler\n",
        "stream_hdlr = logging.StreamHandler()\n",
        "# logging on colab\n",
        "file_hdlr = logging.FileHandler('/content/gdrive/My Drive/IDL_Project/logs/log_{}.log'.format(datetime.datetime.now()))\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
        "stream_hdlr.setFormatter(formatter)\n",
        "file_hdlr.setFormatter(formatter)\n",
        "\n",
        "logger.addHandler(stream_hdlr)\n",
        "logger.addHandler(file_hdlr)\n",
        "\n",
        "logger.setLevel(logging.INFO)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFiB-FbgT9sk"
      },
      "source": [
        "# Get Test Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG5oJYZkah6n"
      },
      "source": [
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0.0, std=1.0):\n",
        "      self.std = std\n",
        "      self.mean = mean\n",
        "        \n",
        "    def __call__(self, tensor):\n",
        "      noise = torch.randn(tensor.size()) * self.std + self.mean\n",
        "      res = tensor + noise\n",
        "      return torch.clamp(input=res, min=0.0, max=1.0)\n",
        "    \n",
        "    def __repr__(self):\n",
        "      return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk68nikXxa4G"
      },
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, X, Y, transform=None):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    self.transform = transform\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.Y)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    if self.transform is None:\n",
        "      return torch.from_numpy(self.X[idx]), torch.tensor(self.Y[idx]).long()\n",
        "    else:\n",
        "      return self.transform(self.X[idx]), torch.tensor(self.Y[idx]).long()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV2pPKKzeESx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151,
          "referenced_widgets": [
            "74fb884d13e940a99b6fd32f31195e97",
            "7c55734aad0b417784bd2c4d45b769a0",
            "381513376c8740e88481ccda1e80a51d",
            "87635946b9934355adeeb99d6eda6639",
            "d73dd716933942378f0a76d2343b77f1",
            "199e147f8a444afe87f19aefd76e54f0",
            "72c5cbe3731b4d1c9493b483a4840d3f",
            "c185786ae7404fb2b49c7b7f0c9b8940",
            "df9f08dd7f414827884ccf1f2401987c",
            "9a7b17c1f7fd4163a6b49384ca64dc17",
            "f5641ae2e2e34f11ac6d9e2f7ac0abfa",
            "d483977698404551a4bef464cc0c1b2a",
            "43f5277c530241a3a6ae22e2c1f86296",
            "f62e1d72bf144dbd9b8125d59471572b",
            "3f8e569f06e44b74afa0f4c661445e81",
            "84c4423316e646d7bac59e76fc94735b"
          ]
        },
        "outputId": "68a59792-15b5-4615-ce5f-0f2ed5a3e68a"
      },
      "source": [
        "test_batchsize = 200\n",
        "num_workers = 4\n",
        "nb_classes = 10\n",
        "img_size = 224\n",
        "subset_size = 2000\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToPILImage(),\n",
        "                                     transforms.Resize(size=img_size),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0, 0, 0), (1, 1, 1))])\n",
        "\n",
        "testset= torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "testset_data = testset.data[0:subset_size]\n",
        "testset_labels = testset.targets[0:subset_size]\n",
        "\n",
        "testset_sub = MyDataset(testset_data, testset_labels, transform=test_transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset_sub, batch_size=test_batchsize, shuffle=False, num_workers=num_workers, drop_last=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74fb884d13e940a99b6fd32f31195e97",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Failed download. Trying https -> http instead. Downloading http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df9f08dd7f414827884ccf1f2401987c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOPbwP7FTWEY"
      },
      "source": [
        "# Load Saved Models (ignore for now)\n",
        "All saved models will be loaded into `model_dict`. Models with seed number smaller than 200 belong to `ShuffleNet` class, and models with seed number greater than 200 belong to `ShuffleNetV2` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUiLYb1EeNtS"
      },
      "source": [
        "# models with 0.1 training noise\n",
        "# model_names = ['ShuffleNet_1',\n",
        "#                'ShuffleNet_2',\n",
        "#                'ShuffleNet_3',\n",
        "#                'ShuffleNet_4',\n",
        "#                'ShuffleNet_5',\n",
        "#                'ShuffleNet_6',\n",
        "#                'ShuffleNet_7',\n",
        "#                'ShuffleNet_8',\n",
        "#                'ShuffleNet_31',\n",
        "#                'ShuffleNet_32',\n",
        "#                'ShuffleNet_33',\n",
        "#                'ShuffleNet_34',\n",
        "#                'ShuffleNet_35',\n",
        "#                'ShuffleNet_36',\n",
        "#                'ShuffleNet_37',\n",
        "#                'ShuffleNet_38',\n",
        "#                'ShuffleNet_39',\n",
        "#                'ShuffleNet_61',\n",
        "#                'ShuffleNet_62',\n",
        "#                'ShuffleNet_63',\n",
        "#                'ShuffleNet_64',\n",
        "#                'ShuffleNet_65',\n",
        "#                'ShuffleNet_66',\n",
        "#                'ShuffleNet_67',\n",
        "#                'ShuffleNet_68',\n",
        "#                'ShuffleNet_69',\n",
        "#                'ShuffleNet_70',\n",
        "#                'ShuffleNet_71',\n",
        "#                'ShuffleNet_72',\n",
        "#                'ShuffleNet_73']\n",
        "\n",
        "# model_names_v2 = ['ShuffleNet_200', \n",
        "#                   'ShuffleNet_201',\n",
        "#                   'ShuffleNet_202',\n",
        "#                   'ShuffleNet_203',\n",
        "#                   'ShuffleNet_204',\n",
        "#                   'ShuffleNet_205',\n",
        "#                   'ShuffleNet_206',\n",
        "#                   'ShuffleNet_207',\n",
        "#                   'ShuffleNet_208',\n",
        "#                   'ShuffleNet_209']\n",
        "\n",
        "# model_dict = {}\n",
        "\n",
        "# for model_name in model_names:\n",
        "#   model = ShuffleNet()\n",
        "#   model_data = torch.load('/content/gdrive/My Drive/IDL_Project/modelS/{}'.format(model_name), map_location=torch.device('cpu'))\n",
        "#   model.load_state_dict(model_data['model_state_dict'])\n",
        "#   model = model.to(device)\n",
        "#   model_dict[model_name] = model\n",
        "\n",
        "# for model_name in model_names_v2:\n",
        "#   model = ShuffleNetV2()\n",
        "#   model_data = torch.load('/content/gdrive/My Drive/IDL_Project/modelS/{}'.format(model_name), map_location=torch.device('cpu'))\n",
        "#   model.load_state_dict(model_data['model_state_dict'])\n",
        "#   model = model.to(device)\n",
        "#   model_dict[model_name] = model\n",
        "\n",
        "# logging.info(\"Total number of models: {}\".format(len(model_names) + len(model_names_v2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__HkMw61YeXo"
      },
      "source": [
        "check accuracy of individual models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s05vRuM6Ydqv"
      },
      "source": [
        "# model_name = 'ShuffleNet_35'\n",
        "# model_data = torch.load('/content/gdrive/My Drive/IDL_Project/modelS/{}'.format(model_name), map_location=torch.device('cpu'))\n",
        "# model = ShuffleNet()\n",
        "# model.load_state_dict(model_data['model_state_dict'])\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# model_classifier = PyTorchClassifier(model=model,\n",
        "#                                      clip_values=(0, 1),\n",
        "#                                      loss=criterion,\n",
        "#                                      input_shape=(3, img_size, img_size),\n",
        "#                                      nb_classes=nb_classes)\n",
        "\n",
        "# Acc_nat = []\n",
        "\n",
        "# for batch_idx, (X, Y) in enumerate(testloader):\n",
        "#   x_test = X.numpy()\n",
        "#   y_test = Y.numpy()\n",
        "#   predictions_nat = model_classifier.predict(x_test)\n",
        "#   accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
        "#   Acc_nat.append(accuracy_nat)\n",
        "#   logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
        "\n",
        "# logging.info(\"Mean Accuracy: {}\".format(np.mean(np.array(Acc_nat))))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKzm9D8h5bFR"
      },
      "source": [
        "# Creating classifier list (ignore for now)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z0e7i6HJi7i"
      },
      "source": [
        "# nb_classes = 10\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# classifier_list = []\n",
        "# for model_name in model_dict:\n",
        "#   classifier_list.append(PyTorchClassifier(\n",
        "#     model=model_dict[model_name],\n",
        "#     clip_values=(0, 1),\n",
        "#     loss=criterion,\n",
        "#     input_shape=(3, img_size, img_size),\n",
        "#     nb_classes=nb_classes,\n",
        "# ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inhJeBV-9ic0"
      },
      "source": [
        "# MyEnsembleClassifier\n",
        "Inherited from `EnsembleClassifier` with `predict`, `loss_gradient` and `loss_gradient_framework` overwritten\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ratuA8nTIKyu"
      },
      "source": [
        "class MyEnsembleClassifier(EnsembleClassifier):\n",
        "    def __init__(\n",
        "        self,\n",
        "        classifiers: List[ClassifierNeuralNetwork],\n",
        "        device,\n",
        "        infer_noise,\n",
        "        num_selected_models,\n",
        "        classifier_weights: Union[list, np.ndarray, None] = None,\n",
        "        channels_first: bool = False,\n",
        "        clip_values: Optional[\"CLIP_VALUES_TYPE\"] = None,\n",
        "        preprocessing_defences: Union[\"Preprocessor\", List[\"Preprocessor\"], None] = None,\n",
        "        postprocessing_defences: Union[\"Postprocessor\", List[\"Postprocessor\"], None] = None,\n",
        "        preprocessing: \"PREPROCESSING_TYPE\" = (0, 1),\n",
        "    ) -> None:\n",
        "      super().__init__(\n",
        "          classifiers=classifiers,\n",
        "          classifier_weights=classifier_weights,\n",
        "          channels_first=channels_first,\n",
        "          clip_values=clip_values,\n",
        "          preprocessing_defences=preprocessing_defences,\n",
        "          postprocessing_defences=postprocessing_defences,\n",
        "          preprocessing=preprocessing\n",
        "      )\n",
        "      self.device = device\n",
        "      self.infer_noise = infer_noise\n",
        "      self.num_models = len(classifiers)\n",
        "      self.num_selected_models = num_selected_models\n",
        "      self.indices = None\n",
        "\n",
        "    def predict(self, x: np.ndarray, batch_size: int = 128, raw: bool = False, **kwargs) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Perform prediction for a batch of inputs. Predictions from classifiers should only be aggregated if they all\n",
        "        have the same type of output (e.g., probabilities). Otherwise, use `raw=True` to get predictions from all\n",
        "        models without aggregation. The same option should be used for logits output, as logits are not comparable\n",
        "        between models and should not be aggregated.\n",
        "        :param x: Test set.\n",
        "        :param batch_size: Size of batches.\n",
        "        :param raw: Return the individual classifier raw outputs (not aggregated).\n",
        "        :return: Array of predictions of shape `(nb_inputs, nb_classes)`, or of shape\n",
        "                 `(nb_classifiers, nb_inputs, nb_classes)` if `raw=True`.\n",
        "        \"\"\"\n",
        "\n",
        "        self.indices = np.random.choice(self.num_models, self.num_selected_models, replace=True)\n",
        "\n",
        "        noise_list = [np.random.randn(*x.shape) * self.infer_noise for i in self.indices]\n",
        "\n",
        "        preds = []\n",
        "        for iidx, i in enumerate(self.indices):\n",
        "          preds.append(softmax(self._classifiers[i].predict(np.float32(x + noise_list[iidx])), axis=1))\n",
        "        preds = np.array(preds)\n",
        "\n",
        "\n",
        "        # del indices\n",
        "        del noise_list\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        if raw:\n",
        "            return preds\n",
        "\n",
        "        # 6 x 100\n",
        "        preds_classes = np.argmax(preds, axis=2)\n",
        "        row, col = preds_classes.shape\n",
        "\n",
        "        # 100,\n",
        "        majority_vote = np.array(\n",
        "            [\n",
        "             np.bincount(preds_classes[:,c]).argmax()\n",
        "             for c in range(col)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        mask = preds_classes == majority_vote\n",
        "        del majority_vote\n",
        "\n",
        "        mask = np.repeat(np.expand_dims(mask, axis=2), repeats=10, axis=2)\n",
        "\n",
        "        # Aggregate predictions only at probabilities level, as logits are not comparable between models\n",
        "        var_z = np.sum(mask * preds, axis=0) / np.sum(mask, axis=0) # take mean (check sum to 1)\n",
        "        del mask\n",
        "        del preds\n",
        "\n",
        "        # Apply postprocessing\n",
        "        predictions = self._apply_postprocessing(preds=var_z, fit=False)\n",
        "        del var_z\n",
        "\n",
        "        return predictions\n",
        "\n",
        "\n",
        "    def loss_gradient(self, x: np.ndarray, y: np.ndarray, raw: bool = False, **kwargs) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute the gradient of the loss function w.r.t. `x`.\n",
        "        :param x: Sample input with shape as expected by the model.\n",
        "        :param y: Target values (class labels) one-hot-encoded of shape (nb_samples, nb_classes) or indices of shape\n",
        "                  (nb_samples,).\n",
        "        :param raw: Return the individual classifier raw outputs (not aggregated).\n",
        "        :return: Array of gradients of the same shape as `x`. If `raw=True`, shape becomes `[nb_classifiers, x.shape]`.\n",
        "        \"\"\"\n",
        "\n",
        "        # indices = np.random.choice(self.num_models, self.num_selected_models, replace=True)\n",
        "\n",
        "        noise_list = [np.random.randn(*x.shape) * self.infer_noise for i in self.indices]\n",
        "        \n",
        "        grads = np.array(\n",
        "            [\n",
        "                self._classifiers[i].loss_gradient(np.float32(x + noise_list[iidx]), y)\n",
        "                for iidx, i in enumerate(self.indices)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        # del indices\n",
        "        del noise_list\n",
        "\n",
        "        if raw:\n",
        "            return grads\n",
        "\n",
        "        return np.sum(grads, axis=0)\n",
        "\n",
        "    def loss_gradient_framework(self, x: \"torch.Tensor\", y: \"torch.Tensor\", **kwargs) -> \"torch.Tensor\":\n",
        "        \"\"\"\n",
        "        Compute the gradient of the loss function w.r.t. `x`.\n",
        "        :param x: Sample input with shape as expected by the model.\n",
        "        :param y: Target values (class labels) one-hot-encoded of shape (nb_samples, nb_classes) or indices of shape\n",
        "                  (nb_samples,).\n",
        "        :param raw: Return the individual classifier raw outputs (not aggregated).\n",
        "        :return: Array of gradients of the same shape as `x`. If `raw=True`, shape becomes `[nb_classifiers, x.shape]`.\n",
        "        \"\"\"\n",
        "\n",
        "        \n",
        "        # indices = np.random.choice(self.num_models, self.num_selected_models, replace=True)\n",
        "\n",
        "        noise_list = [(torch.randn(x.size()) * self.infer_noise).to(device) for i in self.indices]\n",
        "\n",
        "\n",
        "        # gradient shape: batch_size, 3, 224, 224\n",
        "        accumulator = torch.zeros(test_batchsize, 3, img_size, img_size)\n",
        "        for iidx, i in enumerate(self.indices):\n",
        "          accumulator += self._classifiers[i].loss_gradient_framework(x + noise_list[iidx], y).cpu()\n",
        "        \n",
        "        torch.cuda.empty_cache()\n",
        "        # del indices\n",
        "        del noise_list\n",
        "\n",
        "        return accumulator.to(device)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80wKHjAJV9EF"
      },
      "source": [
        "# Experiment Settings\n",
        "### common settings:\n",
        "*   inference noise: 0.1\n",
        "*   eps: 0.5\n",
        "*   eps step: 0.4\n",
        "*   max iter: 20\n",
        "*   L2 norm\n",
        "*   number of samplings: 50\n",
        "\n",
        "### to experiment:\n",
        "1. Total number of candidate models in the ensemble: 1 (randomized smoothing)\n",
        "2. Total number of candidate models in the ensemble: 5\n",
        "3. Total number of candidate models in the ensemble: 9\n",
        "4. Total number of candidate models in the ensemble: 13"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qgPgn2hXwzB"
      },
      "source": [
        "# Run this code block to load common variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge6pmhN3X51_"
      },
      "source": [
        "infer_noise = 0.1\n",
        "eps = 0.5\n",
        "eps_step = 0.4\n",
        "max_iter = 20\n",
        "norm = 2"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ2kzTjNYcg4"
      },
      "source": [
        "# Experiment 1 (Shivani)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzefdlNUYbx5"
      },
      "source": [
        "model_names = ['ShuffleNet_1']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oqPQe7LYyx8"
      },
      "source": [
        "# Experiment 2 (Kriti)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RX-Ey7FZFld"
      },
      "source": [
        "model_names = ['ShuffleNet_1',\n",
        "               'ShuffleNet_2',\n",
        "               'ShuffleNet_3',\n",
        "               'ShuffleNet_4',\n",
        "               'ShuffleNet_5']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z__h3gZpY1AS"
      },
      "source": [
        "# Experiment 3 (Shriti)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SphoToOzZHdy"
      },
      "source": [
        "model_names = ['ShuffleNet_1',\n",
        "               'ShuffleNet_2',\n",
        "               'ShuffleNet_3',\n",
        "               'ShuffleNet_4',\n",
        "               'ShuffleNet_5',\n",
        "               'ShuffleNet_6',\n",
        "               'ShuffleNet_7',\n",
        "               'ShuffleNet_8',\n",
        "               'ShuffleNet_31']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF-zmm6WY3XP"
      },
      "source": [
        "# Experiment 4 (Kaicheng)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuSizTHpZIbN"
      },
      "source": [
        "model_names = ['ShuffleNet_1',\n",
        "               'ShuffleNet_2',\n",
        "               'ShuffleNet_3',\n",
        "               'ShuffleNet_4',\n",
        "               'ShuffleNet_5',\n",
        "               'ShuffleNet_6',\n",
        "               'ShuffleNet_7',\n",
        "               'ShuffleNet_8',\n",
        "               'ShuffleNet_31',\n",
        "               'ShuffleNet_32',\n",
        "               'ShuffleNet_33',\n",
        "               'ShuffleNet_34',\n",
        "               'ShuffleNet_35']"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D7HuN4sY5cz"
      },
      "source": [
        "# Load models and create `classifier_list`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TVPQL9ci-WK",
        "outputId": "9640d215-1230-4f4a-ab23-034111f2ebba"
      },
      "source": [
        "# load models\n",
        "model_dict = {}\n",
        "for model_name in model_names:\n",
        "  model = ShuffleNet()\n",
        "  model_data = torch.load('/content/gdrive/My Drive/IDL_Project/modelS/{}'.format(model_name), map_location=torch.device('cpu'))\n",
        "  model.load_state_dict(model_data['model_state_dict'])\n",
        "  model = model.to(device)\n",
        "  model_dict[model_name] = model\n",
        "\n",
        "logging.info(\"Models loaded successfully!\")\n",
        "logging.info(\"Total number of models: {}\".format(len(model_names)))\n",
        "\n",
        "# create classifier list\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "classifier_list = []\n",
        "for model_name in model_dict:\n",
        "  classifier_list.append(PyTorchClassifier(model=model_dict[model_name],\n",
        "                                           clip_values=(0, 1),\n",
        "                                           loss=criterion,\n",
        "                                           input_shape=(3, img_size, img_size),\n",
        "                                           nb_classes=nb_classes))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-08 06:30:13,166 INFO Models loaded successfully!\n",
            "2020-12-08 06:30:13,169 INFO Total number of models: 13\n",
            "2020-12-08 06:30:13,172 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-08 06:30:13,178 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-08 06:30:13,185 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-08 06:30:13,192 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-08 06:30:13,198 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-08 06:30:13,205 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-08 06:30:13,211 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-08 06:30:13,217 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-08 06:30:13,224 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-08 06:30:13,230 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-08 06:30:13,237 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-08 06:30:13,244 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-08 06:30:13,250 INFO Inferred 1 hidden layers on PyTorch classifier.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jiin82Cpi2ia"
      },
      "source": [
        "# Change number of samplings here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ekgfiiQin9d"
      },
      "source": [
        "num_selected_models = 10"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKU4Y8C2jZoh"
      },
      "source": [
        "# Kick off experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmA8Hs__Y9Zf",
        "outputId": "a938db10-d19f-4268-c48f-231c1cddfa1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        }
      },
      "source": [
        "# create ensemble classifier\n",
        "my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
        "                                              device, \n",
        "                                              infer_noise=infer_noise,\n",
        "                                              num_selected_models=num_selected_models,\n",
        "                                              clip_values=[0., 1.], \n",
        "                                              channels_first=True)\n",
        "\n",
        "# create attack object\n",
        "attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, \n",
        "                                         eps=eps, \n",
        "                                         eps_step=eps_step, \n",
        "                                         norm=norm, \n",
        "                                         max_iter=max_iter, \n",
        "                                         batch_size=test_batchsize)\n",
        "\n",
        "# kick off experiment\n",
        "Acc_adv = []\n",
        "Acc_nat = []\n",
        "for batch_idx, (X, Y) in enumerate(testloader):\n",
        "  x_test = X.numpy()\n",
        "  y_test = Y.numpy()\n",
        "  predictions_nat = my_ensemble_classifier.predict(x_test)\n",
        "  accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
        "  Acc_nat.append(accuracy_nat)\n",
        "\n",
        "  x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "  predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
        "  accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
        "  Acc_adv.append(accuracy_adv)\n",
        "\n",
        "  logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
        "  logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
        "    \n",
        "\n",
        "# saving the dataframe \n",
        "Acc_adv = np.asanyarray(Acc_adv)\n",
        "Acc_nat = np.asanyarray(Acc_nat)\n",
        "res = {'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
        "df = pd.DataFrame(res)\n",
        "# df.to_csv('./gdrive/My Drive/IDL_Project/results/result_{}models_{}samples.csv'.format(len(model_names), num_selected_models),index=True)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "PGD - Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d17827c07a0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mAcc_nat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_nat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m   \u001b[0mx_test_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m   \u001b[0mpredictions_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_ensemble_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_adv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0maccuracy_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_adv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/art/attacks/attack.py\u001b[0m in \u001b[0;36mreplacement_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfunc_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mreplacement_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfunc_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mbatch_index_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_id\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0madv_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index_1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_index_2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_random_init\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py\u001b[0m in \u001b[0;36m_generate_batch\u001b[0;34m(self, x, targets, mask)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi_max_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             adv_x = self._compute_torch(\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0madv_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_random_init\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi_max_iter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m             )\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py\u001b[0m in \u001b[0;36m_compute_torch\u001b[0;34m(self, x, x_init, y, mask, eps, eps_step, random_init)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;31m# Get perturbation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mperturbation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_perturbation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_adv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;31m# Apply perturbation and clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py\u001b[0m in \u001b[0;36m_compute_perturbation\u001b[0;34m(self, x, y, mask)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Get gradient wrt loss; invert it if attack is targeted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_gradient_framework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargeted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# Apply norm bound\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-d9710c2449c7>\u001b[0m in \u001b[0;36mloss_gradient_framework\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0maccumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_batchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m           \u001b[0maccumulator\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_classifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_gradient_framework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miidx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/art/estimators/classification/pytorch.py\u001b[0m in \u001b[0;36mloss_gradient_framework\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# Compute the gradient and return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/art/estimators/classification/pytorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m                         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m                             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m                             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-a7ef69399513>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/shufflenetv2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/shufflenetv2.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/shufflenetv2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             self.weight, self.bias, bn_training, exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2056\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   2057\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2058\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2059\u001b[0m     )\n\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 15.90 GiB total capacity; 14.73 GiB already allocated; 27.81 MiB free; 15.06 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdUjTkdWBdgh"
      },
      "source": [
        "# **The rest parts of the notebook was regarding previous work. Discard them for now.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emCbgIPQ5yA5"
      },
      "source": [
        "# Experiment Settings (**deprecated**)\n",
        "Refer to [this google doc](https://docs.google.com/document/d/1VaS5THALdudd_63Zv7ZR2u8rj8r8iDF6xgXzzLHXR8w/edit)\n",
        "\n",
        "1.   `num_selected_models`=15, `infer_noise`=[0.0025, 0.005, 0.01, 0.05, 0.08, 0.1], `eps`=[0.0025, 0.01, 0.1, 0.5, 0.8], `norm`=Linf\n",
        "2.   `num_selected_models`=[5, 10, 15, 20, 25, 30, 35, 40], `infer_noise`=0.1, `eps`=[0.0025, 0.01, 0.1, 0.5, 0.8], `norm`=Linf\n",
        "3.   `num_selected_models`=15, `infer_noise`=[0.0025, 0.005, 0.01, 0.05, 0.08, 0.1], `eps`=[0.1, 0.5, 0.8, 0.9, 1], `norm`=L2\n",
        "4.   `num_selected_models`=[5, 10, 15, 20, 25, 30, 35, 40], `infer_noise`=0.1, `eps`=[0.1, 0.5, 0.8, 0.9, 1], `norm`=L2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfpLU-YJlx92"
      },
      "source": [
        "### Arbitrary Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64xN40H5nqHR"
      },
      "source": [
        "num_selected_models = 50\n",
        "# norm = np.inf\n",
        "norm = 2\n",
        "infer_noise = 0.1\n",
        "# eps = 8/255\n",
        "eps = 0.5\n",
        "logging.info(\"Current experiment setting: eps={}, inference noise={}\".format(eps, infer_noise))\n",
        "my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
        "                                              device, \n",
        "                                              infer_noise=infer_noise,\n",
        "                                              num_selected_models=num_selected_models,\n",
        "                                              clip_values=[0., 1.], \n",
        "                                              channels_first=True)\n",
        "attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=eps, eps_step=0.4, norm=norm, max_iter=20, batch_size=test_batchsize)\n",
        "Acc_adv = []\n",
        "Acc_nat = []\n",
        "for batch_idx, (X, Y) in enumerate(testloader):\n",
        "  x_test = X.numpy()\n",
        "  y_test = Y.numpy()\n",
        "  predictions_nat = my_ensemble_classifier.predict(x_test)\n",
        "  accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
        "  Acc_nat.append(accuracy_nat)\n",
        "\n",
        "  x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "  predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
        "  accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
        "  Acc_adv.append(accuracy_adv)\n",
        "  \n",
        "  # print('accuracy (adversarial): {}'.format(accuracy_adv))\n",
        "  # print('accuracy (natural): {}'.format(accuracy_nat))\n",
        "\n",
        "  # logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
        "  # logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
        "    \n",
        "Acc_adv = np.asanyarray(Acc_adv)\n",
        "Acc_nat = np.asanyarray(Acc_nat)\n",
        "idx = np.arange(0, len(testset)/test_batchsize).tolist()\n",
        "res = {'Id': idx ,'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
        "df = pd.DataFrame(res) \n",
        "# saving the dataframe \n",
        "df.to_csv('./gdrive/My Drive/IDL_Project/results/PGD_exp1_infer_noise{}_eps{}.csv'.format(infer_noise, eps),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEzaTmhp7qFg"
      },
      "source": [
        "# Experiment 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIZS0qT17u3_"
      },
      "source": [
        "num_selected_models = 15\n",
        "norm = np.inf\n",
        "infer_noise_list = [0.0025, 0.005, 0.01, 0.05, 0.08, 0.1]\n",
        "eps_list = [0.0025, 0.01, 0.1, 0.5, 0.8]\n",
        "\n",
        "for infer_noise in infer_noise_list:\n",
        "  for eps in eps_list:\n",
        "    my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
        "                                                  device, \n",
        "                                                  infer_noise=infer_noise,\n",
        "                                                  num_selected_models=num_selected_models,\n",
        "                                                  clip_values=[0., 1.], \n",
        "                                                  channels_first=True)\n",
        "    attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=eps, eps_step=eps/3, norm=norm)\n",
        "    Acc_adv = []\n",
        "    Acc_nat = []\n",
        "    for batch_idx, (X, Y) in enumerate(testloader):\n",
        "      x_test = X.numpy()\n",
        "      y_test = Y.numpy()\n",
        "      predictions_nat = my_ensemble_classifier.predict(x_test)\n",
        "      accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
        "      Acc_nat.append(accuracy_nat)\n",
        "\n",
        "      x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "      predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
        "      accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
        "      Acc_adv.append(accuracy_adv)\n",
        "      \n",
        "      logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
        "      logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
        "    \n",
        "    Acc_adv = np.asanyarray(Acc_adv)\n",
        "    Acc_nat = np.asanyarray(Acc_nat)\n",
        "    idx = np.arange(0, len(testset)/test_batchsize).tolist()\n",
        "    res = {'Id': idx ,'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
        "    df = pd.DataFrame(res) \n",
        "    # saving the dataframe \n",
        "    df.to_csv('./gdrive/My Drive/IDL_Project/results/PGD_exp1_infer_noise{}_eps{}.csv'.format(infer_noise, eps),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH-xmYJL8n14"
      },
      "source": [
        "# Experiment 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e44AHWvw8qxI"
      },
      "source": [
        "num_selected_models_list = [5, 10, 15, 20, 25, 30, 35, 40]\n",
        "norm = np.inf\n",
        "infer_noise = 0.1\n",
        "eps_list = [0.0025, 0.01, 0.1, 0.5, 0.8]\n",
        "\n",
        "for num_selected_models in num_selected_models_list:\n",
        "  for eps in eps_list:\n",
        "    my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
        "                                                  device, \n",
        "                                                  infer_noise=infer_noise,\n",
        "                                                  num_selected_models=num_selected_models,\n",
        "                                                  clip_values=[0., 1.], \n",
        "                                                  channels_first=True)\n",
        "    attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=eps, eps_step=eps/3, norm=norm)\n",
        "    Acc_adv = []\n",
        "    Acc_nat = []\n",
        "    for batch_idx, (X, Y) in enumerate(testloader):\n",
        "      x_test = X.numpy()\n",
        "      y_test = Y.numpy()\n",
        "      x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "      predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
        "      predictions_nat = my_ensemble_classifier.predict(x_test)\n",
        "      accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
        "      accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
        "      Acc_adv.append(accuracy_adv)\n",
        "      Acc_nat.append(accuracy_nat)\n",
        "      logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
        "      logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
        "    \n",
        "    Acc_adv = np.asanyarray(Acc_adv)\n",
        "    Acc_nat = np.asanyarray(Acc_nat)\n",
        "    idx = np.arange(0, len(testset)/test_batchsize).tolist()\n",
        "    res = {'Id': idx ,'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
        "    df = pd.DataFrame(res) \n",
        "    # saving the dataframe \n",
        "    df.to_csv('./gdrive/My Drive/IDL_Project/results/PGD_exp2_N{}_eps{}.csv'.format(num_selected_models, eps),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng7Xe9Zq8rai"
      },
      "source": [
        "# Experiment 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3rzYvCu8uoa"
      },
      "source": [
        "num_selected_models = 15\n",
        "norm = 2\n",
        "infer_noise_list = [0.0025, 0.005, 0.01, 0.05, 0.08, 0.1]\n",
        "eps_list = [0.1, 0.5, 0.8, 0.9, 1]\n",
        "\n",
        "for infer_noise in infer_noise_list:\n",
        "  for eps in eps_list:\n",
        "    my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
        "                                                  device, \n",
        "                                                  infer_noise=infer_noise,\n",
        "                                                  num_selected_models=num_selected_models,\n",
        "                                                  clip_values=[0., 1.], \n",
        "                                                  channels_first=True)\n",
        "    attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=eps, eps_step=eps/3, norm=norm)\n",
        "    Acc_adv = []\n",
        "    Acc_nat = []\n",
        "    for batch_idx, (X, Y) in enumerate(testloader):\n",
        "      x_test = X.numpy()\n",
        "      y_test = Y.numpy()\n",
        "      x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "      predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
        "      predictions_nat = my_ensemble_classifier.predict(x_test)\n",
        "      accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
        "      accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
        "      Acc_adv.append(accuracy_adv)\n",
        "      Acc_nat.append(accuracy_nat)\n",
        "      logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
        "      logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
        "    \n",
        "    Acc_adv = np.asanyarray(Acc_adv)\n",
        "    Acc_nat = np.asanyarray(Acc_nat)\n",
        "    idx = np.arange(0, len(testset)/test_batchsize).tolist()\n",
        "    res = {'Id': idx ,'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
        "    df = pd.DataFrame(res) \n",
        "    # saving the dataframe \n",
        "    df.to_csv('./gdrive/My Drive/IDL_Project/results/PGD_exp3_infer_noise{}_eps{}.csv'.format(infer_noise, eps),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-w5HNiC8vDY"
      },
      "source": [
        "# Experiment 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTCAdeqj8w17"
      },
      "source": [
        "num_selected_models_list = [5, 10, 15, 20, 25, 30, 35, 40]\n",
        "norm = 2\n",
        "infer_noise = 0.1\n",
        "eps_list = [0.1, 0.5, 0.8, 0.9, 1]\n",
        "\n",
        "for num_selected_models in num_selected_models_list:\n",
        "  for eps in eps_list:\n",
        "    my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
        "                                                  device, \n",
        "                                                  infer_noise=infer_noise,\n",
        "                                                  num_selected_models=num_selected_models,\n",
        "                                                  clip_values=[0., 1.], \n",
        "                                                  channels_first=True)\n",
        "    attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=eps, eps_step=eps/3, norm=norm)\n",
        "    Acc_adv = []\n",
        "    Acc_nat = []\n",
        "    for batch_idx, (X, Y) in enumerate(testloader):\n",
        "      x_test = X.numpy()\n",
        "      y_test = Y.numpy()\n",
        "      x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "      predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
        "      predictions_nat = my_ensemble_classifier.predict(x_test)\n",
        "      accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
        "      accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
        "      Acc_adv.append(accuracy_adv)\n",
        "      Acc_nat.append(accuracy_nat)\n",
        "      logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
        "      logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
        "    \n",
        "    Acc_adv = np.asanyarray(Acc_adv)\n",
        "    Acc_nat = np.asanyarray(Acc_nat)\n",
        "    idx = np.arange(0, len(testset)/test_batchsize).tolist()\n",
        "    res = {'Id': idx ,'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
        "    df = pd.DataFrame(res) \n",
        "    # saving the dataframe \n",
        "    df.to_csv('./gdrive/My Drive/IDL_Project/results/PGD_exp4_N{}_eps{}.csv'.format(num_selected_models, eps),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP_Gxq-2jc-9"
      },
      "source": [
        "# FGM Attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmXkglUUKXO3"
      },
      "source": [
        "attack = FastGradientMethod(estimator=my_ensemble_classifier, eps=0.5, targeted=False, norm=2)\n",
        "Acc = []\n",
        "for batch_idx, (X, Y) in enumerate(testloader):\n",
        "    x_test = X.numpy()\n",
        "    y_test = Y.numpy()\n",
        "    # target = (y_test + 1) % 10\n",
        "    x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "    predictions = my_ensemble_classifier.predict(x_test_adv)\n",
        "    predictions_test = my_ensemble_classifier.predict(x_test)\n",
        "    accuracy = np.sum(np.argmax(predictions, axis=1) == y_test) / len(y_test)\n",
        "    acc_test = np.sum(np.argmax(predictions_test, axis=1) == y_test) / len(y_test)\n",
        "    # print(y_test.shape)\n",
        "    # acc_rate, coverage_rate = compute_accuracy(preds=predictions, labels=np.reshape(y_test, (test_batchsize, 1)))\n",
        "    # logging.info('acc rate: {}, coverage rate: {}'.format(acc_rate, coverage_rate))\n",
        "    Acc.append(accuracy)\n",
        "    logging.info('accuracy: {}'.format(accuracy))\n",
        "    logging.info('accuracy non adv: {}'.format(acc_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoYiKeGCjfDR"
      },
      "source": [
        "# PGD Attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZRM5XEZjchO"
      },
      "source": [
        "attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=0.5, eps_step=0.03)\n",
        "Acc = []\n",
        "for batch_idx, (X, Y) in enumerate(testloader):\n",
        "    x_test = X.numpy()\n",
        "    y_test = Y.numpy()\n",
        "    x_test_adv = attack.generate(torch.from_numpy(x_test))\n",
        "    predictions = my_ensemble_classifier.predict(x_test_adv)\n",
        "    accuracy = np.sum(np.argmax(predictions, axis=1) == np.array(y_test)) / len(y_test)\n",
        "    Acc.append(accuracy)\n",
        "    logging.info('accuracy: {}'.format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyMeGON8BFNL"
      },
      "source": [
        "# Experiment settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjUPKB3jvijF"
      },
      "source": [
        "fgm_epsilon = np.array([0.005, 0.01, 0.02, 0.04, 0.08])\n",
        "pgd_epsilon = 8.0 / 255.0 / np.array([2.0, 4.0, 8.0, 16.0])\n",
        "diff = np.array([1e-3, 1e-4, 1e-5, 1e-6])\n",
        "pgd_epsilon_step = pgd_epsilon - diff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XG44M27BNIT"
      },
      "source": [
        "Experiment on inference noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U79zJmCsDEKc"
      },
      "source": [
        "noise_stds = [0.02, 0.04, 0.06, 0.08, 0.1]\n",
        "for i in range(len(noise_stds)):\n",
        "  my_ensemble = MyEnsemble(model_dict, num_models_selected, noise_std=noise_stds[i], num_classes=10)\n",
        "  classifier = PyTorchClassifier(\n",
        "      model=my_ensemble,\n",
        "      clip_values=(0, 1),\n",
        "      loss=nn.CrossEntropyLoss(),\n",
        "      input_shape=(3, 224, 224),\n",
        "      nb_classes=10)\n",
        "  attack = FastGradientMethod(estimator=classifier, eps=0.01)\n",
        "  Acc = []\n",
        "  for batch_idx, (X, Y) in enumerate(testloader):\n",
        "    x_test = X.numpy()\n",
        "    y_test = Y.numpy()\n",
        "    x_test_adv = attack.generate(torch.from_numpy(x_test))\n",
        "    predictions = classifier.predict(x_test_adv)\n",
        "    accuracy = np.sum(np.argmax(predictions, axis=1) == np.array(y_test)) / len(y_test)\n",
        "    Acc.append(accuracy)\n",
        "    logging.info('accuracy: {}'.format(accuracy))\n",
        "  \n",
        "  logging.info(\"Accuracy for eps={}: {}\".format(fgm_epsilon[i], np.mean(np.array(Acc))))\n",
        "\n",
        "  pred_list = np.asanyarray(Acc)\n",
        "  idx = np.arange(0,len(testset)/test_batchsize).tolist()\n",
        "  dict = {'Id': idx ,'acc': pred_list} \n",
        "  df = pd.DataFrame(dict) \n",
        "  # saving the dataframe \n",
        "  df.to_csv('./gdrive/My Drive/IDL_Project/results/FGM_result_noise{}_{}.csv'.format(noise_stds[i], datetime.datetime.now()),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LotwWsVpMYKN"
      },
      "source": [
        "pred_list = np.asanyarray(Acc)\n",
        "idx = np.arange(0,len(testset)/test_batchsize).tolist()\n",
        "dict = {'Id': idx ,'acc': pred_list} \n",
        "df = pd.DataFrame(dict) \n",
        "# saving the dataframe \n",
        "df.to_csv('./gdrive/My Drive/IDL_Project/results/FGM_result_eps{}_{}.csv'.format(fgm_epsilon[-1], datetime.datetime.now()),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqIgjfhwBXVL"
      },
      "source": [
        "Plot PGD adversarial examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Eu7qdDQMnGq"
      },
      "source": [
        "attack = ProjectedGradientDescentPyTorch(estimator=classifier, eps=pgd_epsilon[0], eps_step=pgd_epsilon_step[0])\n",
        "Acc = []\n",
        "adv = []\n",
        "orig = []\n",
        "labels = []\n",
        "pred = []\n",
        "for batch_idx, (X, Y) in enumerate(testloader):\n",
        "  if batch_idx > 0:\n",
        "    break\n",
        "  x_test = X.numpy()\n",
        "  y_test = Y.numpy()\n",
        "  x_test_adv = attack.generate(torch.from_numpy(x_test))\n",
        "  predictions = classifier.predict(x_test_adv)\n",
        "  accuracy = np.sum(np.argmax(predictions, axis=1) == np.array(y_test)) / len(y_test)\n",
        "  Acc.append(accuracy)\n",
        "  adv.append(x_test_adv)\n",
        "  orig.append(x_test)\n",
        "  labels.append(y_test)\n",
        "  pred.append(predictions)\n",
        "  logging.info('accuracy: {}'.format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8sj2LdnNV5n"
      },
      "source": [
        "example_adv = adv[-1]\n",
        "example_orig = orig[-1]\n",
        "label = labels[-1]\n",
        "predi = np.argmax(predictions, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoD6QHEwNt4T"
      },
      "source": [
        "print(label)\n",
        "print(predi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnkXnHRePFeW"
      },
      "source": [
        "idx = 2\n",
        "selected_adv = example_adv[idx]\n",
        "selected_orig = example_orig[idx]\n",
        "selected_lab = label[idx]\n",
        "selected_pred = predi[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZuD1OfoWroX"
      },
      "source": [
        "print(selected_lab)\n",
        "print(selected_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkxSl4PTPab0"
      },
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(np.transpose(selected_orig, (1, 2, 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tpehq_0hVobG"
      },
      "source": [
        "plt.imshow(np.transpose(selected_adv, (1, 2, 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuLSJh8QVsHy"
      },
      "source": [
        "plt.imshow(10*(np.transpose(selected_orig, (1, 2, 0)) - np.transpose(selected_adv, (1, 2, 0))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lOaioJIBh1G"
      },
      "source": [
        "Experiment PGD attacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuxeEuf-8UVE"
      },
      "source": [
        "for i in range(len(pgd_epsilon)):\n",
        "  attack = FastGradientMethod(estimator=classifier, eps=fgm_epsilon[i])\n",
        "  Acc = []\n",
        "  for batch_idx, (X, Y) in enumerate(testloader):\n",
        "    x_test = X.numpy()\n",
        "    y_test = Y.numpy()\n",
        "    x_test_adv = attack.generate(torch.from_numpy(x_test))\n",
        "    predictions = classifier.predict(x_test_adv)\n",
        "    accuracy = np.sum(np.argmax(predictions, axis=1) == np.array(y_test)) / len(y_test)\n",
        "    Acc.append(accuracy)\n",
        "    logging.info('accuracy: {}'.format(accuracy))\n",
        "  \n",
        "  logging.info(\"Accuracy for eps={}: {}\".format(fgm_epsilon[i], np.mean(np.array(Acc))))\n",
        "\n",
        "  pred_list = np.asanyarray(Acc)\n",
        "  idx = np.arange(0,len(testset)/test_batchsize).tolist()\n",
        "  dict = {'Id': idx ,'acc': pred_list} \n",
        "  df = pd.DataFrame(dict) \n",
        "  # saving the dataframe \n",
        "  df.to_csv('./gdrive/My Drive/IDL_Project/results/FGM_result_eps{}_{}.csv'.format(fgm_epsilon[i], datetime.datetime.now()),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}