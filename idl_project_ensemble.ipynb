{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of Copy of Copy of idl_project_ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "J0oc512o9c4d",
        "IffxeALQ_ajk",
        "inhJeBV-9ic0"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaichengDING/Triple-Defense/blob/main/idl_project_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8idp0UC0Oj6"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ESeViSYxQfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0b337fa-e9a9-4bd6-cf8c-292095e20005"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgPpHa2d0Rt4"
      },
      "source": [
        "# Install ART"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzKNCLpGws5x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "654ddf3d-f997-4dbb-c5f5-392d2472708a"
      },
      "source": [
        "!pip install adversarial-robustness-toolbox==1.4.3"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting adversarial-robustness-toolbox==1.4.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/aa/1136628240b23de5c311aa1595d327f3908296a71f72093370a0459b8f0b/adversarial_robustness_toolbox-1.4.3-py3-none-any.whl (765kB)\n",
            "\u001b[K     |████████████████████████████████| 768kB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (1.18.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (7.0.0)\n",
            "Collecting ffmpeg-python\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/0c/56be52741f75bad4dc6555991fabd2e07b432d333da82c11ad701123888a/ffmpeg_python-0.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (1.15.0)\n",
            "Collecting scikit-learn==0.22.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/7f/366dcba1ba076a88a50bea732dbc033c0c5bbf7876010e6edc67948579d5/scikit_learn-0.22.2-cp36-cp36m-manylinux1_x86_64.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 13.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (3.2.2)\n",
            "Collecting pydub\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/d1/fbfa79371a8cd9bb15c2e3c480d7e6e340ed5cc55005174e16f48418333a/pydub-0.24.1-py2.py3-none-any.whl\n",
            "Collecting mypy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/cb/cf5530d063e7e703e2fbec677bfba633de6e70fe44bc323deeaa27f273b8/mypy-0.790-cp36-cp36m-manylinux1_x86_64.whl (21.0MB)\n",
            "\u001b[K     |████████████████████████████████| 21.0MB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: resampy in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (0.2.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (50.3.2)\n",
            "Collecting cma\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/c0/0a1c41f7cad0a51e07991cf86423d0e6651d035f1fe7dcff48e8858848f2/cma-3.0.3-py2.py3-none-any.whl (230kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 48.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (4.41.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from ffmpeg-python->adversarial-robustness-toolbox==1.4.3) (0.16.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.22.2->adversarial-robustness-toolbox==1.4.3) (0.17.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox==1.4.3) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox==1.4.3) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox==1.4.3) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox==1.4.3) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from mypy->adversarial-robustness-toolbox==1.4.3) (3.7.4.3)\n",
            "Collecting typed-ast<1.5.0,>=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/ed/5459080d95eb87a02fe860d447197be63b6e2b5e9ff73c2b0a85622994f4/typed_ast-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (737kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 43.4MB/s \n",
            "\u001b[?25hCollecting mypy-extensions<0.5.0,>=0.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numba>=0.32 in /usr/local/lib/python3.6/dist-packages (from resampy->adversarial-robustness-toolbox==1.4.3) (0.48.0)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels->adversarial-robustness-toolbox==1.4.3) (0.5.1)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from statsmodels->adversarial-robustness-toolbox==1.4.3) (1.1.4)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.32->resampy->adversarial-robustness-toolbox==1.4.3) (0.31.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->statsmodels->adversarial-robustness-toolbox==1.4.3) (2018.9)\n",
            "Installing collected packages: ffmpeg-python, scikit-learn, pydub, typed-ast, mypy-extensions, mypy, cma, adversarial-robustness-toolbox\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed adversarial-robustness-toolbox-1.4.3 cma-3.0.3 ffmpeg-python-0.2.0 mypy-0.790 mypy-extensions-0.4.3 pydub-0.24.1 scikit-learn-0.22.2 typed-ast-1.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL8u7HvJ0Xka"
      },
      "source": [
        "# Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SqUPdAKkDIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67ec31c0-a41e-4edc-f00d-47c2d04ac058"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import PIL\n",
        "\n",
        "import sys\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "from torch.utils import data\n",
        "\n",
        "from art.attacks.evasion import FastGradientMethod\n",
        "from art.attacks.evasion import ProjectedGradientDescentPyTorch\n",
        "from art.estimators.classification import PyTorchClassifier\n",
        "\n",
        "from art.estimators.classification import EnsembleClassifier\n",
        "from typing import List, Optional, Union, TYPE_CHECKING\n",
        "from art.estimators.classification.classifier import ClassifierNeuralNetwork\n",
        "from scipy.special import softmax\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import logging\n",
        "import datetime\n",
        "import random\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "cuda"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcOnXEFW1m4K"
      },
      "source": [
        "# Model Definition\n",
        "`ShuffleNet` and `ShuffleNetV2` are actually identical, but models with seed number smaller than 200 are created using ShuffleNet and models with seed number greater than 200 are created using ShuffleNetV2. In order to load all models into ensemble, the two definitions are provided here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfyaOczXoRqQ"
      },
      "source": [
        "class ShuffleNet(nn.Module):\n",
        "    def __init__(self, nb_classes =10):\n",
        "        super(ShuffleNet, self).__init__()\n",
        "        self.shuffle = models.shufflenet_v2_x2_0()\n",
        "        self.linear = nn.Linear(1000, nb_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.shuffle(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ShuffleNetV2(nn.Module):\n",
        "    def __init__(self, nb_classes=10):\n",
        "        super(ShuffleNetV2, self).__init__()\n",
        "        self.shufflenet = models.shufflenet_v2_x2_0()\n",
        "        self.linear = nn.Linear(1000, nb_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.shufflenet(x)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlygMiHF0iSB"
      },
      "source": [
        "# Logging Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXX4443Y16Ul"
      },
      "source": [
        "# configure logging\n",
        "logger = logging.getLogger(\"\")\n",
        "\n",
        "# reset handler\n",
        "for handler in logging.root.handlers[:]:\n",
        "  logging.root.removeHandler(handler)\n",
        "\n",
        "# set handler\n",
        "stream_hdlr = logging.StreamHandler()\n",
        "# logging on colab\n",
        "file_hdlr = logging.FileHandler('/content/gdrive/My Drive/IDL_Project/logs/log_{}.log'.format(datetime.datetime.now()))\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
        "stream_hdlr.setFormatter(formatter)\n",
        "file_hdlr.setFormatter(formatter)\n",
        "\n",
        "logger.addHandler(stream_hdlr)\n",
        "logger.addHandler(file_hdlr)\n",
        "\n",
        "logger.setLevel(logging.INFO)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFiB-FbgT9sk"
      },
      "source": [
        "# Get Test Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk68nikXxa4G"
      },
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, X, Y, transform=None):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    self.transform = transform\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.Y)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    if self.transform is None:\n",
        "      return torch.from_numpy(self.X[idx]), torch.tensor(self.Y[idx]).long()\n",
        "    else:\n",
        "      return self.transform(self.X[idx]), torch.tensor(self.Y[idx]).long()"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV2pPKKzeESx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cb9611a-0d68-43a7-e332-2dac40d6f3d8"
      },
      "source": [
        "test_batchsize = 200\n",
        "num_workers = 4\n",
        "img_size = 224\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToPILImage(),\n",
        "                                     transforms.Resize(size=img_size),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0, 0, 0), (1, 1, 1))])\n",
        "\n",
        "testset= torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "testset_data = testset.data[0:1000]\n",
        "testset_labels = testset.targets[0:1000]\n",
        "\n",
        "testset_sub = MyDataset(testset_data, testset_labels, transform=test_transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset_sub, batch_size=test_batchsize, shuffle=False, num_workers=num_workers, drop_last=True)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOPbwP7FTWEY"
      },
      "source": [
        "# Load Saved Models\n",
        "All saved models will be loaded into `model_dict`. Models with seed number smaller than 200 belong to `ShuffleNet` class, and models with seed number greater than 200 belong to `ShuffleNetV2` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUiLYb1EeNtS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17546cf5-9d7e-4371-dce9-e4d9aa40db47"
      },
      "source": [
        "# models with 0.1 training noise\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "model_names = ['ShuffleNet_1',\n",
        "               'ShuffleNet_2',\n",
        "               'ShuffleNet_3',\n",
        "               'ShuffleNet_4',\n",
        "               'ShuffleNet_5',\n",
        "               'ShuffleNet_6',\n",
        "               'ShuffleNet_7',\n",
        "               'ShuffleNet_8',\n",
        "               'ShuffleNet_31',\n",
        "               'ShuffleNet_32',\n",
        "               'ShuffleNet_33',\n",
        "               'ShuffleNet_34',\n",
        "               'ShuffleNet_35',\n",
        "               'ShuffleNet_36',\n",
        "               'ShuffleNet_37',\n",
        "               'ShuffleNet_38',\n",
        "               'ShuffleNet_39',\n",
        "               'ShuffleNet_61',\n",
        "               'ShuffleNet_62',\n",
        "               'ShuffleNet_63',\n",
        "               'ShuffleNet_64',\n",
        "               'ShuffleNet_65',\n",
        "               'ShuffleNet_66',\n",
        "               'ShuffleNet_67',\n",
        "               'ShuffleNet_68',\n",
        "               'ShuffleNet_69',\n",
        "               'ShuffleNet_70',\n",
        "               'ShuffleNet_71',\n",
        "               'ShuffleNet_72',\n",
        "               'ShuffleNet_73']\n",
        "\n",
        "model_names_v2 = ['ShuffleNet_200', \n",
        "                  'ShuffleNet_201',\n",
        "                  'ShuffleNet_202',\n",
        "                  'ShuffleNet_203',\n",
        "                  'ShuffleNet_204',\n",
        "                  'ShuffleNet_205',\n",
        "                  'ShuffleNet_206',\n",
        "                  'ShuffleNet_207',\n",
        "                  'ShuffleNet_208',\n",
        "                  'ShuffleNet_209']\n",
        "\n",
        "model_dict = {}\n",
        "\n",
        "for model_name in model_names:\n",
        "  model = ShuffleNet()\n",
        "  model_data = torch.load('/content/gdrive/My Drive/IDL_Project/modelS/{}'.format(model_name), map_location=torch.device('cpu'))\n",
        "  model.load_state_dict(model_data['model_state_dict'])\n",
        "  model = model.to(device)\n",
        "  model_dict[model_name] = model\n",
        "\n",
        "for model_name in model_names_v2:\n",
        "  model = ShuffleNetV2()\n",
        "  model_data = torch.load('/content/gdrive/My Drive/IDL_Project/modelS/{}'.format(model_name), map_location=torch.device('cpu'))\n",
        "  model.load_state_dict(model_data['model_state_dict'])\n",
        "  model = model.to(device)\n",
        "  model_dict[model_name] = model\n",
        "\n",
        "logging.info(\"Total number of models: {}\".format(len(model_names) + len(model_names_v2)))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-02 18:40:01,693 INFO Total number of models: 40\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inhJeBV-9ic0"
      },
      "source": [
        "# MyEnsembleClassifier: Inherited from `EnsembleClassifier` with `predict`, `loss_gradient` and `loss_gradient` overwritten\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moVnk9Nwnk5y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd7dca3e-c711-4ad5-dd3d-36896ea1e2ca"
      },
      "source": [
        "np.random.choice(10, 20, replace=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([8, 1, 8, 4, 0, 4, 4, 4, 5, 1, 8, 1, 7, 4, 4, 5, 8, 4, 7, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ratuA8nTIKyu"
      },
      "source": [
        "class MyEnsembleClassifier(EnsembleClassifier):\n",
        "    def __init__(\n",
        "        self,\n",
        "        classifiers: List[ClassifierNeuralNetwork],\n",
        "        device,\n",
        "        infer_noise,\n",
        "        num_selected_models,\n",
        "        classifier_weights: Union[list, np.ndarray, None] = None,\n",
        "        channels_first: bool = False,\n",
        "        clip_values: Optional[\"CLIP_VALUES_TYPE\"] = None,\n",
        "        preprocessing_defences: Union[\"Preprocessor\", List[\"Preprocessor\"], None] = None,\n",
        "        postprocessing_defences: Union[\"Postprocessor\", List[\"Postprocessor\"], None] = None,\n",
        "        preprocessing: \"PREPROCESSING_TYPE\" = (0, 1),\n",
        "    ) -> None:\n",
        "      super().__init__(\n",
        "          classifiers=classifiers,\n",
        "          classifier_weights=classifier_weights,\n",
        "          channels_first=channels_first,\n",
        "          clip_values=clip_values,\n",
        "          preprocessing_defences=preprocessing_defences,\n",
        "          postprocessing_defences=postprocessing_defences,\n",
        "          preprocessing=preprocessing\n",
        "      )\n",
        "      self.device = device\n",
        "      self.infer_noise = infer_noise\n",
        "      self.num_models = len(classifiers)\n",
        "      self.num_selected_models = num_selected_models\n",
        "\n",
        "    def predict(self, x: np.ndarray, batch_size: int = 128, raw: bool = False, **kwargs) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Perform prediction for a batch of inputs. Predictions from classifiers should only be aggregated if they all\n",
        "        have the same type of output (e.g., probabilities). Otherwise, use `raw=True` to get predictions from all\n",
        "        models without aggregation. The same option should be used for logits output, as logits are not comparable\n",
        "        between models and should not be aggregated.\n",
        "        :param x: Test set.\n",
        "        :param batch_size: Size of batches.\n",
        "        :param raw: Return the individual classifier raw outputs (not aggregated).\n",
        "        :return: Array of predictions of shape `(nb_inputs, nb_classes)`, or of shape\n",
        "                 `(nb_classifiers, nb_inputs, nb_classes)` if `raw=True`.\n",
        "        \"\"\"\n",
        "        # indices = [i for i in range(self.num_models)]\n",
        "        # random.shuffle(indices)\n",
        "        # indices = indices[:self.num_selected_models]\n",
        "\n",
        "        # selected_classifiers = random.choice(self._classifiers, k=self.num_selected_models)\n",
        "\n",
        "        indices = np.random.choice(self.num_models, self.num_selected_models, replace=True)\n",
        "\n",
        "        x_noised = np.float32(x + np.random.randn(*x.shape) * self.infer_noise)\n",
        "\n",
        "        preds = np.array(\n",
        "            [softmax(self._classifiers[i].predict(x_noised), axis=1) for i in indices]\n",
        "        )\n",
        "\n",
        "        # preds = np.array(\n",
        "        #     [softmax(selected_classifiers.predict(x_noised), axis=1) for sc in selected_classifiers]\n",
        "        # )\n",
        "\n",
        "        if raw:\n",
        "            return preds\n",
        "\n",
        "        # 6 x 100\n",
        "        preds_classes = np.argmax(preds, axis=2)\n",
        "        row, col = preds_classes.shape\n",
        "\n",
        "        # 100,\n",
        "        majority_vote = np.array(\n",
        "            [\n",
        "             np.bincount(preds_classes[:,c]).argmax()\n",
        "             for c in range(col)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        mask = preds_classes == majority_vote\n",
        "        mask = np.repeat(np.expand_dims(mask, axis=2), repeats=10, axis=2)\n",
        "\n",
        "        # var_z = np.sum(mask * preds, axis=0)\n",
        "\n",
        "        # Aggregate predictions only at probabilities level, as logits are not comparable between models\n",
        "        var_z = np.sum(mask * preds, axis=0)\n",
        "\n",
        "        # Apply postprocessing\n",
        "        predictions = self._apply_postprocessing(preds=var_z, fit=False)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "\n",
        "    def loss_gradient(self, x: np.ndarray, y: np.ndarray, raw: bool = False, **kwargs) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute the gradient of the loss function w.r.t. `x`.\n",
        "        :param x: Sample input with shape as expected by the model.\n",
        "        :param y: Target values (class labels) one-hot-encoded of shape (nb_samples, nb_classes) or indices of shape\n",
        "                  (nb_samples,).\n",
        "        :param raw: Return the individual classifier raw outputs (not aggregated).\n",
        "        :return: Array of gradients of the same shape as `x`. If `raw=True`, shape becomes `[nb_classifiers, x.shape]`.\n",
        "        \"\"\"\n",
        "\n",
        "        # indices = [i for i in range(self.num_models)]\n",
        "        # random.shuffle(indices)\n",
        "        # indices = indices[:self.num_selected_models]\n",
        "\n",
        "        indices = np.random.choice(self.num_models, self.num_selected_models, replace=True)\n",
        "\n",
        "        \n",
        "        x_noised = np.float32(x + np.random.randn(*x.shape) * self.infer_noise)\n",
        "\n",
        "        grads = np.array(\n",
        "            [\n",
        "                self._classifiers[i].loss_gradient(x_noised, y)\n",
        "                for i in indices\n",
        "            ]\n",
        "        )\n",
        "        if raw:\n",
        "            return grads\n",
        "\n",
        "        return np.sum(grads, axis=0)\n",
        "\n",
        "    def loss_gradient_framework(self, x: \"torch.Tensor\", y: \"torch.Tensor\", **kwargs) -> \"torch.Tensor\":\n",
        "        \"\"\"\n",
        "        Compute the gradient of the loss function w.r.t. `x`.\n",
        "        :param x: Sample input with shape as expected by the model.\n",
        "        :param y: Target values (class labels) one-hot-encoded of shape (nb_samples, nb_classes) or indices of shape\n",
        "                  (nb_samples,).\n",
        "        :param raw: Return the individual classifier raw outputs (not aggregated).\n",
        "        :return: Array of gradients of the same shape as `x`. If `raw=True`, shape becomes `[nb_classifiers, x.shape]`.\n",
        "        \"\"\"\n",
        "\n",
        "        # indices = [i for i in range(self.num_models)]\n",
        "        # random.shuffle(indices)\n",
        "        # indices = indices[:self.num_selected_models]\n",
        "        \n",
        "        indices = np.random.choice(self.num_models, self.num_selected_models, replace=True)\n",
        "\n",
        "        noise = torch.randn(x.size()) * self.infer_noise\n",
        "        noise = noise.to(device)\n",
        "        x_noised = x + noise\n",
        "\n",
        "\n",
        "        grads = [\n",
        "                  self._classifiers[i].loss_gradient_framework(x_noised, y).unsqueeze(0)\n",
        "                  for i in indices\n",
        "                ]\n",
        "        \n",
        "        return torch.sum(torch.cat(grads, axis=0), axis=0)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKzm9D8h5bFR"
      },
      "source": [
        "# Creating classifier list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z0e7i6HJi7i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f90f5b4d-e47c-44bf-e46e-40eabe01f4f3"
      },
      "source": [
        "nb_classes = 10\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "classifier_list = []\n",
        "for model_name in model_dict:\n",
        "  classifier_list.append(PyTorchClassifier(\n",
        "    model=model_dict[model_name],\n",
        "    clip_values=(0, 1),\n",
        "    loss=criterion,\n",
        "    input_shape=(3, img_size, img_size),\n",
        "    nb_classes=nb_classes,\n",
        "))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-02 18:40:57,150 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,159 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,166 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,173 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,180 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,187 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,194 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,202 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,208 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,215 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,222 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,228 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,234 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,242 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,250 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,259 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,267 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,275 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,284 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,292 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,301 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,309 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,316 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,325 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,333 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,341 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,349 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,355 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,365 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,372 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,380 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,386 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,392 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,400 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,407 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,414 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,420 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,427 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,435 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 18:40:57,442 INFO Inferred 1 hidden layers on PyTorch classifier.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emCbgIPQ5yA5"
      },
      "source": [
        "# Experiment Settings\n",
        "Refer to [this google doc](https://docs.google.com/document/d/1VaS5THALdudd_63Zv7ZR2u8rj8r8iDF6xgXzzLHXR8w/edit)\n",
        "\n",
        "1.   `num_selected_models`=15, `infer_noise`=[0.0025, 0.005, 0.01, 0.05, 0.08, 0.1], `eps`=[0.0025, 0.01, 0.1, 0.5, 0.8], `norm`=Linf\n",
        "2.   `num_selected_models`=[5, 10, 15, 20, 25, 30, 35, 40], `infer_noise`=0.1, `eps`=[0.0025, 0.01, 0.1, 0.5, 0.8], `norm`=Linf\n",
        "3.   `num_selected_models`=15, `infer_noise`=[0.0025, 0.005, 0.01, 0.05, 0.08, 0.1], `eps`=[0.1, 0.5, 0.8, 0.9, 1], `norm`=L2\n",
        "4.   `num_selected_models`=[5, 10, 15, 20, 25, 30, 35, 40], `infer_noise`=0.1, `eps`=[0.1, 0.5, 0.8, 0.9, 1], `norm`=L2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfpLU-YJlx92"
      },
      "source": [
        "### Arbitrary Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xzKyhJXww1M"
      },
      "source": [
        "for batch_idx, (X, Y) in enumerate(testloader):\n",
        "  print(X.shape)\n",
        "  print(Y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "64xN40H5nqHR",
        "outputId": "0c7e12d1-1c0f-484f-d3cf-a51315cba229"
      },
      "source": [
        "num_selected_models = 50\n",
        "norm = np.inf\n",
        "infer_noise = 0.08\n",
        "eps = 8/255\n",
        "logging.info(\"Current experiment setting: eps={}, inference noise={}\".format(eps, infer_noise))\n",
        "my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
        "                                              device, \n",
        "                                              infer_noise=infer_noise,\n",
        "                                              num_selected_models=num_selected_models,\n",
        "                                              clip_values=[0., 1.], \n",
        "                                              channels_first=True)\n",
        "attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=eps, eps_step=0.025, norm=norm, max_iter=20, batch_size=test_batchsize)\n",
        "Acc_adv = []\n",
        "Acc_nat = []\n",
        "for batch_idx, (X, Y) in enumerate(testloader):\n",
        "  x_test = X.numpy()\n",
        "  print(x_test.shape)\n",
        "  x_test_nat = x_test + np.random.randn(*x_test.shape) * eps\n",
        "  y_test = Y.numpy()\n",
        "  print(y_test.shape)\n",
        "  predictions_nat = my_ensemble_classifier.predict(x_test_nat)\n",
        "  accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
        "  Acc_nat.append(accuracy_nat)\n",
        "\n",
        "  x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "  predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
        "  accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
        "  Acc_adv.append(accuracy_adv)\n",
        "      \n",
        "  logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
        "  logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
        "    \n",
        "Acc_adv = np.asanyarray(Acc_adv)\n",
        "Acc_nat = np.asanyarray(Acc_nat)\n",
        "idx = np.arange(0, len(testset)/test_batchsize).tolist()\n",
        "# res = {'Id': idx ,'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
        "# df = pd.DataFrame(res) \n",
        "# # saving the dataframe \n",
        "# df.to_csv('./gdrive/My Drive/IDL_Project/results/PGD_exp1_infer_noise{}_eps{}.csv'.format(infer_noise, eps),index=False)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-02 19:00:57,464 INFO Current experiment setting: eps=0.03137254901960784, inference noise=0.08\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(200, 3, 224, 224)\n",
            "(200,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "PGD - Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-beeeea9c9769>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mAcc_nat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_nat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mx_test_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0mpredictions_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_ensemble_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_adv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0maccuracy_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_adv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/art/attacks/attack.py\u001b[0m in \u001b[0;36mreplacement_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfunc_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mreplacement_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfunc_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mbatch_index_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_id\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0madv_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index_1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_index_2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_random_init\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py\u001b[0m in \u001b[0;36m_generate_batch\u001b[0;34m(self, x, targets, mask)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi_max_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             adv_x = self._compute_torch(\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0madv_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_random_init\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi_max_iter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m             )\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py\u001b[0m in \u001b[0;36m_compute_torch\u001b[0;34m(self, x, x_init, y, mask, eps, eps_step, random_init)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;31m# Get perturbation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mperturbation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_perturbation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_adv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;31m# Apply perturbation and clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py\u001b[0m in \u001b[0;36m_compute_perturbation\u001b[0;34m(self, x, y, mask)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Get gradient wrt loss; invert it if attack is targeted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_gradient_framework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargeted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# Apply norm bound\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-db2fa2bd7026>\u001b[0m in \u001b[0;36mloss_gradient_framework\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m         grads = [\n\u001b[1;32m    140\u001b[0m                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_classifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_gradient_framework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_noised\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 ]\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-db2fa2bd7026>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m         grads = [\n\u001b[1;32m    140\u001b[0m                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_classifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_gradient_framework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_noised\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 ]\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/art/estimators/classification/pytorch.py\u001b[0m in \u001b[0;36mloss_gradient_framework\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# Compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 15.90 GiB total capacity; 14.14 GiB already allocated; 35.88 MiB free; 15.05 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEzaTmhp7qFg"
      },
      "source": [
        "# Experiment 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIZS0qT17u3_"
      },
      "source": [
        "num_selected_models = 15\n",
        "norm = np.inf\n",
        "infer_noise_list = [0.0025, 0.005, 0.01, 0.05, 0.08, 0.1]\n",
        "eps_list = [0.0025, 0.01, 0.1, 0.5, 0.8]\n",
        "\n",
        "for infer_noise in infer_noise_list:\n",
        "  for eps in eps_list:\n",
        "    my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
        "                                                  device, \n",
        "                                                  infer_noise=infer_noise,\n",
        "                                                  num_selected_models=num_selected_models,\n",
        "                                                  clip_values=[0., 1.], \n",
        "                                                  channels_first=True)\n",
        "    attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=eps, eps_step=eps/3, norm=norm)\n",
        "    Acc_adv = []\n",
        "    Acc_nat = []\n",
        "    for batch_idx, (X, Y) in enumerate(testloader):\n",
        "      x_test = X.numpy()\n",
        "      y_test = Y.numpy()\n",
        "      predictions_nat = my_ensemble_classifier.predict(x_test)\n",
        "      accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
        "      Acc_nat.append(accuracy_nat)\n",
        "\n",
        "      x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "      predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
        "      accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
        "      Acc_adv.append(accuracy_adv)\n",
        "      \n",
        "      logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
        "      logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
        "    \n",
        "    Acc_adv = np.asanyarray(Acc_adv)\n",
        "    Acc_nat = np.asanyarray(Acc_nat)\n",
        "    idx = np.arange(0, len(testset)/test_batchsize).tolist()\n",
        "    res = {'Id': idx ,'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
        "    df = pd.DataFrame(res) \n",
        "    # saving the dataframe \n",
        "    df.to_csv('./gdrive/My Drive/IDL_Project/results/PGD_exp1_infer_noise{}_eps{}.csv'.format(infer_noise, eps),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH-xmYJL8n14"
      },
      "source": [
        "# Experiment 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e44AHWvw8qxI"
      },
      "source": [
        "num_selected_models_list = [5, 10, 15, 20, 25, 30, 35, 40]\n",
        "norm = np.inf\n",
        "infer_noise = 0.1\n",
        "eps_list = [0.0025, 0.01, 0.1, 0.5, 0.8]\n",
        "\n",
        "for num_selected_models in num_selected_models_list:\n",
        "  for eps in eps_list:\n",
        "    my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
        "                                                  device, \n",
        "                                                  infer_noise=infer_noise,\n",
        "                                                  num_selected_models=num_selected_models,\n",
        "                                                  clip_values=[0., 1.], \n",
        "                                                  channels_first=True)\n",
        "    attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=eps, eps_step=eps/3, norm=norm)\n",
        "    Acc_adv = []\n",
        "    Acc_nat = []\n",
        "    for batch_idx, (X, Y) in enumerate(testloader):\n",
        "      x_test = X.numpy()\n",
        "      y_test = Y.numpy()\n",
        "      x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "      predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
        "      predictions_nat = my_ensemble_classifier.predict(x_test)\n",
        "      accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
        "      accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
        "      Acc_adv.append(accuracy_adv)\n",
        "      Acc_nat.append(accuracy_nat)\n",
        "      logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
        "      logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
        "    \n",
        "    Acc_adv = np.asanyarray(Acc_adv)\n",
        "    Acc_nat = np.asanyarray(Acc_nat)\n",
        "    idx = np.arange(0, len(testset)/test_batchsize).tolist()\n",
        "    res = {'Id': idx ,'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
        "    df = pd.DataFrame(res) \n",
        "    # saving the dataframe \n",
        "    df.to_csv('./gdrive/My Drive/IDL_Project/results/PGD_exp2_N{}_eps{}.csv'.format(num_selected_models, eps),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng7Xe9Zq8rai"
      },
      "source": [
        "# Experiment 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3rzYvCu8uoa"
      },
      "source": [
        "num_selected_models = 15\n",
        "norm = 2\n",
        "infer_noise_list = [0.0025, 0.005, 0.01, 0.05, 0.08, 0.1]\n",
        "eps_list = [0.1, 0.5, 0.8, 0.9, 1]\n",
        "\n",
        "for infer_noise in infer_noise_list:\n",
        "  for eps in eps_list:\n",
        "    my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
        "                                                  device, \n",
        "                                                  infer_noise=infer_noise,\n",
        "                                                  num_selected_models=num_selected_models,\n",
        "                                                  clip_values=[0., 1.], \n",
        "                                                  channels_first=True)\n",
        "    attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=eps, eps_step=eps/3, norm=norm)\n",
        "    Acc_adv = []\n",
        "    Acc_nat = []\n",
        "    for batch_idx, (X, Y) in enumerate(testloader):\n",
        "      x_test = X.numpy()\n",
        "      y_test = Y.numpy()\n",
        "      x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "      predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
        "      predictions_nat = my_ensemble_classifier.predict(x_test)\n",
        "      accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
        "      accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
        "      Acc_adv.append(accuracy_adv)\n",
        "      Acc_nat.append(accuracy_nat)\n",
        "      logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
        "      logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
        "    \n",
        "    Acc_adv = np.asanyarray(Acc_adv)\n",
        "    Acc_nat = np.asanyarray(Acc_nat)\n",
        "    idx = np.arange(0, len(testset)/test_batchsize).tolist()\n",
        "    res = {'Id': idx ,'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
        "    df = pd.DataFrame(res) \n",
        "    # saving the dataframe \n",
        "    df.to_csv('./gdrive/My Drive/IDL_Project/results/PGD_exp3_infer_noise{}_eps{}.csv'.format(infer_noise, eps),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-w5HNiC8vDY"
      },
      "source": [
        "# Experiment 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTCAdeqj8w17"
      },
      "source": [
        "num_selected_models_list = [5, 10, 15, 20, 25, 30, 35, 40]\n",
        "norm = 2\n",
        "infer_noise = 0.1\n",
        "eps_list = [0.1, 0.5, 0.8, 0.9, 1]\n",
        "\n",
        "for num_selected_models in num_selected_models_list:\n",
        "  for eps in eps_list:\n",
        "    my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
        "                                                  device, \n",
        "                                                  infer_noise=infer_noise,\n",
        "                                                  num_selected_models=num_selected_models,\n",
        "                                                  clip_values=[0., 1.], \n",
        "                                                  channels_first=True)\n",
        "    attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=eps, eps_step=eps/3, norm=norm)\n",
        "    Acc_adv = []\n",
        "    Acc_nat = []\n",
        "    for batch_idx, (X, Y) in enumerate(testloader):\n",
        "      x_test = X.numpy()\n",
        "      y_test = Y.numpy()\n",
        "      x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "      predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
        "      predictions_nat = my_ensemble_classifier.predict(x_test)\n",
        "      accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
        "      accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
        "      Acc_adv.append(accuracy_adv)\n",
        "      Acc_nat.append(accuracy_nat)\n",
        "      logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
        "      logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
        "    \n",
        "    Acc_adv = np.asanyarray(Acc_adv)\n",
        "    Acc_nat = np.asanyarray(Acc_nat)\n",
        "    idx = np.arange(0, len(testset)/test_batchsize).tolist()\n",
        "    res = {'Id': idx ,'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
        "    df = pd.DataFrame(res) \n",
        "    # saving the dataframe \n",
        "    df.to_csv('./gdrive/My Drive/IDL_Project/results/PGD_exp4_N{}_eps{}.csv'.format(num_selected_models, eps),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdUjTkdWBdgh"
      },
      "source": [
        "# **The rest parts of the notebook was regarding previous work. Discard them for now.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP_Gxq-2jc-9"
      },
      "source": [
        "# FGM Attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmXkglUUKXO3"
      },
      "source": [
        "attack = FastGradientMethod(estimator=my_ensemble_classifier, eps=0.5, targeted=False, norm=2)\n",
        "Acc = []\n",
        "for batch_idx, (X, Y) in enumerate(testloader):\n",
        "    x_test = X.numpy()\n",
        "    y_test = Y.numpy()\n",
        "    # target = (y_test + 1) % 10\n",
        "    x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "    predictions = my_ensemble_classifier.predict(x_test_adv)\n",
        "    predictions_test = my_ensemble_classifier.predict(x_test)\n",
        "    accuracy = np.sum(np.argmax(predictions, axis=1) == y_test) / len(y_test)\n",
        "    acc_test = np.sum(np.argmax(predictions_test, axis=1) == y_test) / len(y_test)\n",
        "    # print(y_test.shape)\n",
        "    # acc_rate, coverage_rate = compute_accuracy(preds=predictions, labels=np.reshape(y_test, (test_batchsize, 1)))\n",
        "    # logging.info('acc rate: {}, coverage rate: {}'.format(acc_rate, coverage_rate))\n",
        "    Acc.append(accuracy)\n",
        "    logging.info('accuracy: {}'.format(accuracy))\n",
        "    logging.info('accuracy non adv: {}'.format(acc_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoYiKeGCjfDR"
      },
      "source": [
        "# PGD Attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZRM5XEZjchO"
      },
      "source": [
        "attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=0.5, eps_step=0.03)\n",
        "Acc = []\n",
        "for batch_idx, (X, Y) in enumerate(testloader):\n",
        "    x_test = X.numpy()\n",
        "    y_test = Y.numpy()\n",
        "    x_test_adv = attack.generate(torch.from_numpy(x_test))\n",
        "    predictions = my_ensemble_classifier.predict(x_test_adv)\n",
        "    accuracy = np.sum(np.argmax(predictions, axis=1) == np.array(y_test)) / len(y_test)\n",
        "    Acc.append(accuracy)\n",
        "    logging.info('accuracy: {}'.format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyMeGON8BFNL"
      },
      "source": [
        "# Experiment settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjUPKB3jvijF"
      },
      "source": [
        "fgm_epsilon = np.array([0.005, 0.01, 0.02, 0.04, 0.08])\n",
        "pgd_epsilon = 8.0 / 255.0 / np.array([2.0, 4.0, 8.0, 16.0])\n",
        "diff = np.array([1e-3, 1e-4, 1e-5, 1e-6])\n",
        "pgd_epsilon_step = pgd_epsilon - diff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XG44M27BNIT"
      },
      "source": [
        "Experiment on inference noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U79zJmCsDEKc"
      },
      "source": [
        "noise_stds = [0.02, 0.04, 0.06, 0.08, 0.1]\n",
        "for i in range(len(noise_stds)):\n",
        "  my_ensemble = MyEnsemble(model_dict, num_models_selected, noise_std=noise_stds[i], num_classes=10)\n",
        "  classifier = PyTorchClassifier(\n",
        "      model=my_ensemble,\n",
        "      clip_values=(0, 1),\n",
        "      loss=nn.CrossEntropyLoss(),\n",
        "      input_shape=(3, 224, 224),\n",
        "      nb_classes=10)\n",
        "  attack = FastGradientMethod(estimator=classifier, eps=0.01)\n",
        "  Acc = []\n",
        "  for batch_idx, (X, Y) in enumerate(testloader):\n",
        "    x_test = X.numpy()\n",
        "    y_test = Y.numpy()\n",
        "    x_test_adv = attack.generate(torch.from_numpy(x_test))\n",
        "    predictions = classifier.predict(x_test_adv)\n",
        "    accuracy = np.sum(np.argmax(predictions, axis=1) == np.array(y_test)) / len(y_test)\n",
        "    Acc.append(accuracy)\n",
        "    logging.info('accuracy: {}'.format(accuracy))\n",
        "  \n",
        "  logging.info(\"Accuracy for eps={}: {}\".format(fgm_epsilon[i], np.mean(np.array(Acc))))\n",
        "\n",
        "  pred_list = np.asanyarray(Acc)\n",
        "  idx = np.arange(0,len(testset)/test_batchsize).tolist()\n",
        "  dict = {'Id': idx ,'acc': pred_list} \n",
        "  df = pd.DataFrame(dict) \n",
        "  # saving the dataframe \n",
        "  df.to_csv('./gdrive/My Drive/IDL_Project/results/FGM_result_noise{}_{}.csv'.format(noise_stds[i], datetime.datetime.now()),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LotwWsVpMYKN"
      },
      "source": [
        "pred_list = np.asanyarray(Acc)\n",
        "idx = np.arange(0,len(testset)/test_batchsize).tolist()\n",
        "dict = {'Id': idx ,'acc': pred_list} \n",
        "df = pd.DataFrame(dict) \n",
        "# saving the dataframe \n",
        "df.to_csv('./gdrive/My Drive/IDL_Project/results/FGM_result_eps{}_{}.csv'.format(fgm_epsilon[-1], datetime.datetime.now()),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqIgjfhwBXVL"
      },
      "source": [
        "Plot PGD adversarial examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Eu7qdDQMnGq"
      },
      "source": [
        "attack = ProjectedGradientDescentPyTorch(estimator=classifier, eps=pgd_epsilon[0], eps_step=pgd_epsilon_step[0])\n",
        "Acc = []\n",
        "adv = []\n",
        "orig = []\n",
        "labels = []\n",
        "pred = []\n",
        "for batch_idx, (X, Y) in enumerate(testloader):\n",
        "  if batch_idx > 0:\n",
        "    break\n",
        "  x_test = X.numpy()\n",
        "  y_test = Y.numpy()\n",
        "  x_test_adv = attack.generate(torch.from_numpy(x_test))\n",
        "  predictions = classifier.predict(x_test_adv)\n",
        "  accuracy = np.sum(np.argmax(predictions, axis=1) == np.array(y_test)) / len(y_test)\n",
        "  Acc.append(accuracy)\n",
        "  adv.append(x_test_adv)\n",
        "  orig.append(x_test)\n",
        "  labels.append(y_test)\n",
        "  pred.append(predictions)\n",
        "  logging.info('accuracy: {}'.format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8sj2LdnNV5n"
      },
      "source": [
        "example_adv = adv[-1]\n",
        "example_orig = orig[-1]\n",
        "label = labels[-1]\n",
        "predi = np.argmax(predictions, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoD6QHEwNt4T"
      },
      "source": [
        "print(label)\n",
        "print(predi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnkXnHRePFeW"
      },
      "source": [
        "idx = 2\n",
        "selected_adv = example_adv[idx]\n",
        "selected_orig = example_orig[idx]\n",
        "selected_lab = label[idx]\n",
        "selected_pred = predi[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZuD1OfoWroX"
      },
      "source": [
        "print(selected_lab)\n",
        "print(selected_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkxSl4PTPab0"
      },
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(np.transpose(selected_orig, (1, 2, 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tpehq_0hVobG"
      },
      "source": [
        "plt.imshow(np.transpose(selected_adv, (1, 2, 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuLSJh8QVsHy"
      },
      "source": [
        "plt.imshow(10*(np.transpose(selected_orig, (1, 2, 0)) - np.transpose(selected_adv, (1, 2, 0))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lOaioJIBh1G"
      },
      "source": [
        "Experiment PGD attacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuxeEuf-8UVE"
      },
      "source": [
        "for i in range(len(pgd_epsilon)):\n",
        "  attack = FastGradientMethod(estimator=classifier, eps=fgm_epsilon[i])\n",
        "  Acc = []\n",
        "  for batch_idx, (X, Y) in enumerate(testloader):\n",
        "    x_test = X.numpy()\n",
        "    y_test = Y.numpy()\n",
        "    x_test_adv = attack.generate(torch.from_numpy(x_test))\n",
        "    predictions = classifier.predict(x_test_adv)\n",
        "    accuracy = np.sum(np.argmax(predictions, axis=1) == np.array(y_test)) / len(y_test)\n",
        "    Acc.append(accuracy)\n",
        "    logging.info('accuracy: {}'.format(accuracy))\n",
        "  \n",
        "  logging.info(\"Accuracy for eps={}: {}\".format(fgm_epsilon[i], np.mean(np.array(Acc))))\n",
        "\n",
        "  pred_list = np.asanyarray(Acc)\n",
        "  idx = np.arange(0,len(testset)/test_batchsize).tolist()\n",
        "  dict = {'Id': idx ,'acc': pred_list} \n",
        "  df = pd.DataFrame(dict) \n",
        "  # saving the dataframe \n",
        "  df.to_csv('./gdrive/My Drive/IDL_Project/results/FGM_result_eps{}_{}.csv'.format(fgm_epsilon[i], datetime.datetime.now()),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}