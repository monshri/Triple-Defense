{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/KaichengDING/Triple-Defense/blob/main/idl_project_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8idp0UC0Oj6"
   },
   "source": [
    "# Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4ESeViSYxQfa"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgPpHa2d0Rt4"
   },
   "source": [
    "# Install ART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LzKNCLpGws5x",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: adversarial-robustness-toolbox in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (1.4.3)\n",
      "Requirement already satisfied: pydub in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from adversarial-robustness-toolbox) (0.24.1)\n",
      "Requirement already satisfied: cma in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from adversarial-robustness-toolbox) (3.0.3)\n",
      "Requirement already satisfied: statsmodels in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from adversarial-robustness-toolbox) (0.11.0)\n",
      "Requirement already satisfied: matplotlib in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from adversarial-robustness-toolbox) (3.1.3)\n",
      "Requirement already satisfied: ffmpeg-python in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from adversarial-robustness-toolbox) (0.2.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from adversarial-robustness-toolbox) (1.18.1)\n",
      "Requirement already satisfied: scikit-learn==0.22.2 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from adversarial-robustness-toolbox) (0.22.2)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from adversarial-robustness-toolbox) (1.14.0)\n",
      "Requirement already satisfied: mypy in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from adversarial-robustness-toolbox) (0.790)\n",
      "Requirement already satisfied: resampy in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from adversarial-robustness-toolbox) (0.2.2)\n",
      "Requirement already satisfied: Pillow in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from adversarial-robustness-toolbox) (7.0.0)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from adversarial-robustness-toolbox) (4.42.1)\n",
      "Requirement already satisfied: scipy==1.4.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from adversarial-robustness-toolbox) (1.4.1)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from adversarial-robustness-toolbox) (45.2.0.post20200210)\n",
      "Requirement already satisfied: patsy>=0.5 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from statsmodels->adversarial-robustness-toolbox) (0.5.1)\n",
      "Requirement already satisfied: pandas>=0.21 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from statsmodels->adversarial-robustness-toolbox) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from matplotlib->adversarial-robustness-toolbox) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from matplotlib->adversarial-robustness-toolbox) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from matplotlib->adversarial-robustness-toolbox) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from matplotlib->adversarial-robustness-toolbox) (0.10.0)\n",
      "Requirement already satisfied: future in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from ffmpeg-python->adversarial-robustness-toolbox) (0.18.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from scikit-learn==0.22.2->adversarial-robustness-toolbox) (0.14.1)\n",
      "Requirement already satisfied: mypy-extensions<0.5.0,>=0.4.3 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from mypy->adversarial-robustness-toolbox) (0.4.3)\n",
      "Requirement already satisfied: typed-ast<1.5.0,>=1.4.0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from mypy->adversarial-robustness-toolbox) (1.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from mypy->adversarial-robustness-toolbox) (3.7.4.3)\n",
      "Requirement already satisfied: numba>=0.32 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from resampy->adversarial-robustness-toolbox) (0.48.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from pandas>=0.21->statsmodels->adversarial-robustness-toolbox) (2019.3)\n",
      "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from numba>=0.32->resampy->adversarial-robustness-toolbox) (0.31.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install adversarial-robustness-toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HL8u7HvJ0Xka"
   },
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6SqUPdAKkDIn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import PIL\n",
    "\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "from art.attacks.evasion import FastGradientMethod\n",
    "from art.attacks.evasion import ProjectedGradientDescentPyTorch\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "\n",
    "from art.estimators.classification import EnsembleClassifier\n",
    "from typing import List, Optional, Union, TYPE_CHECKING\n",
    "from art.estimators.classification.classifier import ClassifierNeuralNetwork\n",
    "from scipy.special import softmax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcOnXEFW1m4K"
   },
   "source": [
    "# Model Definition\n",
    "`ShuffleNet` and `ShuffleNetV2` are actually identical, but models with seed number smaller than 200 are created using ShuffleNet and models with seed number greater than 200 are created using ShuffleNetV2. In order to load all models into ensemble, the two definitions are provided here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PfyaOczXoRqQ"
   },
   "outputs": [],
   "source": [
    "class ShuffleNet(nn.Module):\n",
    "    def __init__(self, nb_classes =10):\n",
    "        super(ShuffleNet, self).__init__()\n",
    "        self.shuffle = models.shufflenet_v2_x2_0()\n",
    "        self.linear = nn.Linear(1000, nb_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.shuffle(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ShuffleNetV2(nn.Module):\n",
    "    def __init__(self, nb_classes=10):\n",
    "        super(ShuffleNetV2, self).__init__()\n",
    "        self.shufflenet = models.shufflenet_v2_x2_0()\n",
    "        self.linear = nn.Linear(1000, nb_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.shufflenet(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlygMiHF0iSB"
   },
   "source": [
    "# Logging Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dXX4443Y16Ul"
   },
   "outputs": [],
   "source": [
    "# configure logging\n",
    "logger = logging.getLogger(\"\")\n",
    "\n",
    "# reset handler\n",
    "for handler in logging.root.handlers[:]:\n",
    "  logging.root.removeHandler(handler)\n",
    "\n",
    "# set handler\n",
    "stream_hdlr = logging.StreamHandler()\n",
    "# logging on colab\n",
    "# file_hdlr = logging.FileHandler('/content/gdrive/My Drive/IDL_Project/logs/log_{}.log'.format(datetime.datetime.now()))\n",
    "# logging on ec2\n",
    "file_hdlr = logging.FileHandler('/home/ubuntu/project/logs/log_{}.log'.format(datetime.datetime.now()))\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "stream_hdlr.setFormatter(formatter)\n",
    "file_hdlr.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(stream_hdlr)\n",
    "logger.addHandler(file_hdlr)\n",
    "\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFiB-FbgT9sk"
   },
   "source": [
    "# Get Test Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XV2pPKKzeESx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d44e2b88624a6988fe0f87ea073e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
     ]
    }
   ],
   "source": [
    "test_batchsize = 200\n",
    "num_workers = 4\n",
    "img_size = 224\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize(size=img_size),\n",
    "                     transforms.ToTensor(),\n",
    "                     transforms.Normalize((0, 0, 0), (1, 1, 1))])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=test_batchsize, shuffle=False, num_workers=num_workers, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOPbwP7FTWEY"
   },
   "source": [
    "# Load Saved Models\n",
    "All saved models will be loaded into `model_dict`. Models with seed number smaller than 200 belong to `ShuffleNet` class, and models with seed number greater than 200 belong to `ShuffleNetV2` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MUiLYb1EeNtS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-30 16:55:34,827 INFO Total number of models: 40\n"
     ]
    }
   ],
   "source": [
    "# models with 0.1 training noise\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "model_names = ['ShuffleNet_1',\n",
    "               'ShuffleNet_2',\n",
    "               'ShuffleNet_3',\n",
    "               'ShuffleNet_4',\n",
    "               'ShuffleNet_5',\n",
    "               'ShuffleNet_6',\n",
    "               'ShuffleNet_7',\n",
    "               'ShuffleNet_8',\n",
    "               'ShuffleNet_31',\n",
    "               'ShuffleNet_32',\n",
    "               'ShuffleNet_33',\n",
    "               'ShuffleNet_34',\n",
    "               'ShuffleNet_35',\n",
    "               'ShuffleNet_36',\n",
    "               'ShuffleNet_37',\n",
    "               'ShuffleNet_38',\n",
    "               'ShuffleNet_39',\n",
    "               'ShuffleNet_61',\n",
    "               'ShuffleNet_62',\n",
    "               'ShuffleNet_63',\n",
    "               'ShuffleNet_64',\n",
    "               'ShuffleNet_65',\n",
    "               'ShuffleNet_66',\n",
    "               'ShuffleNet_67',\n",
    "               'ShuffleNet_68',\n",
    "               'ShuffleNet_69',\n",
    "               'ShuffleNet_70',\n",
    "               'ShuffleNet_71',\n",
    "               'ShuffleNet_72',\n",
    "               'ShuffleNet_73']\n",
    "\n",
    "model_names_v2 = ['ShuffleNet_200', \n",
    "                  'ShuffleNet_201',\n",
    "                  'ShuffleNet_202',\n",
    "                  'ShuffleNet_203',\n",
    "                  'ShuffleNet_204',\n",
    "                  'ShuffleNet_205',\n",
    "                  'ShuffleNet_206',\n",
    "                  'ShuffleNet_207',\n",
    "                  'ShuffleNet_208',\n",
    "                  'ShuffleNet_209']\n",
    "\n",
    "model_dict = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "  model = ShuffleNet()\n",
    "#   model_data = torch.load('/content/gdrive/My Drive/IDL_Project/modelS/{}'.format(model_name), map_location=torch.device('cpu'))\n",
    "  model_data = torch.load('/home/ubuntu/project/models/{}'.format(model_name), map_location=torch.device('cpu'))\n",
    "  model.load_state_dict(model_data['model_state_dict'])\n",
    "  model = model.to(device)\n",
    "  model_dict[model_name] = model\n",
    "\n",
    "for model_name in model_names_v2:\n",
    "  model = ShuffleNetV2()\n",
    "#   model_data = torch.load('/content/gdrive/My Drive/IDL_Project/modelS/{}'.format(model_name), map_location=torch.device('cpu'))\n",
    "  model_data = torch.load('/home/ubuntu/project/models/{}'.format(model_name), map_location=torch.device('cpu'))\n",
    "  model.load_state_dict(model_data['model_state_dict'])\n",
    "  model = model.to(device)\n",
    "  model_dict[model_name] = model\n",
    "\n",
    "logging.info(\"Total number of models: {}\".format(len(model_names) + len(model_names_v2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inhJeBV-9ic0"
   },
   "source": [
    "# MyEnsembleClassifier: Inherited from `EnsembleClassifier` with `predict`, `loss_gradient` and `loss_gradient` overwritten\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ratuA8nTIKyu"
   },
   "outputs": [],
   "source": [
    "class MyEnsembleClassifier(EnsembleClassifier):\n",
    "    def __init__(\n",
    "        self,\n",
    "        classifiers: List[ClassifierNeuralNetwork],\n",
    "        device,\n",
    "        infer_noise,\n",
    "        num_selected_models,\n",
    "        classifier_weights: Union[list, np.ndarray, None] = None,\n",
    "        channels_first: bool = False,\n",
    "        clip_values: Optional[\"CLIP_VALUES_TYPE\"] = None,\n",
    "        preprocessing_defences: Union[\"Preprocessor\", List[\"Preprocessor\"], None] = None,\n",
    "        postprocessing_defences: Union[\"Postprocessor\", List[\"Postprocessor\"], None] = None,\n",
    "        preprocessing: \"PREPROCESSING_TYPE\" = (0, 1),\n",
    "    ) -> None:\n",
    "      super().__init__(\n",
    "          classifiers=classifiers,\n",
    "          classifier_weights=classifier_weights,\n",
    "          channels_first=channels_first,\n",
    "          clip_values=clip_values,\n",
    "          preprocessing_defences=preprocessing_defences,\n",
    "          postprocessing_defences=postprocessing_defences,\n",
    "          preprocessing=preprocessing\n",
    "      )\n",
    "      self.device = device\n",
    "      self.infer_noise = infer_noise\n",
    "      self.num_models = len(classifiers)\n",
    "      self.num_selected_models = num_selected_models\n",
    "\n",
    "    def predict(self, x: np.ndarray, batch_size: int = 128, raw: bool = False, **kwargs) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform prediction for a batch of inputs. Predictions from classifiers should only be aggregated if they all\n",
    "        have the same type of output (e.g., probabilities). Otherwise, use `raw=True` to get predictions from all\n",
    "        models without aggregation. The same option should be used for logits output, as logits are not comparable\n",
    "        between models and should not be aggregated.\n",
    "        :param x: Test set.\n",
    "        :param batch_size: Size of batches.\n",
    "        :param raw: Return the individual classifier raw outputs (not aggregated).\n",
    "        :return: Array of predictions of shape `(nb_inputs, nb_classes)`, or of shape\n",
    "                 `(nb_classifiers, nb_inputs, nb_classes)` if `raw=True`.\n",
    "        \"\"\"\n",
    "        indices = [i for i in range(self.num_models)]\n",
    "        random.shuffle(indices)\n",
    "        indices = indices[:self.num_selected_models]\n",
    "\n",
    "        x_noised = np.float32(x + np.random.randn(*x.shape) * self.infer_noise)\n",
    "\n",
    "        preds = np.array(\n",
    "            [softmax(self._classifiers[i].predict(x_noised), axis=1) for i in indices]\n",
    "        )\n",
    "        if raw:\n",
    "            return preds\n",
    "\n",
    "        # 6 x 100\n",
    "        preds_classes = np.argmax(preds, axis=2)\n",
    "        row, col = preds_classes.shape\n",
    "\n",
    "        # 100,\n",
    "        majority_vote = np.array(\n",
    "            [\n",
    "             np.bincount(preds_classes[:,c]).argmax()\n",
    "             for c in range(col)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        mask = preds_classes == majority_vote\n",
    "        mask = np.repeat(np.expand_dims(mask, axis=2), repeats=10, axis=2)\n",
    "\n",
    "        var_z = np.sum(mask * preds, axis=0)\n",
    "\n",
    "        # Aggregate predictions only at probabilities level, as logits are not comparable between models\n",
    "        var_z = np.sum(mask * preds, axis=0)\n",
    "\n",
    "        # Apply postprocessing\n",
    "        predictions = self._apply_postprocessing(preds=var_z, fit=False)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def loss_gradient(self, x: np.ndarray, y: np.ndarray, raw: bool = False, **kwargs) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the gradient of the loss function w.r.t. `x`.\n",
    "        :param x: Sample input with shape as expected by the model.\n",
    "        :param y: Target values (class labels) one-hot-encoded of shape (nb_samples, nb_classes) or indices of shape\n",
    "                  (nb_samples,).\n",
    "        :param raw: Return the individual classifier raw outputs (not aggregated).\n",
    "        :return: Array of gradients of the same shape as `x`. If `raw=True`, shape becomes `[nb_classifiers, x.shape]`.\n",
    "        \"\"\"\n",
    "\n",
    "        indices = [i for i in range(self.num_models)]\n",
    "        random.shuffle(indices)\n",
    "        indices = indices[:self.num_selected_models]\n",
    "        \n",
    "        x_noised = np.float32(x + np.random.randn(*x.shape) * self.infer_noise)\n",
    "\n",
    "        grads = np.array(\n",
    "            [\n",
    "                self._classifiers[i].loss_gradient(x_noised, y)\n",
    "                for i in indices\n",
    "            ]\n",
    "        )\n",
    "        if raw:\n",
    "            return grads\n",
    "\n",
    "        return np.sum(grads, axis=0)\n",
    "\n",
    "    def loss_gradient_framework(self, x: \"torch.Tensor\", y: \"torch.Tensor\", **kwargs) -> \"torch.Tensor\":\n",
    "        \"\"\"\n",
    "        Compute the gradient of the loss function w.r.t. `x`.\n",
    "        :param x: Sample input with shape as expected by the model.\n",
    "        :param y: Target values (class labels) one-hot-encoded of shape (nb_samples, nb_classes) or indices of shape\n",
    "                  (nb_samples,).\n",
    "        :param raw: Return the individual classifier raw outputs (not aggregated).\n",
    "        :return: Array of gradients of the same shape as `x`. If `raw=True`, shape becomes `[nb_classifiers, x.shape]`.\n",
    "        \"\"\"\n",
    "\n",
    "        indices = [i for i in range(self.num_models)]\n",
    "        random.shuffle(indices)\n",
    "        indices = indices[:self.num_selected_models]\n",
    "        \n",
    "        noise = torch.randn(x.size()) * self.infer_noise\n",
    "        noise = noise.to(device)\n",
    "        x_noised = x + noise\n",
    "\n",
    "\n",
    "        grads = [\n",
    "                  self._classifiers[i].loss_gradient_framework(x_noised, y).unsqueeze(0)\n",
    "                  for i in indices\n",
    "                ]\n",
    "        \n",
    "        return torch.sum(torch.cat(grads, axis=0), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKzm9D8h5bFR"
   },
   "source": [
    "# Creating classifier list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9Z0e7i6HJi7i"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-30 17:27:59,666 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,670 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,674 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,678 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,681 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,685 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,688 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,692 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,696 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,699 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,703 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,707 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,711 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,715 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,719 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,722 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,726 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,729 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,733 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,737 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,742 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,745 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,749 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,754 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,758 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,761 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,765 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,769 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,773 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,777 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,781 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,784 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,788 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,791 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,795 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,798 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,803 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,806 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,810 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
      "2020-11-30 17:27:59,814 INFO Inferred 1 hidden layers on PyTorch classifier.\n"
     ]
    }
   ],
   "source": [
    "nb_classes = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "classifier_list = []\n",
    "for model_name in model_dict:\n",
    "  classifier_list.append(PyTorchClassifier(\n",
    "    model=model_dict[model_name],\n",
    "    clip_values=(0, 1),\n",
    "    loss=criterion,\n",
    "    input_shape=(3, img_size, img_size),\n",
    "    nb_classes=nb_classes,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emCbgIPQ5yA5"
   },
   "source": [
    "# Experiment Settings\n",
    "Refer to [this google doc](https://docs.google.com/document/d/1VaS5THALdudd_63Zv7ZR2u8rj8r8iDF6xgXzzLHXR8w/edit)\n",
    "\n",
    "1.   `num_selected_models`=15, `infer_noise`=[0.0025, 0.005, 0.01, 0.05, 0.08, 0.1], `eps`=[0.0025, 0.01, 0.1, 0.5, 0.8], `norm`=Linf\n",
    "2.   `num_selected_models`=[5, 10, 15, 20, 25, 30, 35, 40], `infer_noise`=0.1, `eps`=[0.0025, 0.01, 0.1, 0.5, 0.8], `norm`=Linf\n",
    "3.   `num_selected_models`=15, `infer_noise`=[0.0025, 0.005, 0.01, 0.05, 0.08, 0.1], `eps`=[0.1, 0.5, 0.8, 0.9, 1], `norm`=L2\n",
    "4.   `num_selected_models`=[5, 10, 15, 20, 25, 30, 35, 40], `infer_noise`=0.1, `eps`=[0.1, 0.5, 0.8, 0.9, 1], `norm`=L2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEzaTmhp7qFg"
   },
   "source": [
    "# Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIZS0qT17u3_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-30 17:28:19,984 INFO \n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:25<14:35, 145.91s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:50<12:07, 145.53s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:16<09:42, 145.66s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:41<07:16, 145.59s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [12:07<04:51, 145.62s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:33<02:25, 145.68s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:21<00:00, 116.36s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:21<00:00, 921.47s/it]\n",
      "2020-11-30 17:43:59,232 INFO Success rate of attack: 14.00%\n",
      "2020-11-30 17:44:04,291 INFO accuracy (adversarial): 0.795\n",
      "2020-11-30 17:44:04,292 INFO accuracy (natural): 0.93\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:24<14:29, 144.95s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:46<11:59, 143.97s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:11<09:37, 144.29s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:33<07:10, 143.54s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:58<04:47, 143.93s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:20<02:23, 143.41s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:05<00:00, 114.00s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:05<00:00, 905.92s/it]\n",
      "2020-11-30 17:59:25,531 INFO Success rate of attack: 9.50%\n",
      "2020-11-30 17:59:30,619 INFO accuracy (adversarial): 0.76\n",
      "2020-11-30 17:59:30,620 INFO accuracy (natural): 0.855\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:24<14:25, 144.21s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:56, 143.38s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:10<09:35, 143.78s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:31<07:09, 143.07s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:56<04:47, 143.61s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:17<02:22, 142.93s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:03<00:00, 113.56s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:03<00:00, 903.08s/it]\n",
      "2020-11-30 18:14:49,001 INFO Success rate of attack: 15.00%\n",
      "2020-11-30 18:14:54,065 INFO accuracy (adversarial): 0.835\n",
      "2020-11-30 18:14:54,066 INFO accuracy (natural): 0.94\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:23<14:22, 143.72s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:55, 143.09s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:09<09:33, 143.46s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:31<07:08, 142.86s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:55<04:46, 143.38s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:16<02:22, 142.75s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:02<00:00, 113.50s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:02<00:00, 902.31s/it]\n",
      "2020-11-30 18:30:11,691 INFO Success rate of attack: 11.50%\n",
      "2020-11-30 18:30:16,778 INFO accuracy (adversarial): 0.8\n",
      "2020-11-30 18:30:16,779 INFO accuracy (natural): 0.895\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:23<14:20, 143.33s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:54, 142.99s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:09<09:33, 143.29s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:31<07:08, 142.84s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:55<04:46, 143.28s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:17<02:22, 142.76s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:02<00:00, 113.62s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:02<00:00, 902.84s/it]\n",
      "2020-11-30 18:45:35,006 INFO Success rate of attack: 11.00%\n",
      "2020-11-30 18:45:40,116 INFO accuracy (adversarial): 0.87\n",
      "2020-11-30 18:45:40,116 INFO accuracy (natural): 0.93\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:22<14:14, 142.48s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:52, 142.59s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:08<09:31, 142.77s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:30<07:07, 142.63s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:54<04:45, 143.00s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:16<02:22, 142.70s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:02<00:00, 113.71s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:02<00:00, 902.81s/it]\n",
      "2020-11-30 19:00:58,360 INFO Success rate of attack: 13.00%\n",
      "2020-11-30 19:01:03,491 INFO accuracy (adversarial): 0.77\n",
      "2020-11-30 19:01:03,492 INFO accuracy (natural): 0.905\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:21<14:09, 141.58s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:51, 142.25s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:07<09:29, 142.34s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:31<07:07, 142.67s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:54<04:45, 142.82s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:17<02:22, 142.75s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:03<00:00, 113.91s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:03<00:00, 903.82s/it]\n",
      "2020-11-30 19:16:22,687 INFO Success rate of attack: 12.50%\n",
      "2020-11-30 19:16:27,756 INFO accuracy (adversarial): 0.835\n",
      "2020-11-30 19:16:27,757 INFO accuracy (natural): 0.875\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:21<14:06, 141.17s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:51, 142.24s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:07<09:27, 141.99s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:31<07:07, 142.65s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:53<04:44, 142.37s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:16<02:22, 142.74s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:03<00:00, 113.77s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:03<00:00, 903.07s/it]\n",
      "2020-11-30 19:31:46,095 INFO Success rate of attack: 12.50%\n",
      "2020-11-30 19:31:51,148 INFO accuracy (adversarial): 0.825\n",
      "2020-11-30 19:31:51,149 INFO accuracy (natural): 0.925\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:21<14:07, 141.23s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:51, 142.23s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:07<09:27, 141.93s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:31<07:08, 142.76s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:53<04:44, 142.32s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:17<02:22, 142.98s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:02<00:00, 113.61s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:02<00:00, 902.67s/it]\n",
      "2020-11-30 19:47:09,107 INFO Success rate of attack: 10.50%\n",
      "2020-11-30 19:47:14,159 INFO accuracy (adversarial): 0.775\n",
      "2020-11-30 19:47:14,160 INFO accuracy (natural): 0.885\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:21<14:08, 141.43s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:51, 142.33s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:07<09:28, 142.04s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:31<07:08, 142.83s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:53<04:44, 142.36s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:17<02:23, 143.08s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:02<00:00, 113.63s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:02<00:00, 902.88s/it]\n",
      "2020-11-30 20:02:32,337 INFO Success rate of attack: 10.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-30 20:02:37,408 INFO accuracy (adversarial): 0.79\n",
      "2020-11-30 20:02:37,409 INFO accuracy (natural): 0.895\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:21<14:11, 141.84s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:52, 142.46s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:07<09:28, 142.23s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:31<07:08, 142.85s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:53<04:44, 142.47s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:18<02:23, 143.17s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:03<00:00, 113.70s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:03<00:00, 903.12s/it]\n",
      "2020-11-30 20:17:55,807 INFO Success rate of attack: 11.50%\n",
      "2020-11-30 20:18:00,861 INFO accuracy (adversarial): 0.755\n",
      "2020-11-30 20:18:00,862 INFO accuracy (natural): 0.865\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:22<14:14, 142.40s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:53, 142.68s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:07<09:30, 142.55s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:31<07:08, 142.92s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:53<04:45, 142.67s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:18<02:23, 143.14s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:03<00:00, 113.71s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:03<00:00, 903.19s/it]\n",
      "2020-11-30 20:33:19,325 INFO Success rate of attack: 16.50%\n",
      "2020-11-30 20:33:24,397 INFO accuracy (adversarial): 0.75\n",
      "2020-11-30 20:33:24,397 INFO accuracy (natural): 0.925\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:23<14:20, 143.49s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:55, 143.14s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:08<09:32, 143.12s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:32<07:09, 143.13s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:54<04:45, 143.00s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:18<02:23, 143.16s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:03<00:00, 113.74s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:03<00:00, 903.43s/it]\n",
      "2020-11-30 20:48:43,114 INFO Success rate of attack: 16.00%\n",
      "2020-11-30 20:48:48,171 INFO accuracy (adversarial): 0.79\n",
      "2020-11-30 20:48:48,171 INFO accuracy (natural): 0.88\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:24<14:26, 144.34s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:57, 143.46s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:09<09:34, 143.69s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:31<07:09, 143.12s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:55<04:46, 143.23s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:17<02:23, 143.03s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:02<00:00, 113.59s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:02<00:00, 902.76s/it]\n",
      "2020-11-30 21:04:06,219 INFO Success rate of attack: 9.00%\n",
      "2020-11-30 21:04:11,281 INFO accuracy (adversarial): 0.85\n",
      "2020-11-30 21:04:11,281 INFO accuracy (natural): 0.935\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:24<14:27, 144.57s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:56, 143.34s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:04<09:29, 142.28s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:25<07:05, 141.70s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:49<04:44, 142.48s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:11<02:22, 142.26s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [14:56<00:00, 113.10s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [14:56<00:00, 896.35s/it]\n",
      "2020-11-30 21:19:22,893 INFO Success rate of attack: 13.00%\n",
      "2020-11-30 21:19:27,958 INFO accuracy (adversarial): 0.815\n",
      "2020-11-30 21:19:27,959 INFO accuracy (natural): 0.91\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:24<14:25, 144.20s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:56, 143.33s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:10<09:34, 143.69s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:31<07:08, 142.93s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:55<04:46, 143.49s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:16<02:22, 142.65s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:01<00:00, 113.40s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:01<00:00, 901.88s/it]\n",
      "2020-11-30 21:34:45,077 INFO Success rate of attack: 9.50%\n",
      "2020-11-30 21:34:50,129 INFO accuracy (adversarial): 0.805\n",
      "2020-11-30 21:34:50,130 INFO accuracy (natural): 0.875\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:19<13:58, 139.83s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:40<11:41, 140.23s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:05<09:25, 141.46s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:26<07:04, 141.44s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:51<04:44, 142.35s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:12<02:22, 142.03s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [14:57<00:00, 112.96s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [14:57<00:00, 897.66s/it]\n",
      "2020-11-30 21:50:03,063 INFO Success rate of attack: 14.00%\n",
      "2020-11-30 21:50:08,141 INFO accuracy (adversarial): 0.72\n",
      "2020-11-30 21:50:08,142 INFO accuracy (natural): 0.905\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:23<14:20, 143.36s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:54, 142.93s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:09<09:33, 143.26s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:31<07:08, 142.79s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:55<04:46, 143.31s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:16<02:22, 142.74s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:02<00:00, 113.54s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:02<00:00, 902.41s/it]\n",
      "2020-11-30 22:05:25,880 INFO Success rate of attack: 11.50%\n",
      "2020-11-30 22:05:30,973 INFO accuracy (adversarial): 0.775\n",
      "2020-11-30 22:05:30,974 INFO accuracy (natural): 0.88\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:22<14:16, 142.82s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:53, 142.71s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:08<09:31, 142.93s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:30<07:08, 142.68s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:54<04:46, 143.10s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:16<02:22, 142.74s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:02<00:00, 113.67s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:02<00:00, 902.69s/it]\n",
      "2020-11-30 22:20:49,044 INFO Success rate of attack: 14.50%\n",
      "2020-11-30 22:20:54,150 INFO accuracy (adversarial): 0.77\n",
      "2020-11-30 22:20:54,150 INFO accuracy (natural): 0.925\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:21<14:10, 141.82s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:51, 142.29s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:08<09:29, 142.45s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:30<07:07, 142.56s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:54<04:45, 142.81s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:16<02:22, 142.71s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:03<00:00, 113.83s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:03<00:00, 903.23s/it]\n",
      "2020-11-30 22:36:12,790 INFO Success rate of attack: 7.50%\n",
      "2020-11-30 22:36:17,874 INFO accuracy (adversarial): 0.84\n",
      "2020-11-30 22:36:17,875 INFO accuracy (natural): 0.945\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:21<14:07, 141.20s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:50, 142.15s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:07<09:28, 142.03s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:31<07:07, 142.55s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:53<04:45, 142.50s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:16<02:22, 142.72s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:02<00:00, 113.77s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:02<00:00, 902.99s/it]\n",
      "2020-11-30 22:51:36,152 INFO Success rate of attack: 10.00%\n",
      "2020-11-30 22:51:41,215 INFO accuracy (adversarial): 0.8\n",
      "2020-11-30 22:51:41,215 INFO accuracy (natural): 0.88\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:21<14:06, 141.05s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:50, 142.12s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:06<09:27, 141.84s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:31<07:08, 142.68s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:52<04:44, 142.31s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:17<02:22, 142.89s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:02<00:00, 113.65s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:02<00:00, 902.65s/it]\n",
      "2020-11-30 23:06:59,092 INFO Success rate of attack: 14.00%\n",
      "2020-11-30 23:07:04,158 INFO accuracy (adversarial): 0.825\n",
      "2020-11-30 23:07:04,159 INFO accuracy (natural): 0.925\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:21<14:08, 141.34s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:51, 142.30s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:07<09:28, 142.01s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:31<07:08, 142.83s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:53<04:44, 142.36s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:18<02:23, 143.10s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:03<00:00, 113.69s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:03<00:00, 903.18s/it]\n",
      "2020-11-30 23:22:22,591 INFO Success rate of attack: 18.50%\n",
      "2020-11-30 23:22:27,653 INFO accuracy (adversarial): 0.765\n",
      "2020-11-30 23:22:27,654 INFO accuracy (natural): 0.935\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:21<14:10, 141.71s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:52, 142.47s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:07<09:28, 142.17s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:31<07:08, 142.88s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:53<04:44, 142.43s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:17<02:23, 143.04s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:03<00:00, 113.70s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:03<00:00, 903.12s/it]\n",
      "2020-11-30 23:37:45,990 INFO Success rate of attack: 16.50%\n",
      "2020-11-30 23:37:51,043 INFO accuracy (adversarial): 0.795\n",
      "2020-11-30 23:37:51,044 INFO accuracy (natural): 0.91\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:22<14:13, 142.28s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:46<11:53, 142.71s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:08<09:30, 142.53s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:32<07:09, 143.02s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:54<04:45, 142.64s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:18<02:23, 143.17s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:03<00:00, 113.71s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:03<00:00, 903.46s/it]\n",
      "2020-11-30 23:53:09,744 INFO Success rate of attack: 12.50%\n",
      "2020-11-30 23:53:14,792 INFO accuracy (adversarial): 0.825\n",
      "2020-11-30 23:53:14,793 INFO accuracy (natural): 0.905\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:23<14:19, 143.29s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:46<11:55, 143.18s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:09<09:32, 143.07s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:32<07:09, 143.20s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:54<04:45, 142.93s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:18<02:23, 143.19s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:03<00:00, 113.70s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:03<00:00, 903.59s/it]\n",
      "2020-12-01 00:08:33,607 INFO Success rate of attack: 9.00%\n",
      "2020-12-01 00:08:38,662 INFO accuracy (adversarial): 0.78\n",
      "2020-12-01 00:08:38,663 INFO accuracy (natural): 0.91\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:24<14:24, 144.12s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:56, 143.36s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:09<09:33, 143.46s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:31<07:09, 143.16s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:54<04:46, 143.13s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:17<02:23, 143.06s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:02<00:00, 113.61s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:02<00:00, 902.78s/it]\n",
      "2020-12-01 00:23:56,689 INFO Success rate of attack: 13.50%\n",
      "2020-12-01 00:24:01,755 INFO accuracy (adversarial): 0.795\n",
      "2020-12-01 00:24:01,756 INFO accuracy (natural): 0.88\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:24<14:26, 144.41s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:57, 143.47s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:10<09:35, 143.87s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:31<07:09, 143.15s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:56<04:46, 143.46s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:18<02:22, 142.98s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:02<00:00, 113.55s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:02<00:00, 902.93s/it]\n",
      "2020-12-01 00:39:19,939 INFO Success rate of attack: 8.50%\n",
      "2020-12-01 00:39:25,004 INFO accuracy (adversarial): 0.775\n",
      "2020-12-01 00:39:25,005 INFO accuracy (natural): 0.9\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:24<14:25, 144.31s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:56, 143.40s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:10<09:35, 143.83s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:30<07:08, 142.84s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:50<04:43, 141.99s/it]\u001b[A\n",
      "PGD - Batches:  86%|████████▌ | 6/7 [14:11<02:21, 141.52s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [14:56<00:00, 112.56s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [14:56<00:00, 896.44s/it]\n",
      "2020-12-01 00:54:36,723 INFO Success rate of attack: 14.50%\n",
      "2020-12-01 00:54:41,800 INFO accuracy (adversarial): 0.84\n",
      "2020-12-01 00:54:41,801 INFO accuracy (natural): 0.915\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:23<14:23, 143.84s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:55, 143.14s/it]\u001b[A\n",
      "PGD - Batches:  43%|████▎     | 3/7 [07:10<09:34, 143.60s/it]\u001b[A\n",
      "PGD - Batches:  57%|█████▋    | 4/7 [09:31<07:08, 142.92s/it]\u001b[A\n",
      "PGD - Batches:  71%|███████▏  | 5/7 [11:56<04:46, 143.48s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PGD - Batches:  86%|████████▌ | 6/7 [14:17<02:22, 142.87s/it]\u001b[A\n",
      "PGD - Batches: 100%|██████████| 7/7 [15:02<00:00, 113.56s/it]\u001b[A\n",
      "PGD - Random Initializations: 100%|██████████| 1/1 [15:02<00:00, 902.82s/it]\n",
      "2020-12-01 01:09:59,939 INFO Success rate of attack: 10.00%\n",
      "2020-12-01 01:10:05,040 INFO accuracy (adversarial): 0.775\n",
      "2020-12-01 01:10:05,040 INFO accuracy (natural): 0.855\n",
      "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "PGD - Batches:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "PGD - Batches:  14%|█▍        | 1/7 [02:23<14:21, 143.60s/it]\u001b[A\n",
      "PGD - Batches:  29%|██▊       | 2/7 [04:45<11:55, 143.09s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "num_selected_models = 15\n",
    "norm = np.inf\n",
    "# infer_noise_list = [0.0025, 0.005, 0.01, 0.05, 0.08, 0.1]\n",
    "# eps_list = [0.0025, 0.01, 0.1, 0.5, 0.8]\n",
    "# infer_noise_list = [0.05, 0.08, 0.1]\n",
    "infer_noise_list = [0.04, 0.07, 0.1]\n",
    "eps_list = [0.0025, 0.005, 0.0075, 0.01, 0.02]\n",
    "\n",
    "for infer_noise in infer_noise_list:\n",
    "  for eps in eps_list:\n",
    "    my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
    "                                                  device, \n",
    "                                                  infer_noise=infer_noise,\n",
    "                                                  num_selected_models=num_selected_models,\n",
    "                                                  clip_values=[0., 1.], \n",
    "                                                  channels_first=True)\n",
    "    attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=eps, eps_step=eps/3, norm=norm)\n",
    "    Acc_adv = []\n",
    "    Acc_nat = []\n",
    "    logging.info('')\n",
    "    for batch_idx, (X, Y) in enumerate(testloader):\n",
    "      x_test = X.numpy()\n",
    "      y_test = Y.numpy()\n",
    "      predictions_nat = my_ensemble_classifier.predict(x_test)\n",
    "      accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
    "      Acc_nat.append(accuracy_nat)\n",
    "\n",
    "      x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
    "      predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
    "      accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
    "      Acc_adv.append(accuracy_adv)\n",
    "      \n",
    "      logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
    "      logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
    "    \n",
    "    Acc_adv = np.asanyarray(Acc_adv)\n",
    "    Acc_nat = np.asanyarray(Acc_nat)\n",
    "    idx = np.arange(0, len(testset)/test_batchsize).tolist()\n",
    "    res = {'Id': idx ,'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
    "    df = pd.DataFrame(res) \n",
    "    # saving the dataframe \n",
    "#     df.to_csv('./gdrive/My Drive/IDL_Project/results/PGD_exp1_infer_noise{}_eps{}.csv'.format(infer_noise, eps),index=False)\n",
    "    df.to_csv('/home/ubuntu/project/results/PGD_exp1_infer_noise{}_eps{}.csv'.format(infer_noise, eps),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SH-xmYJL8n14"
   },
   "source": [
    "# Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e44AHWvw8qxI"
   },
   "outputs": [],
   "source": [
    "num_selected_models_list = [5, 10, 15, 20, 25, 30, 35, 40]\n",
    "norm = np.inf\n",
    "infer_noise = 0.1\n",
    "eps_list = [0.0025, 0.01, 0.1, 0.5, 0.8]\n",
    "\n",
    "for num_selected_models in num_selected_models_list:\n",
    "  for eps in eps_list:\n",
    "    my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
    "                                                  device, \n",
    "                                                  infer_noise=infer_noise,\n",
    "                                                  num_selected_models=num_selected_models,\n",
    "                                                  clip_values=[0., 1.], \n",
    "                                                  channels_first=True)\n",
    "    attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=eps, eps_step=eps/3, norm=norm)\n",
    "    Acc_adv = []\n",
    "    Acc_nat = []\n",
    "    for batch_idx, (X, Y) in enumerate(testloader):\n",
    "      x_test = X.numpy()\n",
    "      y_test = Y.numpy()\n",
    "      x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
    "      predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
    "      predictions_nat = my_ensemble_classifier.predict(x_test)\n",
    "      accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
    "      accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
    "      Acc_adv.append(accuracy_adv)\n",
    "      Acc_nat.append(accuracy_nat)\n",
    "      logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
    "      logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
    "    \n",
    "    Acc_adv = np.asanyarray(Acc_adv)\n",
    "    Acc_nat = np.asanyarray(Acc_nat)\n",
    "    idx = np.arange(0, len(testset)/test_batchsize).tolist()\n",
    "    res = {'Id': idx ,'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
    "    df = pd.DataFrame(res) \n",
    "    # saving the dataframe \n",
    "    df.to_csv('./gdrive/My Drive/IDL_Project/results/PGD_exp2_N{}_eps{}.csv'.format(num_selected_models, eps),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ng7Xe9Zq8rai"
   },
   "source": [
    "# Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3rzYvCu8uoa"
   },
   "outputs": [],
   "source": [
    "num_selected_models = 15\n",
    "norm = 2\n",
    "infer_noise_list = [0.0025, 0.005, 0.01, 0.05, 0.08, 0.1]\n",
    "eps_list = [0.1, 0.5, 0.8, 0.9, 1]\n",
    "\n",
    "for infer_noise in infer_noise_list:\n",
    "  for eps in eps_list:\n",
    "    my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
    "                                                  device, \n",
    "                                                  infer_noise=infer_noise,\n",
    "                                                  num_selected_models=num_selected_models,\n",
    "                                                  clip_values=[0., 1.], \n",
    "                                                  channels_first=True)\n",
    "    attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=eps, eps_step=eps/3, norm=norm)\n",
    "    Acc_adv = []\n",
    "    Acc_nat = []\n",
    "    for batch_idx, (X, Y) in enumerate(testloader):\n",
    "      x_test = X.numpy()\n",
    "      y_test = Y.numpy()\n",
    "      x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
    "      predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
    "      predictions_nat = my_ensemble_classifier.predict(x_test)\n",
    "      accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
    "      accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
    "      Acc_adv.append(accuracy_adv)\n",
    "      Acc_nat.append(accuracy_nat)\n",
    "      logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
    "      logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
    "    \n",
    "    Acc_adv = np.asanyarray(Acc_adv)\n",
    "    Acc_nat = np.asanyarray(Acc_nat)\n",
    "    idx = np.arange(0, len(testset)/test_batchsize).tolist()\n",
    "    res = {'Id': idx ,'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
    "    df = pd.DataFrame(res) \n",
    "    # saving the dataframe \n",
    "    df.to_csv('./gdrive/My Drive/IDL_Project/results/PGD_exp3_infer_noise{}_eps{}.csv'.format(infer_noise, eps),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-w5HNiC8vDY"
   },
   "source": [
    "# Experiment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTCAdeqj8w17"
   },
   "outputs": [],
   "source": [
    "num_selected_models_list = [5, 10, 15, 20, 25, 30, 35, 40]\n",
    "norm = 2\n",
    "infer_noise = 0.1\n",
    "eps_list = [0.1, 0.5, 0.8, 0.9, 1]\n",
    "\n",
    "for num_selected_models in num_selected_models_list:\n",
    "  for eps in eps_list:\n",
    "    my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
    "                                                  device, \n",
    "                                                  infer_noise=infer_noise,\n",
    "                                                  num_selected_models=num_selected_models,\n",
    "                                                  clip_values=[0., 1.], \n",
    "                                                  channels_first=True)\n",
    "    attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=eps, eps_step=eps/3, norm=norm)\n",
    "    Acc_adv = []\n",
    "    Acc_nat = []\n",
    "    for batch_idx, (X, Y) in enumerate(testloader):\n",
    "      x_test = X.numpy()\n",
    "      y_test = Y.numpy()\n",
    "      x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
    "      predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
    "      predictions_nat = my_ensemble_classifier.predict(x_test)\n",
    "      accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
    "      accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
    "      Acc_adv.append(accuracy_adv)\n",
    "      Acc_nat.append(accuracy_nat)\n",
    "      logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
    "      logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
    "    \n",
    "    Acc_adv = np.asanyarray(Acc_adv)\n",
    "    Acc_nat = np.asanyarray(Acc_nat)\n",
    "    idx = np.arange(0, len(testset)/test_batchsize).tolist()\n",
    "    res = {'Id': idx ,'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
    "    df = pd.DataFrame(res) \n",
    "    # saving the dataframe \n",
    "    df.to_csv('./gdrive/My Drive/IDL_Project/results/PGD_exp4_N{}_eps{}.csv'.format(num_selected_models, eps),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdUjTkdWBdgh"
   },
   "source": [
    "# **The rest parts of the notebook was regarding previous work. Discard them for now.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gP_Gxq-2jc-9"
   },
   "source": [
    "# FGM Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmXkglUUKXO3"
   },
   "outputs": [],
   "source": [
    "attack = FastGradientMethod(estimator=my_ensemble_classifier, eps=0.5, targeted=False, norm=2)\n",
    "Acc = []\n",
    "for batch_idx, (X, Y) in enumerate(testloader):\n",
    "    x_test = X.numpy()\n",
    "    y_test = Y.numpy()\n",
    "    # target = (y_test + 1) % 10\n",
    "    x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
    "    predictions = my_ensemble_classifier.predict(x_test_adv)\n",
    "    predictions_test = my_ensemble_classifier.predict(x_test)\n",
    "    accuracy = np.sum(np.argmax(predictions, axis=1) == y_test) / len(y_test)\n",
    "    acc_test = np.sum(np.argmax(predictions_test, axis=1) == y_test) / len(y_test)\n",
    "    # print(y_test.shape)\n",
    "    # acc_rate, coverage_rate = compute_accuracy(preds=predictions, labels=np.reshape(y_test, (test_batchsize, 1)))\n",
    "    # logging.info('acc rate: {}, coverage rate: {}'.format(acc_rate, coverage_rate))\n",
    "    Acc.append(accuracy)\n",
    "    logging.info('accuracy: {}'.format(accuracy))\n",
    "    logging.info('accuracy non adv: {}'.format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoYiKeGCjfDR"
   },
   "source": [
    "# PGD Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZRM5XEZjchO"
   },
   "outputs": [],
   "source": [
    "attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=0.5, eps_step=0.03)\n",
    "Acc = []\n",
    "for batch_idx, (X, Y) in enumerate(testloader):\n",
    "    x_test = X.numpy()\n",
    "    y_test = Y.numpy()\n",
    "    x_test_adv = attack.generate(torch.from_numpy(x_test))\n",
    "    predictions = my_ensemble_classifier.predict(x_test_adv)\n",
    "    accuracy = np.sum(np.argmax(predictions, axis=1) == np.array(y_test)) / len(y_test)\n",
    "    Acc.append(accuracy)\n",
    "    logging.info('accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyMeGON8BFNL"
   },
   "source": [
    "# Experiment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjUPKB3jvijF"
   },
   "outputs": [],
   "source": [
    "fgm_epsilon = np.array([0.005, 0.01, 0.02, 0.04, 0.08])\n",
    "pgd_epsilon = 8.0 / 255.0 / np.array([2.0, 4.0, 8.0, 16.0])\n",
    "diff = np.array([1e-3, 1e-4, 1e-5, 1e-6])\n",
    "pgd_epsilon_step = pgd_epsilon - diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XG44M27BNIT"
   },
   "source": [
    "Experiment on inference noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U79zJmCsDEKc"
   },
   "outputs": [],
   "source": [
    "noise_stds = [0.02, 0.04, 0.06, 0.08, 0.1]\n",
    "for i in range(len(noise_stds)):\n",
    "  my_ensemble = MyEnsemble(model_dict, num_models_selected, noise_std=noise_stds[i], num_classes=10)\n",
    "  classifier = PyTorchClassifier(\n",
    "      model=my_ensemble,\n",
    "      clip_values=(0, 1),\n",
    "      loss=nn.CrossEntropyLoss(),\n",
    "      input_shape=(3, 224, 224),\n",
    "      nb_classes=10)\n",
    "  attack = FastGradientMethod(estimator=classifier, eps=0.01)\n",
    "  Acc = []\n",
    "  for batch_idx, (X, Y) in enumerate(testloader):\n",
    "    x_test = X.numpy()\n",
    "    y_test = Y.numpy()\n",
    "    x_test_adv = attack.generate(torch.from_numpy(x_test))\n",
    "    predictions = classifier.predict(x_test_adv)\n",
    "    accuracy = np.sum(np.argmax(predictions, axis=1) == np.array(y_test)) / len(y_test)\n",
    "    Acc.append(accuracy)\n",
    "    logging.info('accuracy: {}'.format(accuracy))\n",
    "  \n",
    "  logging.info(\"Accuracy for eps={}: {}\".format(fgm_epsilon[i], np.mean(np.array(Acc))))\n",
    "\n",
    "  pred_list = np.asanyarray(Acc)\n",
    "  idx = np.arange(0,len(testset)/test_batchsize).tolist()\n",
    "  dict = {'Id': idx ,'acc': pred_list} \n",
    "  df = pd.DataFrame(dict) \n",
    "  # saving the dataframe \n",
    "  df.to_csv('./gdrive/My Drive/IDL_Project/results/FGM_result_noise{}_{}.csv'.format(noise_stds[i], datetime.datetime.now()),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LotwWsVpMYKN"
   },
   "outputs": [],
   "source": [
    "pred_list = np.asanyarray(Acc)\n",
    "idx = np.arange(0,len(testset)/test_batchsize).tolist()\n",
    "dict = {'Id': idx ,'acc': pred_list} \n",
    "df = pd.DataFrame(dict) \n",
    "# saving the dataframe \n",
    "df.to_csv('./gdrive/My Drive/IDL_Project/results/FGM_result_eps{}_{}.csv'.format(fgm_epsilon[-1], datetime.datetime.now()),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqIgjfhwBXVL"
   },
   "source": [
    "Plot PGD adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Eu7qdDQMnGq"
   },
   "outputs": [],
   "source": [
    "attack = ProjectedGradientDescentPyTorch(estimator=classifier, eps=pgd_epsilon[0], eps_step=pgd_epsilon_step[0])\n",
    "Acc = []\n",
    "adv = []\n",
    "orig = []\n",
    "labels = []\n",
    "pred = []\n",
    "for batch_idx, (X, Y) in enumerate(testloader):\n",
    "  if batch_idx > 0:\n",
    "    break\n",
    "  x_test = X.numpy()\n",
    "  y_test = Y.numpy()\n",
    "  x_test_adv = attack.generate(torch.from_numpy(x_test))\n",
    "  predictions = classifier.predict(x_test_adv)\n",
    "  accuracy = np.sum(np.argmax(predictions, axis=1) == np.array(y_test)) / len(y_test)\n",
    "  Acc.append(accuracy)\n",
    "  adv.append(x_test_adv)\n",
    "  orig.append(x_test)\n",
    "  labels.append(y_test)\n",
    "  pred.append(predictions)\n",
    "  logging.info('accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8sj2LdnNV5n"
   },
   "outputs": [],
   "source": [
    "example_adv = adv[-1]\n",
    "example_orig = orig[-1]\n",
    "label = labels[-1]\n",
    "predi = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SoD6QHEwNt4T"
   },
   "outputs": [],
   "source": [
    "print(label)\n",
    "print(predi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnkXnHRePFeW"
   },
   "outputs": [],
   "source": [
    "idx = 2\n",
    "selected_adv = example_adv[idx]\n",
    "selected_orig = example_orig[idx]\n",
    "selected_lab = label[idx]\n",
    "selected_pred = predi[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZuD1OfoWroX"
   },
   "outputs": [],
   "source": [
    "print(selected_lab)\n",
    "print(selected_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KkxSl4PTPab0"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(np.transpose(selected_orig, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tpehq_0hVobG"
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.transpose(selected_adv, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuLSJh8QVsHy"
   },
   "outputs": [],
   "source": [
    "plt.imshow(10*(np.transpose(selected_orig, (1, 2, 0)) - np.transpose(selected_adv, (1, 2, 0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lOaioJIBh1G"
   },
   "source": [
    "Experiment PGD attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iuxeEuf-8UVE"
   },
   "outputs": [],
   "source": [
    "for i in range(len(pgd_epsilon)):\n",
    "  attack = FastGradientMethod(estimator=classifier, eps=fgm_epsilon[i])\n",
    "  Acc = []\n",
    "  for batch_idx, (X, Y) in enumerate(testloader):\n",
    "    x_test = X.numpy()\n",
    "    y_test = Y.numpy()\n",
    "    x_test_adv = attack.generate(torch.from_numpy(x_test))\n",
    "    predictions = classifier.predict(x_test_adv)\n",
    "    accuracy = np.sum(np.argmax(predictions, axis=1) == np.array(y_test)) / len(y_test)\n",
    "    Acc.append(accuracy)\n",
    "    logging.info('accuracy: {}'.format(accuracy))\n",
    "  \n",
    "  logging.info(\"Accuracy for eps={}: {}\".format(fgm_epsilon[i], np.mean(np.array(Acc))))\n",
    "\n",
    "  pred_list = np.asanyarray(Acc)\n",
    "  idx = np.arange(0,len(testset)/test_batchsize).tolist()\n",
    "  dict = {'Id': idx ,'acc': pred_list} \n",
    "  df = pd.DataFrame(dict) \n",
    "  # saving the dataframe \n",
    "  df.to_csv('./gdrive/My Drive/IDL_Project/results/FGM_result_eps{}_{}.csv'.format(fgm_epsilon[i], datetime.datetime.now()),index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "J0oc512o9c4d",
    "IffxeALQ_ajk",
    "inhJeBV-9ic0"
   ],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Copy of Copy of Copy of idl_project_ensemble.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
