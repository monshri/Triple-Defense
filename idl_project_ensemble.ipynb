{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of Copy of Copy of idl_project_ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "J0oc512o9c4d",
        "IffxeALQ_ajk",
        "inhJeBV-9ic0"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "98fad916b3564edd9a949d8bd951e3b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e7b380495b8841358ed9b779f5bc9dfe",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fce2cb0cc01142d5bb1994302671aa21",
              "IPY_MODEL_9a212e0cda304c509f778b8e70e5e250"
            ]
          }
        },
        "e7b380495b8841358ed9b779f5bc9dfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fce2cb0cc01142d5bb1994302671aa21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_619cd1cc2bd643dc9caf15620b4558f0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c74a617381ce4f6d8db37378cef2be3c"
          }
        },
        "9a212e0cda304c509f778b8e70e5e250": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bf8bc4e10e3c40f09722e4a1bf89f6b9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:30&lt;00:00, 17582249.31it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_823035981ce843d1a2c29c56c6542690"
          }
        },
        "619cd1cc2bd643dc9caf15620b4558f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c74a617381ce4f6d8db37378cef2be3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bf8bc4e10e3c40f09722e4a1bf89f6b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "823035981ce843d1a2c29c56c6542690": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaichengDING/Triple-Defense/blob/main/idl_project_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8idp0UC0Oj6"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ESeViSYxQfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f893fc12-6137-4752-b0b8-ebcdb218ab6e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgPpHa2d0Rt4"
      },
      "source": [
        "# Install ART"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzKNCLpGws5x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d77fc40-78d6-45ce-c612-7818780bd010"
      },
      "source": [
        "!pip install adversarial-robustness-toolbox==1.4.3"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting adversarial-robustness-toolbox==1.4.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/aa/1136628240b23de5c311aa1595d327f3908296a71f72093370a0459b8f0b/adversarial_robustness_toolbox-1.4.3-py3-none-any.whl (765kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 768kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (1.18.5)\n",
            "Collecting scikit-learn==0.22.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/7f/366dcba1ba076a88a50bea732dbc033c0c5bbf7876010e6edc67948579d5/scikit_learn-0.22.2-cp36-cp36m-manylinux1_x86_64.whl (7.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.1MB 27.8MB/s \n",
            "\u001b[?25hCollecting cma\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/c0/0a1c41f7cad0a51e07991cf86423d0e6651d035f1fe7dcff48e8858848f2/cma-3.0.3-py2.py3-none-any.whl (230kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235kB 66.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (0.10.2)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (50.3.2)\n",
            "Collecting pydub\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/d1/fbfa79371a8cd9bb15c2e3c480d7e6e340ed5cc55005174e16f48418333a/pydub-0.24.1-py2.py3-none-any.whl\n",
            "Collecting mypy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/cb/cf5530d063e7e703e2fbec677bfba633de6e70fe44bc323deeaa27f273b8/mypy-0.790-cp36-cp36m-manylinux1_x86_64.whl (21.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21.0MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (4.41.1)\n",
            "Collecting ffmpeg-python\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/0c/56be52741f75bad4dc6555991fabd2e07b432d333da82c11ad701123888a/ffmpeg_python-0.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: resampy in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (0.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (3.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from adversarial-robustness-toolbox==1.4.3) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.22.2->adversarial-robustness-toolbox==1.4.3) (0.17.0)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from statsmodels->adversarial-robustness-toolbox==1.4.3) (1.1.4)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels->adversarial-robustness-toolbox==1.4.3) (0.5.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from mypy->adversarial-robustness-toolbox==1.4.3) (3.7.4.3)\n",
            "Collecting typed-ast<1.5.0,>=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/ed/5459080d95eb87a02fe860d447197be63b6e2b5e9ff73c2b0a85622994f4/typed_ast-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (737kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 747kB 67.0MB/s \n",
            "\u001b[?25hCollecting mypy-extensions<0.5.0,>=0.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from ffmpeg-python->adversarial-robustness-toolbox==1.4.3) (0.16.0)\n",
            "Requirement already satisfied: numba>=0.32 in /usr/local/lib/python3.6/dist-packages (from resampy->adversarial-robustness-toolbox==1.4.3) (0.48.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox==1.4.3) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox==1.4.3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox==1.4.3) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->adversarial-robustness-toolbox==1.4.3) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->statsmodels->adversarial-robustness-toolbox==1.4.3) (2018.9)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.32->resampy->adversarial-robustness-toolbox==1.4.3) (0.31.0)\n",
            "Installing collected packages: scikit-learn, cma, pydub, typed-ast, mypy-extensions, mypy, ffmpeg-python, adversarial-robustness-toolbox\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed adversarial-robustness-toolbox-1.4.3 cma-3.0.3 ffmpeg-python-0.2.0 mypy-0.790 mypy-extensions-0.4.3 pydub-0.24.1 scikit-learn-0.22.2 typed-ast-1.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL8u7HvJ0Xka"
      },
      "source": [
        "# Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SqUPdAKkDIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b90a8af1-fd1d-4472-a660-2c44eb3f91d9"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import PIL\n",
        "\n",
        "import sys\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "from torch.utils import data\n",
        "\n",
        "from art.attacks.evasion import FastGradientMethod\n",
        "from art.attacks.evasion import ProjectedGradientDescentPyTorch\n",
        "from art.estimators.classification import PyTorchClassifier\n",
        "\n",
        "from art.estimators.classification import EnsembleClassifier\n",
        "from typing import List, Optional, Union, TYPE_CHECKING\n",
        "from art.estimators.classification.classifier import ClassifierNeuralNetwork\n",
        "from scipy.special import softmax\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import logging\n",
        "import datetime\n",
        "import random\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "cuda"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcOnXEFW1m4K"
      },
      "source": [
        "# Model Definition\n",
        "`ShuffleNet` and `ShuffleNetV2` are actually identical, but models with seed number smaller than 200 are created using ShuffleNet and models with seed number greater than 200 are created using ShuffleNetV2. In order to load all models into ensemble, the two definitions are provided here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfyaOczXoRqQ"
      },
      "source": [
        "class ShuffleNet(nn.Module):\n",
        "    def __init__(self, nb_classes =10):\n",
        "        super(ShuffleNet, self).__init__()\n",
        "        self.shuffle = models.shufflenet_v2_x2_0()\n",
        "        self.linear = nn.Linear(1000, nb_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.shuffle(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ShuffleNetV2(nn.Module):\n",
        "    def __init__(self, nb_classes=10):\n",
        "        super(ShuffleNetV2, self).__init__()\n",
        "        self.shufflenet = models.shufflenet_v2_x2_0()\n",
        "        self.linear = nn.Linear(1000, nb_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.shufflenet(x)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlygMiHF0iSB"
      },
      "source": [
        "# Logging Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXX4443Y16Ul"
      },
      "source": [
        "# configure logging\n",
        "logger = logging.getLogger(\"\")\n",
        "\n",
        "# reset handler\n",
        "for handler in logging.root.handlers[:]:\n",
        "  logging.root.removeHandler(handler)\n",
        "\n",
        "# set handler\n",
        "stream_hdlr = logging.StreamHandler()\n",
        "# logging on colab\n",
        "file_hdlr = logging.FileHandler('/content/gdrive/My Drive/IDL_Project/logs/log_{}.log'.format(datetime.datetime.now()))\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
        "stream_hdlr.setFormatter(formatter)\n",
        "file_hdlr.setFormatter(formatter)\n",
        "\n",
        "logger.addHandler(stream_hdlr)\n",
        "logger.addHandler(file_hdlr)\n",
        "\n",
        "logger.setLevel(logging.INFO)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFiB-FbgT9sk"
      },
      "source": [
        "# Get Test Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk68nikXxa4G"
      },
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, X, Y, transform=None):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    self.transform = transform\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.Y)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    if self.transform is None:\n",
        "      return torch.from_numpy(self.X[idx]), torch.tensor(self.Y[idx]).long()\n",
        "    else:\n",
        "      return self.transform(self.X[idx]), torch.tensor(self.Y[idx]).long()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV2pPKKzeESx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "98fad916b3564edd9a949d8bd951e3b0",
            "e7b380495b8841358ed9b779f5bc9dfe",
            "fce2cb0cc01142d5bb1994302671aa21",
            "9a212e0cda304c509f778b8e70e5e250",
            "619cd1cc2bd643dc9caf15620b4558f0",
            "c74a617381ce4f6d8db37378cef2be3c",
            "bf8bc4e10e3c40f09722e4a1bf89f6b9",
            "823035981ce843d1a2c29c56c6542690"
          ]
        },
        "outputId": "0649edcb-3b91-4a32-c5e3-98126454ba93"
      },
      "source": [
        "test_batchsize = 200\n",
        "num_workers = 4\n",
        "img_size = 224\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToPILImage(),\n",
        "                                     transforms.Resize(size=img_size),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0, 0, 0), (1, 1, 1))])\n",
        "\n",
        "testset= torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n",
        "testset_data = testset.data[0:1000]\n",
        "testset_labels = testset.targets[0:1000]\n",
        "\n",
        "testset_sub = MyDataset(testset_data, testset_labels, transform=test_transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset_sub, batch_size=test_batchsize, shuffle=False, num_workers=num_workers, drop_last=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98fad916b3564edd9a949d8bd951e3b0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOPbwP7FTWEY"
      },
      "source": [
        "# Load Saved Models\n",
        "All saved models will be loaded into `model_dict`. Models with seed number smaller than 200 belong to `ShuffleNet` class, and models with seed number greater than 200 belong to `ShuffleNetV2` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUiLYb1EeNtS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73fbd18e-4971-45a7-9954-e19c6ded66db"
      },
      "source": [
        "# models with 0.1 training noise\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "model_names = ['ShuffleNet_1',\n",
        "               'ShuffleNet_2',\n",
        "               'ShuffleNet_3',\n",
        "               'ShuffleNet_4',\n",
        "               'ShuffleNet_5',\n",
        "               'ShuffleNet_6',\n",
        "               'ShuffleNet_7',\n",
        "               'ShuffleNet_8',\n",
        "               'ShuffleNet_31',\n",
        "               'ShuffleNet_32',\n",
        "               'ShuffleNet_33',\n",
        "               'ShuffleNet_34',\n",
        "               'ShuffleNet_35',\n",
        "               'ShuffleNet_36',\n",
        "               'ShuffleNet_37',\n",
        "               'ShuffleNet_38',\n",
        "               'ShuffleNet_39',\n",
        "               'ShuffleNet_61',\n",
        "               'ShuffleNet_62',\n",
        "               'ShuffleNet_63',\n",
        "               'ShuffleNet_64',\n",
        "               'ShuffleNet_65',\n",
        "               'ShuffleNet_66',\n",
        "               'ShuffleNet_67',\n",
        "               'ShuffleNet_68',\n",
        "               'ShuffleNet_69',\n",
        "               'ShuffleNet_70',\n",
        "               'ShuffleNet_71',\n",
        "               'ShuffleNet_72',\n",
        "               'ShuffleNet_73']\n",
        "\n",
        "model_names_v2 = ['ShuffleNet_200', \n",
        "                  'ShuffleNet_201',\n",
        "                  'ShuffleNet_202',\n",
        "                  'ShuffleNet_203',\n",
        "                  'ShuffleNet_204',\n",
        "                  'ShuffleNet_205',\n",
        "                  'ShuffleNet_206',\n",
        "                  'ShuffleNet_207',\n",
        "                  'ShuffleNet_208',\n",
        "                  'ShuffleNet_209']\n",
        "\n",
        "model_dict = {}\n",
        "\n",
        "for model_name in model_names:\n",
        "  model = ShuffleNet()\n",
        "  model_data = torch.load('/content/gdrive/My Drive/IDL_Project/modelS/{}'.format(model_name), map_location=torch.device('cpu'))\n",
        "  model.load_state_dict(model_data['model_state_dict'])\n",
        "  model = model.to(device)\n",
        "  model_dict[model_name] = model\n",
        "\n",
        "for model_name in model_names_v2:\n",
        "  model = ShuffleNetV2()\n",
        "  model_data = torch.load('/content/gdrive/My Drive/IDL_Project/modelS/{}'.format(model_name), map_location=torch.device('cpu'))\n",
        "  model.load_state_dict(model_data['model_state_dict'])\n",
        "  model = model.to(device)\n",
        "  model_dict[model_name] = model\n",
        "\n",
        "logging.info(\"Total number of models: {}\".format(len(model_names) + len(model_names_v2)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-02 19:11:49,219 INFO Total number of models: 40\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inhJeBV-9ic0"
      },
      "source": [
        "# MyEnsembleClassifier: Inherited from `EnsembleClassifier` with `predict`, `loss_gradient` and `loss_gradient` overwritten\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moVnk9Nwnk5y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd7dca3e-c711-4ad5-dd3d-36896ea1e2ca"
      },
      "source": [
        "np.random.choice(10, 20, replace=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([8, 1, 8, 4, 0, 4, 4, 4, 5, 1, 8, 1, 7, 4, 4, 5, 8, 4, 7, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ratuA8nTIKyu"
      },
      "source": [
        "class MyEnsembleClassifier(EnsembleClassifier):\n",
        "    def __init__(\n",
        "        self,\n",
        "        classifiers: List[ClassifierNeuralNetwork],\n",
        "        device,\n",
        "        infer_noise,\n",
        "        num_selected_models,\n",
        "        classifier_weights: Union[list, np.ndarray, None] = None,\n",
        "        channels_first: bool = False,\n",
        "        clip_values: Optional[\"CLIP_VALUES_TYPE\"] = None,\n",
        "        preprocessing_defences: Union[\"Preprocessor\", List[\"Preprocessor\"], None] = None,\n",
        "        postprocessing_defences: Union[\"Postprocessor\", List[\"Postprocessor\"], None] = None,\n",
        "        preprocessing: \"PREPROCESSING_TYPE\" = (0, 1),\n",
        "    ) -> None:\n",
        "      super().__init__(\n",
        "          classifiers=classifiers,\n",
        "          classifier_weights=classifier_weights,\n",
        "          channels_first=channels_first,\n",
        "          clip_values=clip_values,\n",
        "          preprocessing_defences=preprocessing_defences,\n",
        "          postprocessing_defences=postprocessing_defences,\n",
        "          preprocessing=preprocessing\n",
        "      )\n",
        "      self.device = device\n",
        "      self.infer_noise = infer_noise\n",
        "      self.num_models = len(classifiers)\n",
        "      self.num_selected_models = num_selected_models\n",
        "\n",
        "    def predict(self, x: np.ndarray, batch_size: int = 128, raw: bool = False, **kwargs) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Perform prediction for a batch of inputs. Predictions from classifiers should only be aggregated if they all\n",
        "        have the same type of output (e.g., probabilities). Otherwise, use `raw=True` to get predictions from all\n",
        "        models without aggregation. The same option should be used for logits output, as logits are not comparable\n",
        "        between models and should not be aggregated.\n",
        "        :param x: Test set.\n",
        "        :param batch_size: Size of batches.\n",
        "        :param raw: Return the individual classifier raw outputs (not aggregated).\n",
        "        :return: Array of predictions of shape `(nb_inputs, nb_classes)`, or of shape\n",
        "                 `(nb_classifiers, nb_inputs, nb_classes)` if `raw=True`.\n",
        "        \"\"\"\n",
        "        # indices = [i for i in range(self.num_models)]\n",
        "        # random.shuffle(indices)\n",
        "        # indices = indices[:self.num_selected_models]\n",
        "\n",
        "        # selected_classifiers = random.choice(self._classifiers, k=self.num_selected_models)\n",
        "\n",
        "        indices = np.random.choice(self.num_models, self.num_selected_models, replace=True)\n",
        "\n",
        "        noise_list = [np.random.randn(*x.shape) * self.infer_noise for i in indices]\n",
        "\n",
        "        # replace with for loop\n",
        "        preds = np.array(\n",
        "            [softmax(self._classifiers[i].predict(np.float32(x + noise_list[i])), axis=1) for i in indices]\n",
        "        )\n",
        "\n",
        "        if raw:\n",
        "            return preds\n",
        "\n",
        "        # 6 x 100\n",
        "        preds_classes = np.argmax(preds, axis=2)\n",
        "        row, col = preds_classes.shape\n",
        "\n",
        "        # 100,\n",
        "        majority_vote = np.array(\n",
        "            [\n",
        "             np.bincount(preds_classes[:,c]).argmax()\n",
        "             for c in range(col)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        mask = preds_classes == majority_vote\n",
        "        mask = np.repeat(np.expand_dims(mask, axis=2), repeats=10, axis=2)\n",
        "\n",
        "        # Aggregate predictions only at probabilities level, as logits are not comparable between models\n",
        "        var_z = np.sum(mask * preds, axis=0) # take mean (check sum to 1)\n",
        "\n",
        "        # Apply postprocessing\n",
        "        predictions = self._apply_postprocessing(preds=var_z, fit=False)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "\n",
        "    def loss_gradient(self, x: np.ndarray, y: np.ndarray, raw: bool = False, **kwargs) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute the gradient of the loss function w.r.t. `x`.\n",
        "        :param x: Sample input with shape as expected by the model.\n",
        "        :param y: Target values (class labels) one-hot-encoded of shape (nb_samples, nb_classes) or indices of shape\n",
        "                  (nb_samples,).\n",
        "        :param raw: Return the individual classifier raw outputs (not aggregated).\n",
        "        :return: Array of gradients of the same shape as `x`. If `raw=True`, shape becomes `[nb_classifiers, x.shape]`.\n",
        "        \"\"\"\n",
        "\n",
        "        # indices = [i for i in range(self.num_models)]\n",
        "        # random.shuffle(indices)\n",
        "        # indices = indices[:self.num_selected_models]\n",
        "\n",
        "        indices = np.random.choice(self.num_models, self.num_selected_models, replace=True)\n",
        "\n",
        "        noise_list = [np.random.randn(*x.shape) * self.infer_noise for i in indices]\n",
        "        \n",
        "        grads = np.array(\n",
        "            [\n",
        "                self._classifiers[i].loss_gradient(np.float32(x + noise_list[i]), y)\n",
        "                for i in indices\n",
        "            ]\n",
        "        )\n",
        "        if raw:\n",
        "            return grads\n",
        "\n",
        "        return np.sum(grads, axis=0)\n",
        "\n",
        "    def loss_gradient_framework(self, x: \"torch.Tensor\", y: \"torch.Tensor\", **kwargs) -> \"torch.Tensor\":\n",
        "        \"\"\"\n",
        "        Compute the gradient of the loss function w.r.t. `x`.\n",
        "        :param x: Sample input with shape as expected by the model.\n",
        "        :param y: Target values (class labels) one-hot-encoded of shape (nb_samples, nb_classes) or indices of shape\n",
        "                  (nb_samples,).\n",
        "        :param raw: Return the individual classifier raw outputs (not aggregated).\n",
        "        :return: Array of gradients of the same shape as `x`. If `raw=True`, shape becomes `[nb_classifiers, x.shape]`.\n",
        "        \"\"\"\n",
        "\n",
        "        # indices = [i for i in range(self.num_models)]\n",
        "        # random.shuffle(indices)\n",
        "        # indices = indices[:self.num_selected_models]\n",
        "        \n",
        "        indices = np.random.choice(self.num_models, self.num_selected_models, replace=True)\n",
        "\n",
        "        noise_list = [(torch.randn(x.size()) * self.infer_noise).to(device) for i in indices]\n",
        "\n",
        "        # try detach here\n",
        "        grads = [\n",
        "                  self._classifiers[i].loss_gradient_framework(x + noise_list[i], y).unsqueeze(0)\n",
        "                  for i in indices\n",
        "                ]\n",
        "        \n",
        "        return torch.sum(torch.cat(grads, axis=0), axis=0)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKzm9D8h5bFR"
      },
      "source": [
        "# Creating classifier list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z0e7i6HJi7i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "497b8d62-0eb6-48b4-c0e5-440dc9e07883"
      },
      "source": [
        "nb_classes = 10\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "classifier_list = []\n",
        "for model_name in model_dict:\n",
        "  classifier_list.append(PyTorchClassifier(\n",
        "    model=model_dict[model_name],\n",
        "    clip_values=(0, 1),\n",
        "    loss=criterion,\n",
        "    input_shape=(3, img_size, img_size),\n",
        "    nb_classes=nb_classes,\n",
        "))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-02 19:26:03,207 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,213 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,218 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,224 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,230 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,237 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,242 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,248 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,254 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,259 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,263 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,268 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,273 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,277 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,282 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,287 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,291 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,296 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,300 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,305 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,309 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,313 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,318 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,324 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,329 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,335 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,340 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,345 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,351 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,356 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,362 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,367 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,372 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,377 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,381 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,385 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,390 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,394 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,399 INFO Inferred 1 hidden layers on PyTorch classifier.\n",
            "2020-12-02 19:26:03,403 INFO Inferred 1 hidden layers on PyTorch classifier.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emCbgIPQ5yA5"
      },
      "source": [
        "# Experiment Settings\n",
        "Refer to [this google doc](https://docs.google.com/document/d/1VaS5THALdudd_63Zv7ZR2u8rj8r8iDF6xgXzzLHXR8w/edit)\n",
        "\n",
        "1.   `num_selected_models`=15, `infer_noise`=[0.0025, 0.005, 0.01, 0.05, 0.08, 0.1], `eps`=[0.0025, 0.01, 0.1, 0.5, 0.8], `norm`=Linf\n",
        "2.   `num_selected_models`=[5, 10, 15, 20, 25, 30, 35, 40], `infer_noise`=0.1, `eps`=[0.0025, 0.01, 0.1, 0.5, 0.8], `norm`=Linf\n",
        "3.   `num_selected_models`=15, `infer_noise`=[0.0025, 0.005, 0.01, 0.05, 0.08, 0.1], `eps`=[0.1, 0.5, 0.8, 0.9, 1], `norm`=L2\n",
        "4.   `num_selected_models`=[5, 10, 15, 20, 25, 30, 35, 40], `infer_noise`=0.1, `eps`=[0.1, 0.5, 0.8, 0.9, 1], `norm`=L2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfpLU-YJlx92"
      },
      "source": [
        "### Arbitrary Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xzKyhJXww1M",
        "outputId": "fb8c33a8-06f0-4b33-db68-c21f87e3f483",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for batch_idx, (X, Y) in enumerate(testloader):\n",
        "  print(X.shape)\n",
        "  print(Y.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([200, 3, 224, 224])\n",
            "torch.Size([200])\n",
            "torch.Size([200, 3, 224, 224])\n",
            "torch.Size([200])\n",
            "torch.Size([200, 3, 224, 224])\n",
            "torch.Size([200])\n",
            "torch.Size([200, 3, 224, 224])\n",
            "torch.Size([200])\n",
            "torch.Size([200, 3, 224, 224])\n",
            "torch.Size([200])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "64xN40H5nqHR",
        "outputId": "d1f05d33-051d-4bad-d6c3-7cce7eb214fc"
      },
      "source": [
        "num_selected_models = 15\n",
        "norm = np.inf\n",
        "infer_noise = 0.3\n",
        "eps = 8/255\n",
        "logging.info(\"Current experiment setting: eps={}, inference noise={}\".format(eps, infer_noise))\n",
        "my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
        "                                              device, \n",
        "                                              infer_noise=infer_noise,\n",
        "                                              num_selected_models=num_selected_models,\n",
        "                                              clip_values=[0., 1.], \n",
        "                                              channels_first=True)\n",
        "attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=eps, eps_step=0.025, norm=norm, max_iter=20, batch_size=test_batchsize)\n",
        "Acc_adv = []\n",
        "Acc_nat = []\n",
        "for batch_idx, (X, Y) in enumerate(testloader):\n",
        "  x_test = X.numpy()\n",
        "  x_test_nat = x_test + np.random.randn(*x_test.shape) * eps\n",
        "  y_test = Y.numpy()\n",
        "  predictions_nat = my_ensemble_classifier.predict(x_test_nat)\n",
        "  accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
        "  Acc_nat.append(accuracy_nat)\n",
        "\n",
        "  x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "  predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
        "  accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
        "  Acc_adv.append(accuracy_adv)\n",
        "      \n",
        "  logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
        "  logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
        "    \n",
        "Acc_adv = np.asanyarray(Acc_adv)\n",
        "Acc_nat = np.asanyarray(Acc_nat)\n",
        "idx = np.arange(0, len(testset)/test_batchsize).tolist()\n",
        "# res = {'Id': idx ,'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
        "# df = pd.DataFrame(res) \n",
        "# # saving the dataframe \n",
        "# df.to_csv('./gdrive/My Drive/IDL_Project/results/PGD_exp1_infer_noise{}_eps{}.csv'.format(infer_noise, eps),index=False)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-02 19:26:31,864 INFO Current experiment setting: eps=0.03137254901960784, inference noise=0.3\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(200, 3, 224, 224)\n",
            "(200,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "PGD - Batches:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-d813f2adb25d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mAcc_nat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_nat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mx_test_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0mpredictions_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_ensemble_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_adv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0maccuracy_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_adv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/art/attacks/attack.py\u001b[0m in \u001b[0;36mreplacement_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfunc_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mreplacement_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfunc_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mbatch_index_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_id\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0madv_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index_1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_index_2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_random_init\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py\u001b[0m in \u001b[0;36m_generate_batch\u001b[0;34m(self, x, targets, mask)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi_max_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             adv_x = self._compute_torch(\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0madv_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_random_init\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi_max_iter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m             )\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py\u001b[0m in \u001b[0;36m_compute_torch\u001b[0;34m(self, x, x_init, y, mask, eps, eps_step, random_init)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;31m# Get perturbation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mperturbation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_perturbation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_adv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;31m# Apply perturbation and clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py\u001b[0m in \u001b[0;36m_compute_perturbation\u001b[0;34m(self, x, y, mask)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Get gradient wrt loss; invert it if attack is targeted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_gradient_framework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargeted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# Apply norm bound\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-96ef40430887>\u001b[0m in \u001b[0;36mloss_gradient_framework\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m         grads = [\n\u001b[1;32m    132\u001b[0m                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_classifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_gradient_framework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                 ]\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-96ef40430887>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    131\u001b[0m         grads = [\n\u001b[1;32m    132\u001b[0m                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_classifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_gradient_framework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                 ]\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/art/estimators/classification/pytorch.py\u001b[0m in \u001b[0;36mloss_gradient_framework\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# Compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 15.90 GiB total capacity; 14.63 GiB already allocated; 19.88 MiB free; 15.06 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEzaTmhp7qFg"
      },
      "source": [
        "# Experiment 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIZS0qT17u3_"
      },
      "source": [
        "num_selected_models = 15\n",
        "norm = np.inf\n",
        "infer_noise_list = [0.0025, 0.005, 0.01, 0.05, 0.08, 0.1]\n",
        "eps_list = [0.0025, 0.01, 0.1, 0.5, 0.8]\n",
        "\n",
        "for infer_noise in infer_noise_list:\n",
        "  for eps in eps_list:\n",
        "    my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
        "                                                  device, \n",
        "                                                  infer_noise=infer_noise,\n",
        "                                                  num_selected_models=num_selected_models,\n",
        "                                                  clip_values=[0., 1.], \n",
        "                                                  channels_first=True)\n",
        "    attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=eps, eps_step=eps/3, norm=norm)\n",
        "    Acc_adv = []\n",
        "    Acc_nat = []\n",
        "    for batch_idx, (X, Y) in enumerate(testloader):\n",
        "      x_test = X.numpy()\n",
        "      y_test = Y.numpy()\n",
        "      predictions_nat = my_ensemble_classifier.predict(x_test)\n",
        "      accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
        "      Acc_nat.append(accuracy_nat)\n",
        "\n",
        "      x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "      predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
        "      accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
        "      Acc_adv.append(accuracy_adv)\n",
        "      \n",
        "      logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
        "      logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
        "    \n",
        "    Acc_adv = np.asanyarray(Acc_adv)\n",
        "    Acc_nat = np.asanyarray(Acc_nat)\n",
        "    idx = np.arange(0, len(testset)/test_batchsize).tolist()\n",
        "    res = {'Id': idx ,'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
        "    df = pd.DataFrame(res) \n",
        "    # saving the dataframe \n",
        "    df.to_csv('./gdrive/My Drive/IDL_Project/results/PGD_exp1_infer_noise{}_eps{}.csv'.format(infer_noise, eps),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH-xmYJL8n14"
      },
      "source": [
        "# Experiment 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e44AHWvw8qxI"
      },
      "source": [
        "num_selected_models_list = [5, 10, 15, 20, 25, 30, 35, 40]\n",
        "norm = np.inf\n",
        "infer_noise = 0.1\n",
        "eps_list = [0.0025, 0.01, 0.1, 0.5, 0.8]\n",
        "\n",
        "for num_selected_models in num_selected_models_list:\n",
        "  for eps in eps_list:\n",
        "    my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
        "                                                  device, \n",
        "                                                  infer_noise=infer_noise,\n",
        "                                                  num_selected_models=num_selected_models,\n",
        "                                                  clip_values=[0., 1.], \n",
        "                                                  channels_first=True)\n",
        "    attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=eps, eps_step=eps/3, norm=norm)\n",
        "    Acc_adv = []\n",
        "    Acc_nat = []\n",
        "    for batch_idx, (X, Y) in enumerate(testloader):\n",
        "      x_test = X.numpy()\n",
        "      y_test = Y.numpy()\n",
        "      x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "      predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
        "      predictions_nat = my_ensemble_classifier.predict(x_test)\n",
        "      accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
        "      accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
        "      Acc_adv.append(accuracy_adv)\n",
        "      Acc_nat.append(accuracy_nat)\n",
        "      logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
        "      logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
        "    \n",
        "    Acc_adv = np.asanyarray(Acc_adv)\n",
        "    Acc_nat = np.asanyarray(Acc_nat)\n",
        "    idx = np.arange(0, len(testset)/test_batchsize).tolist()\n",
        "    res = {'Id': idx ,'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
        "    df = pd.DataFrame(res) \n",
        "    # saving the dataframe \n",
        "    df.to_csv('./gdrive/My Drive/IDL_Project/results/PGD_exp2_N{}_eps{}.csv'.format(num_selected_models, eps),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng7Xe9Zq8rai"
      },
      "source": [
        "# Experiment 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3rzYvCu8uoa"
      },
      "source": [
        "num_selected_models = 15\n",
        "norm = 2\n",
        "infer_noise_list = [0.0025, 0.005, 0.01, 0.05, 0.08, 0.1]\n",
        "eps_list = [0.1, 0.5, 0.8, 0.9, 1]\n",
        "\n",
        "for infer_noise in infer_noise_list:\n",
        "  for eps in eps_list:\n",
        "    my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
        "                                                  device, \n",
        "                                                  infer_noise=infer_noise,\n",
        "                                                  num_selected_models=num_selected_models,\n",
        "                                                  clip_values=[0., 1.], \n",
        "                                                  channels_first=True)\n",
        "    attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=eps, eps_step=eps/3, norm=norm)\n",
        "    Acc_adv = []\n",
        "    Acc_nat = []\n",
        "    for batch_idx, (X, Y) in enumerate(testloader):\n",
        "      x_test = X.numpy()\n",
        "      y_test = Y.numpy()\n",
        "      x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "      predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
        "      predictions_nat = my_ensemble_classifier.predict(x_test)\n",
        "      accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
        "      accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
        "      Acc_adv.append(accuracy_adv)\n",
        "      Acc_nat.append(accuracy_nat)\n",
        "      logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
        "      logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
        "    \n",
        "    Acc_adv = np.asanyarray(Acc_adv)\n",
        "    Acc_nat = np.asanyarray(Acc_nat)\n",
        "    idx = np.arange(0, len(testset)/test_batchsize).tolist()\n",
        "    res = {'Id': idx ,'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
        "    df = pd.DataFrame(res) \n",
        "    # saving the dataframe \n",
        "    df.to_csv('./gdrive/My Drive/IDL_Project/results/PGD_exp3_infer_noise{}_eps{}.csv'.format(infer_noise, eps),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-w5HNiC8vDY"
      },
      "source": [
        "# Experiment 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTCAdeqj8w17"
      },
      "source": [
        "num_selected_models_list = [5, 10, 15, 20, 25, 30, 35, 40]\n",
        "norm = 2\n",
        "infer_noise = 0.1\n",
        "eps_list = [0.1, 0.5, 0.8, 0.9, 1]\n",
        "\n",
        "for num_selected_models in num_selected_models_list:\n",
        "  for eps in eps_list:\n",
        "    my_ensemble_classifier = MyEnsembleClassifier(classifier_list,\n",
        "                                                  device, \n",
        "                                                  infer_noise=infer_noise,\n",
        "                                                  num_selected_models=num_selected_models,\n",
        "                                                  clip_values=[0., 1.], \n",
        "                                                  channels_first=True)\n",
        "    attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=eps, eps_step=eps/3, norm=norm)\n",
        "    Acc_adv = []\n",
        "    Acc_nat = []\n",
        "    for batch_idx, (X, Y) in enumerate(testloader):\n",
        "      x_test = X.numpy()\n",
        "      y_test = Y.numpy()\n",
        "      x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "      predictions_adv = my_ensemble_classifier.predict(x_test_adv)\n",
        "      predictions_nat = my_ensemble_classifier.predict(x_test)\n",
        "      accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == y_test) / len(y_test)\n",
        "      accuracy_nat = np.sum(np.argmax(predictions_nat, axis=1) == y_test) / len(y_test)\n",
        "      Acc_adv.append(accuracy_adv)\n",
        "      Acc_nat.append(accuracy_nat)\n",
        "      logging.info('accuracy (adversarial): {}'.format(accuracy_adv))\n",
        "      logging.info('accuracy (natural): {}'.format(accuracy_nat))\n",
        "    \n",
        "    Acc_adv = np.asanyarray(Acc_adv)\n",
        "    Acc_nat = np.asanyarray(Acc_nat)\n",
        "    idx = np.arange(0, len(testset)/test_batchsize).tolist()\n",
        "    res = {'Id': idx ,'AccADV': Acc_adv, 'AccNAT': Acc_nat} \n",
        "    df = pd.DataFrame(res) \n",
        "    # saving the dataframe \n",
        "    df.to_csv('./gdrive/My Drive/IDL_Project/results/PGD_exp4_N{}_eps{}.csv'.format(num_selected_models, eps),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdUjTkdWBdgh"
      },
      "source": [
        "# **The rest parts of the notebook was regarding previous work. Discard them for now.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP_Gxq-2jc-9"
      },
      "source": [
        "# FGM Attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmXkglUUKXO3"
      },
      "source": [
        "attack = FastGradientMethod(estimator=my_ensemble_classifier, eps=0.5, targeted=False, norm=2)\n",
        "Acc = []\n",
        "for batch_idx, (X, Y) in enumerate(testloader):\n",
        "    x_test = X.numpy()\n",
        "    y_test = Y.numpy()\n",
        "    # target = (y_test + 1) % 10\n",
        "    x_test_adv = attack.generate(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "    predictions = my_ensemble_classifier.predict(x_test_adv)\n",
        "    predictions_test = my_ensemble_classifier.predict(x_test)\n",
        "    accuracy = np.sum(np.argmax(predictions, axis=1) == y_test) / len(y_test)\n",
        "    acc_test = np.sum(np.argmax(predictions_test, axis=1) == y_test) / len(y_test)\n",
        "    # print(y_test.shape)\n",
        "    # acc_rate, coverage_rate = compute_accuracy(preds=predictions, labels=np.reshape(y_test, (test_batchsize, 1)))\n",
        "    # logging.info('acc rate: {}, coverage rate: {}'.format(acc_rate, coverage_rate))\n",
        "    Acc.append(accuracy)\n",
        "    logging.info('accuracy: {}'.format(accuracy))\n",
        "    logging.info('accuracy non adv: {}'.format(acc_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoYiKeGCjfDR"
      },
      "source": [
        "# PGD Attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZRM5XEZjchO"
      },
      "source": [
        "attack = ProjectedGradientDescentPyTorch(my_ensemble_classifier, eps=0.5, eps_step=0.03)\n",
        "Acc = []\n",
        "for batch_idx, (X, Y) in enumerate(testloader):\n",
        "    x_test = X.numpy()\n",
        "    y_test = Y.numpy()\n",
        "    x_test_adv = attack.generate(torch.from_numpy(x_test))\n",
        "    predictions = my_ensemble_classifier.predict(x_test_adv)\n",
        "    accuracy = np.sum(np.argmax(predictions, axis=1) == np.array(y_test)) / len(y_test)\n",
        "    Acc.append(accuracy)\n",
        "    logging.info('accuracy: {}'.format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyMeGON8BFNL"
      },
      "source": [
        "# Experiment settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjUPKB3jvijF"
      },
      "source": [
        "fgm_epsilon = np.array([0.005, 0.01, 0.02, 0.04, 0.08])\n",
        "pgd_epsilon = 8.0 / 255.0 / np.array([2.0, 4.0, 8.0, 16.0])\n",
        "diff = np.array([1e-3, 1e-4, 1e-5, 1e-6])\n",
        "pgd_epsilon_step = pgd_epsilon - diff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XG44M27BNIT"
      },
      "source": [
        "Experiment on inference noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U79zJmCsDEKc"
      },
      "source": [
        "noise_stds = [0.02, 0.04, 0.06, 0.08, 0.1]\n",
        "for i in range(len(noise_stds)):\n",
        "  my_ensemble = MyEnsemble(model_dict, num_models_selected, noise_std=noise_stds[i], num_classes=10)\n",
        "  classifier = PyTorchClassifier(\n",
        "      model=my_ensemble,\n",
        "      clip_values=(0, 1),\n",
        "      loss=nn.CrossEntropyLoss(),\n",
        "      input_shape=(3, 224, 224),\n",
        "      nb_classes=10)\n",
        "  attack = FastGradientMethod(estimator=classifier, eps=0.01)\n",
        "  Acc = []\n",
        "  for batch_idx, (X, Y) in enumerate(testloader):\n",
        "    x_test = X.numpy()\n",
        "    y_test = Y.numpy()\n",
        "    x_test_adv = attack.generate(torch.from_numpy(x_test))\n",
        "    predictions = classifier.predict(x_test_adv)\n",
        "    accuracy = np.sum(np.argmax(predictions, axis=1) == np.array(y_test)) / len(y_test)\n",
        "    Acc.append(accuracy)\n",
        "    logging.info('accuracy: {}'.format(accuracy))\n",
        "  \n",
        "  logging.info(\"Accuracy for eps={}: {}\".format(fgm_epsilon[i], np.mean(np.array(Acc))))\n",
        "\n",
        "  pred_list = np.asanyarray(Acc)\n",
        "  idx = np.arange(0,len(testset)/test_batchsize).tolist()\n",
        "  dict = {'Id': idx ,'acc': pred_list} \n",
        "  df = pd.DataFrame(dict) \n",
        "  # saving the dataframe \n",
        "  df.to_csv('./gdrive/My Drive/IDL_Project/results/FGM_result_noise{}_{}.csv'.format(noise_stds[i], datetime.datetime.now()),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LotwWsVpMYKN"
      },
      "source": [
        "pred_list = np.asanyarray(Acc)\n",
        "idx = np.arange(0,len(testset)/test_batchsize).tolist()\n",
        "dict = {'Id': idx ,'acc': pred_list} \n",
        "df = pd.DataFrame(dict) \n",
        "# saving the dataframe \n",
        "df.to_csv('./gdrive/My Drive/IDL_Project/results/FGM_result_eps{}_{}.csv'.format(fgm_epsilon[-1], datetime.datetime.now()),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqIgjfhwBXVL"
      },
      "source": [
        "Plot PGD adversarial examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Eu7qdDQMnGq"
      },
      "source": [
        "attack = ProjectedGradientDescentPyTorch(estimator=classifier, eps=pgd_epsilon[0], eps_step=pgd_epsilon_step[0])\n",
        "Acc = []\n",
        "adv = []\n",
        "orig = []\n",
        "labels = []\n",
        "pred = []\n",
        "for batch_idx, (X, Y) in enumerate(testloader):\n",
        "  if batch_idx > 0:\n",
        "    break\n",
        "  x_test = X.numpy()\n",
        "  y_test = Y.numpy()\n",
        "  x_test_adv = attack.generate(torch.from_numpy(x_test))\n",
        "  predictions = classifier.predict(x_test_adv)\n",
        "  accuracy = np.sum(np.argmax(predictions, axis=1) == np.array(y_test)) / len(y_test)\n",
        "  Acc.append(accuracy)\n",
        "  adv.append(x_test_adv)\n",
        "  orig.append(x_test)\n",
        "  labels.append(y_test)\n",
        "  pred.append(predictions)\n",
        "  logging.info('accuracy: {}'.format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8sj2LdnNV5n"
      },
      "source": [
        "example_adv = adv[-1]\n",
        "example_orig = orig[-1]\n",
        "label = labels[-1]\n",
        "predi = np.argmax(predictions, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoD6QHEwNt4T"
      },
      "source": [
        "print(label)\n",
        "print(predi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnkXnHRePFeW"
      },
      "source": [
        "idx = 2\n",
        "selected_adv = example_adv[idx]\n",
        "selected_orig = example_orig[idx]\n",
        "selected_lab = label[idx]\n",
        "selected_pred = predi[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZuD1OfoWroX"
      },
      "source": [
        "print(selected_lab)\n",
        "print(selected_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkxSl4PTPab0"
      },
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(np.transpose(selected_orig, (1, 2, 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tpehq_0hVobG"
      },
      "source": [
        "plt.imshow(np.transpose(selected_adv, (1, 2, 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuLSJh8QVsHy"
      },
      "source": [
        "plt.imshow(10*(np.transpose(selected_orig, (1, 2, 0)) - np.transpose(selected_adv, (1, 2, 0))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lOaioJIBh1G"
      },
      "source": [
        "Experiment PGD attacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuxeEuf-8UVE"
      },
      "source": [
        "for i in range(len(pgd_epsilon)):\n",
        "  attack = FastGradientMethod(estimator=classifier, eps=fgm_epsilon[i])\n",
        "  Acc = []\n",
        "  for batch_idx, (X, Y) in enumerate(testloader):\n",
        "    x_test = X.numpy()\n",
        "    y_test = Y.numpy()\n",
        "    x_test_adv = attack.generate(torch.from_numpy(x_test))\n",
        "    predictions = classifier.predict(x_test_adv)\n",
        "    accuracy = np.sum(np.argmax(predictions, axis=1) == np.array(y_test)) / len(y_test)\n",
        "    Acc.append(accuracy)\n",
        "    logging.info('accuracy: {}'.format(accuracy))\n",
        "  \n",
        "  logging.info(\"Accuracy for eps={}: {}\".format(fgm_epsilon[i], np.mean(np.array(Acc))))\n",
        "\n",
        "  pred_list = np.asanyarray(Acc)\n",
        "  idx = np.arange(0,len(testset)/test_batchsize).tolist()\n",
        "  dict = {'Id': idx ,'acc': pred_list} \n",
        "  df = pd.DataFrame(dict) \n",
        "  # saving the dataframe \n",
        "  df.to_csv('./gdrive/My Drive/IDL_Project/results/FGM_result_eps{}_{}.csv'.format(fgm_epsilon[i], datetime.datetime.now()),index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}